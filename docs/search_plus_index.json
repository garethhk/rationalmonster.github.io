{"./":{"url":"./","title":"简介","keywords":"","body":"Curiouser's Devops Roadmap This gitbook records the technical roadmap of Devops Curiouser. Link GitBook Access URL: https://gitbook.curiouser.top/docs GitHub: https://github.com/RationalMonster What I had done at Openshift or Kubernetes The technical MindMap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 16:13:29 "},"origin/openshift-allinone安装.html":{"url":"origin/openshift-allinone安装.html","title":"Allinone","keywords":"","body":"搭建Allinone全组件Openshift 3.11 一、Overviews Prerequisite IP地址：192.168.1.86 CentOS：7.5.1804 硬盘划分 系统盘60G / ; 30G /var 数据盘100G /var/lib/docker ; 100G /data/nfs 开启Selinuxsed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/selinux/config && \\ setenforce 0 Context Docker：版本 1.13，Overlay2(执行Ansible准备脚本时会进行安装) Openshift：版本 3.11 Kubernetes：版本 v1.11.0 二、使用Ansible安装部署 设置主机名并在本地Host文件中添加IP地址域名映射关系ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 配置中科大Openshift的YUM 源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ bash -c ' cat > /etc/yum.repos.d/all.repo 安装基础软件yum install -y git vim net-tools lrzsz unzip bind-utils yum-utils bridge-utils python-passlib wget java-1.8.0-openjdk-headless httpd-tools lvm2 安装Ansible 2.6.5 yum install -y https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm 获取openshift ansible部署脚本代码，禁用ansible脚本中的指定repo git clone https://github.com/openshift/openshift-ansible.git -b release-3.11 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2 (可选)将附件中定制化的OKD登陆页面文件放置/etc/origin/master/custom路径下（自定义的登陆首页）# 路径需要新建 mkdir -p /etc/origin/master/custom 配置Ansible部署Openshift的主机清单/etc/ansible/hosts [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] openshift_ip=192.168.1.86 openshift_public_ip=192.168.1.86 ansible_default_ipv4.address=192.168.1.86 ansible_ssh_user=root openshift_deployment_type=origin deployment_type=origin openshift_release=3.11 openshift_image_tag=v3.11.0 ansible_ssh_pass=**Root用户SSH密码** ######################### Components Cert and CA Expire Days ################# openshift_hosted_registry_cert_expire_days=36500 openshift_ca_cert_expire_days=36500 openshift_node_cert_expire_days=36500 openshift_master_cert_expire_days=36500 etcd_ca_default_days=36500 ####################### Multitenant Network ####################### os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant ####################### OKD ####################### openshift_clock_enabled=true openshift_enable_unsupported_configurations=True openshift_node_groups=[{'name': 'allinone', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage ####################### OKD master config ####################### openshift_master_api_port=8443 openshift_master_cluster_public_hostname=allinone.okd311.curiouser.com openshift_master_cluster_hostname=allinone.okd311.curiouser.com openshift_master_default_subdomain=apps.okd311.curiouser.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin':'$apr1$eG8zNL.C$fvACBzDJ7.N7KdJORT12E0'} openshift_master_oauth_template=custom/login.html openshift_master_session_name=ssn openshift_master_session_max_seconds=3600 ####################### Docker ####################### container_runtime_docker_storage_setup_device=/dev/sdb container_runtime_docker_storage_type=overlay2 openshift_examples_modify_imagestreams=true openshift_docker_options=\"--selinux-enabled -l warn --ipv6=false --insecure-registry=0.0.0.0/0 --log-opt max-size=10M --log-opt max-file=3 --registry-mirror=https://zlsoueh7.mirror.aliyuncs.com\" ####################### Web Console ####################### openshift_web_console_extension_script_urls=[\"https://xhua-static.sh1a.qingstor.com/allinone/allinone-webconsole.js\"] openshift_web_console_extension_stylesheet_urls=[\"https://hermes-uat.mbcloud.com/mbcloud/M00/00/3A/rBACF1vz8NyALOS3AAApT8C9PDY549.css\"] ####################### Registry ####################### openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/data/nfs openshift_hosted_registry_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=10Gi ####################### metrics ####################### openshift_metrics_install_metrics=true openshift_metrics_image_version=v3.11.0 openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/data/nfs openshift_metrics_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=10Gi ####################### logging ####################### openshift_logging_install_logging=true openshift_logging_image_version=v3.11.0 openshift_logging_es_ops_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_es_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_elasticsearch_pvc_size=5Gi openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/data/nfs openshift_logging_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=10Gi ################### Prometheus Cluster Monitoring ################### openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_prometheus_storage_enabled=true openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi ##################### Disable Components ############# openshift_enable_service_catalog=false ansible_service_broker_install=false [masters] allinone.okd311.curiouser.com [etcd] allinone.okd311.curiouser.com [nfs] allinone.okd311.curiouser.com [nodes] allinone.okd311.curiouser.com openshift_node_group_name='allinone' 创建NFS挂载目录 pvcreate /dev/sdc && \\ vgcreate -s 4m data /dev/sdc && \\ lvcreate --size 45G -n nfs data && \\ mkfs.xfs /dev/data/nfs && \\ echo \"/dev/data/nfs /data/nfs xfs defaults 0 0\" >> /etc/fstab && \\ mkdir /data/nfs -p && \\ mount -a && \\ df -mh （可选）预先拉取安装过程中可能使用的镜像docker pull docker.io/openshift/origin-node:v3.11.0 && \\ docker pull docker.io/openshift/origin-control-plane:v3.11.0 && \\ docker pull docker.io/openshift/origin-haproxy-router:v3.11.0 && \\ docker pull docker.io/openshift/origin-deployer:v3.11.0 && \\ docker pull docker.io/openshift/origin-pod:v3.11.0 && \\ docker pull docker.io/openshift/origin-docker-registry:v3.11.0 && \\ docker pull docker.io/openshift/origin-console:v3.11.0 && \\ docker pull docker.io/openshift/origin-service-catalog:v3.11.0 && \\ docker pull docker.io/openshift/origin-web-console:v3.11.0 && \\ docker pull docker.io/cockpit/kubernetes:latest && \\ docker pull docker.io/openshift/oauth-proxy:v1.1.0 && \\ docker pull docker.io/openshift/origin-docker-builder:v3.11.0 && \\ docker pull docker.io/openshift/prometheus-alertmanager:v0.15.2 && \\ docker pull docker.io/openshift/prometheus-node-exporter:v0.16.0 && \\ docker pull docker.io/openshift/prometheus:v2.3.2 && \\ docker pull docker.io/grafana/grafana:5.2.1 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker tag quay-mirror.qiniu.com/coreos/etcd:v3.2.22 quay.io/coreos/etcd:v3.2.22 && \\ docker rmi quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 quay.io/coreos/kube-state-metrics:v1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker tag quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 quay.io/coreos/configmap-reload:v0.0.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker pull quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker tag quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-config-reloader:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 执行OKD Ansible Playbook先执行安装检查的Playbook ansible-playbook /root/openshift-ansible/playbooks/prerequisites.yml 再执行安装Playbook ansible-playbook /root/openshift-ansible/playbooks/deploy_cluster.yml 授予admin用户以管理员权限 oc adm policy add-cluster-role-to-user cluster-admin admin 三、配置Openshift的后端存储 使用Ceph RBD作为后端存储 搭建单节点的Ceph，详见（Ceph RBD单节点安装） 创建Storageclass 使用NFS作为后端存储：详见 参考文章链接 当主机有多网卡时指定组件监听的网卡IP地址：https://github.com/ViaQ/Main/blob/master/README-install.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-07-31 18:05:28 "},"origin/openshift-kubernetes的审计日志功能.html":{"url":"origin/openshift-kubernetes的审计日志功能.html","title":"Openshift/Kubernetes的审计日志功能","keywords":"","body":"一、Overviews kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题： 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？ kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有： RequestReceived ：apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted ：在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete ：当响应 body 发送完并且不再发送数据。 Panic：内部服务器出错，请求未完成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成 日志记录级别，当前支持的日志记录级别有： None: 符合这条规则的日志将不会记录。 Metadata: 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request: 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse: 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。 输出的审计日志格式 json{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1beta1\", \"metadata\": { \"creationTimestamp\": \"2019-07-23T09:02:19Z\" }, \"level\": \"Request\", \"timestamp\": \"2019-07-23T09:02:19Z\", \"auditID\": \"eb481add-fdac-48a3-a302-1c33d73bfdbf\", \"stage\": \"RequestReceived\", \"requestURI\": \"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\", \"verb\": \"update\", \"user\": { \"username\": \"system:openshift-master\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.1.96\" ], \"objectRef\": { \"resource\": \"configmaps\", \"namespace\": \"kube-system\", \"name\": \"openshift-master-controllers\", \"apiVersion\": \"v1\" }, \"requestReceivedTimestamp\": \"2019-07-23T09:02:19.148057Z\", \"stageTimestamp\": \"2019-07-23T09:02:19.148057Z\" } legacy 2019-07-23T23:50:06.223368641+08:00 AUDIT: id=\"3574e2e0-06b1-44d8-bc6c-5983c402d55e\" stage=\"ResponseComplete\" ip=\"192.168.1.96\" method=\"update\" user=\"system:openshift-master\" groups=\"\\\"system:masters\\\",\\\"system:authenticated\\\"\" as=\"\" asgroups=\"\" namespace=\"kube-system\" uri=\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\" response=\"200\" 审计后端可以将审计事件导出到外部存储。 Kube-apiserver 提供两个后端： Log 后端: 将事件写入到磁盘 Webhook 后端: 将事件发送到外部 API Note: 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 二、openshift开启自定义策略的审计功能 创建审计日志的存储路径 mkdir /etc/origin/master/audit # 注意：审计日志文件的存储路径必须是kube-system命名空间下apiservser pod挂载目录下的子路径。 # ocp 3.11版本的apiserver是以pod的形式运行在kube-system命名空间下的，它所需要的配置文件等Volume资源都是以hostpath的形式挂载上去的，例如ocp节点上的/etc/origin/master目录 编辑/etc/origin/master/master-config.yaml ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 ****省略******** 创建自定义的审计策略配置文件 kind: Policy omitStages: - \"ResponseStarted\" rules: - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"configmaps\",\"secrets\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the metadata level. - level: None verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the request level. - level: None resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # Log login failures from the web console or CLI. Review the logs and refine your policies. - level: Metadata nonResourceURLs: - /login* - /oauth* - level: Metadata userGroups: [\"system:authenticated:oauth\"] verbs: [\"create\", \"delete\"] resources: - group: \"project.openshift.io\" resources: [\"projectrequests\", \"projects\"] omitStages: - RequestReceived 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 三、使用openshift集群的Fluentd收集审计日志到集群内的elasticsearch 配置OCP集群中的Fluentd挂载审计日志的存储目录(OCP集群中日志系统的fluentd是以DaemonSet形式收集节点上容器的日志到elasticsearch的，它是将节点的/var/lib/docker目录以hostpath形式挂载到容器中的) oc set volume ds/logging-fluentd --add --mount-path=/etc/origin/master/audit --name=audit --type=hostPath --path=/etc/origin/master/audit -n openshift-logging 配置OCP集群中的Fluentd监控审计日志目录下的日志 oc edit cm/logging-fluentd -n openshift-logging *****省略******* ## sources *****省略******* @include configs.d/user/input-audit.conf *****省略******* input-audit.conf: | @type tail @id audit-ocp path /etc/origin/master/audit/audit-ocp.log pos_file /etc/origin/master/audit/audit.pos tag audit.requests format json @type copy @type elasticsearch log_level debug host \"#{ENV['OPS_HOST']}\" port \"#{ENV['OPS_PORT']}\" scheme https ssl_version TLSv1_2 index_name .audit user fluentd password changeme client_key \"#{ENV['OPS_CLIENT_KEY']}\" client_cert \"#{ENV['OPS_CLIENT_CERT']}\" ca_file \"#{ENV['OPS_CA']}\" type_name com.redhat.ocp.audit reload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'false'}\" reload_after \"#{ENV['ES_RELOAD_AFTER'] || '100'}\" sniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::ElasticsearchSimpleSniffer'}\" reload_on_failure false flush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '5s'}\" max_retry_wait \"#{ENV['ES_RETRY_WAIT'] || '300'}\" disable_retry_limit true buffer_type file buffer_path '/var/lib/fluentd/buffer-output-es-auditlog' buffer_queue_limit \"#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }\" buffer_chunk_limit \"#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }\" buffer_queue_full_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}\" request_timeout 2147483648 *****省略******* 重启Fluentd oc delete po -l component=fluentd -n openshift-logging 在ocp集群系统的Kibana上添加\".audit*\"的Index Pattern,并在\"Discover\"查看、筛选审计日志 四、将审计日志通过WebHook 发送到OCP外部的Logstash或者Fluentd 接受后端 可使用Logstash或者Fluentd作为后端来接受Api-Server通过web hook方式发送的审计日志。Logstash和Fluentd可以是ocp集群外二进制方式安装运行的，也可以是原生Docker运行的，甚至可以是另外一个集群中容器化的。一个原则就是不要放到审计日志产生集群的内部。防止apiserver启动起来了，有了一些操作，logstash还没有启动起来，丢失审计日志。再者审计日志后端最好选择适合自己的，审计日志落一份，重复记录也没多大意义。 方式一：使用OCP集群外二进制方式安装的Logstash来接受ApiServer通过web hook方式发送过来的审计日志并过滤、存储到本地文件中 安装logstash bash -c 'cat > /etc/yum.repos.d/elasticsearch.repo 设置logstash，/etc/logstash/logstash.yml # ------------ Pipeline Configuration Settings -------------- # Where to fetch the pipeline configuration for the main pipeline path.config: /etc/logstash/conf.d/ *************************省略****************************** # ------------ Data path ------------------ # Which directory should be used by logstash and its plugins for any persistent needs. Defaults to LOGSTASH_HOME/data path.data: /data/logs/logstash/data/ *************************省略****************************** # ------------ Debugging Settings ------------- # Options for log.level: fatal/error/warn/info (default)/debug/trace log.level: info path.logs: /data/logs/logstash/logs 创建监听HTTP 8081端口的pipeline cat /etc/logstash/conf.d/accept-audit-log.conf input{ http{ host => \"0.0.0.0\" port => 8081 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=>[items] # We only need event subelement, remove others. remove_field=>[headers, metadata, apiVersion, kind, \"@version\", host] } mutate{ rename => {items=>event} } } output{ file{ # Audit events from different users will be saved into different files. path=>\"/data/logs/logstash/ocp-audit-logs/ocp-audit-%{[event][user][username]}/audit-%{+YYYY-MM-dd}.log\" } } EOF 启动logstash mkdir -p /data/logs/logstash/{data,logs,ocp-audit-logs} chown -R logstash:logstash /data/logs/logstash system start logstash # 或者 /usr/share/logstash/bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/ 测试logstash的联通性。一是看logstash pipeline监听的HTTP端口是否开启。二是尝试发送一个带有模拟数据的POST请求，看其是否会pipeline指定的数据目录生成日志文件 ss -ntl |grep 8081 curl -X POST \\ http://192.168.1.96:8081 \\ -H 'Accept: */*' \\ -H 'Cache-Control: no-cache' \\ -H 'Connection: keep-alive' \\ -H 'Content-Type: application/json' \\ -H 'accept-encoding: gzip, deflate' \\ -d '{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1beta1\",\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:27:54Z\"},\"level\":\"Request\",\"timestamp\":\"2019-07-23T14:27:54Z\",\"auditID\":\"29bf32ba-4bea-4b4f-a1fb-cd091b2188ff\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"verb\":\"update\",\"user\":{\"username\":\"system:openshift-master\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"resource\":\"configmaps\",\"namespace\":\"kube-system\",\"name\":\"openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"apiVersion\":\"v1\",\"resourceVersion\":\"8285989\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ConfigMap\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-master-controllers\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"resourceVersion\":\"8285989\",\"creationTimestamp\":\"2019-03-09T11:31:08Z\",\"annotations\":{\"control-plane.alpha.kubernetes.io/leader\":\"{\\\"holderIdentity\\\":\\\"allinone.okd311.curiouser.com\\\",\\\"leaseDurationSeconds\\\":15,\\\"acquireTime\\\":\\\"2019-03-09T11:31:01Z\\\",\\\"renewTime\\\":\\\"2019-07-23T14:27:54Z\\\",\\\"leaderTransitions\\\":0}\"}}},\"requestReceivedTimestamp\":\"2019-07-23T14:27:54.894767Z\",\"stageTimestamp\":\"2019-07-23T14:27:54.899643Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}}' 创建audit的webhook配置文件/etc/origin/master/audit-policy.yaml cat /etc/origin/master/audit-policy.yaml apiVersion: v1 clusters: - cluster: server: http://192.168.1.96:8081 name: logstash contexts: - context: cluster: logstash user: \"\" name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF 编辑/etc/origin/master/master-config.yaml，添加webhook相关的参数 ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 #==========以下配置项为添加的webhook参数=========================================================================== webHookKubeConfig: /etc/origin/master/audit-webhook-config.yaml # 指定WebHook的配置文件（同样路径要指定在ApiServer POD已挂载的路径下） webHookMode: batch # 可选参数\"batch\"和\"blocking\" ****省略******** 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 验证，用除\"system:admin\"用户外的其他用户创建project，然后再删除project，最后查看logstash配置的审计日志存储目录下是否生成对应的文件 oc login -u admin -p oc new-project test oc delete project test $ tree -L 2 /data/logs/logstash/ocp-audit-logs/ /data/logs/logstash/ocp-audit-logs/ ├── ocp-audit-admin │ └── audit-2019-07-23.log └── ocp-audit-system:openshift-master └── audit-2019-07-23.log $ cat /data/logs/logstash/ocp-audit-logs/ocp-audit-admin/audit-2019-07-23.log 产生以下内容。显示一次创建成功，另一次创建失败，原因是project已经存在（特意测试），一次删除project等日志。 {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:03Z\"},\"stageTimestamp\":\"2019-07-23T14:45:03.019249Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:02Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:02.964347Z\",\"auditID\":\"eec24884-b70a-4b27-80c1-431111d2f4f5\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:10Z\"},\"stageTimestamp\":\"2019-07-23T14:45:10.842999Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:10Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"reason\":\"AlreadyExists\",\"code\":409},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:10.836487Z\",\"auditID\":\"a7b64f47-94eb-4723-b101-24e112cd0735\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"self-provisioners\\\" of ClusterRole \\\"self-provisioner\\\" to Group \\\"system:authenticated:oauth\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:19Z\"},\"stageTimestamp\":\"2019-07-23T14:45:19.813911Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:19Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projects\",\"namespace\":\"test\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Success\",\"code\":200},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:19.805940Z\",\"auditID\":\"9d3260ca-3bff-49da-9fe9-346043a29991\",\"verb\":\"delete\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projects/test\"}} 方式二：复用ocp集群内日志系统的Fluentd接受ApiServer通过web hook方式发送过来的审计日志并过滤、存储到挂载的文件中 整得太晚了，后续更新。 五、审计策略配置详解 整得太晚了，后续更新。 六、相关链接 https://austindewey.com/2018/10/17/integrating-advanced-audit-with-aggregated-logging-in-openshift-3-11/#test-it-outhttps://www.outcoldsolutions.com/docs/monitoring-openshift/v4/audit/https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-config-advanced-audithttps://docs.openshift.com/container-platform/3.11/security/monitoring.htmlhttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6chttps://www.jianshu.com/p/8117bc2fb966https://cloud.google.com/kubernetes-engine/docs/concepts/audit-policy?hl=zh-cnhttps://github.com/rbo/openshift-examples/tree/master/efk-auditloghttps://github.com/openshift/origin-aggregated-logging/issues/1226 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 14:46:19 "},"origin/ceph-rbd单节点安装.html":{"url":"origin/ceph-rbd单节点安装.html","title":"Ceph RBD单节点安装","keywords":"","body":"Prerequisite Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 关闭防火墙和SeLinuxsystemctl disable firewalld ; systemctl stop firewalld ; sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config ; setenforce 0 ; sestatus -v SSH免密码登录打通ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ; ssh-copy-id root@allinone.curiouser.com ; ssh allinone.curiouser.com Hosts绑定IP地址域名解析echo \"192.168.1.21 allinone.curiouser.com\" >> /etc/hosts 创建ceph用户并设置用户密码和为其添加root权限useradd ceph && echo ceph:ceph | chpasswd ; echo \"ceph ALL=(root) NOPASSWD:ALL\" > /etc/sudoers.d/ceph ; chmod 0440 /etc/sudoers.d/ceph 配置Ceph和Epel的yum源仓库 vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=[http://download.ceph.com/rpm-jewel/el7/$basearch](http://download.ceph.com/rpm-jewel/el7/$basearch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=[http://download.ceph.com/rpm-jewel/el7/noarch](http://download.ceph.com/rpm-jewel/el7/noarch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 # =========================================================================================== vim /etc/yum.repos.d/epel.repo [epel] name=Extra Packages for Enterprise Linux 7 - $basearch # baseurl=[http://download.fedoraproject.org/pub/epel/7/$basearch](http://download.fedoraproject.org/pub/epel/7/$basearch) metalink=[https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch](https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch) failovermethod=priority enabled=1 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 # =========================================================================================== 可以修改ceph源（外国的源总是timeout） export CEPH_DEPLOY_REPO_URL=[http://mirrors.163.com/ceph/rpm-jewel/el7](http://mirrors.163.com/ceph/rpm-jewel/el7) ; export CEPH_DEPLOY_GPG_URL=[http://mirrors.163.com/ceph/keys/release.asc](http://mirrors.163.com/ceph/keys/release.asc) (可选)手动安装下载ceph的rpm包（使用ceph-deploy install 安装ceph包网速太慢。）官方下载: http://download.ceph.com/rpm-jewel/el7/x86_64/ 。需要下载的包如下： ceph-10.2.10-0.el7.x86_64.rpm ceph-base-10.2.10-0.el7.x86_64.rpm ceph-common-10.2.10-0.el7.x86_64.rpm ceph-mds-10.2.10-0.el7.x86_64.rpm ceph-mon-10.2.10-0.el7.x86_64.rpm ceph-osd-10.2.10-0.el7.x86_64.rpm ceph-radosgw-10.2.10-0.el7.x86_64.rpm ceph-selinux-10.2.10-0.el7.x86_64.rpm rbd-mirror-10.2.10-0.el7.x86_64.rpm yum localinstall -y ./*.rpm 一、安装Ceph-Deploy 安装Ceph-deploy yum install ceph-deploy -y 安装ceph相关的软件 ceph-deploy install $HOSTNAME 二、创建集群配置文件 创建ceph-deploy的集群配置文件夹，路径并切换过去mkdir my-cluster ;cd my-cluster 用 ceph-deploy 创建集群，用 new 命令、并指定主机作为初始监视器。 ceph-deploy new $HOSTNAME # 该操作会在~/my-cluster下会生成三个文件 $&gt; ls -rw-rw-r-- 1 ceph ceph 251 Jan 12 16:34 ceph.conf -rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log -rw------- 1 ceph ceph 73 Jan 12 16:30 ceph.mon.keyring # ceph.conf中默认的osd pool为3，对应了三个node节点。如果只有两个node节点，则需要修改ceph.conf中的默认值 [global] fsid = 25c13add-967e-4912-bb33-ebbc2cb9376d mon_initial_members = allinone.curiouser.com mon_host = 172.16.2.3 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx filestore_xattr_use_omap = true osd pool default size=1 三、创建Monitor ceph-deploy mon create $HOSTNAME ; ceph-deploy gatherkeys $HOSTNAME ; ceph mds stat #查看mds节点状态 四、创建OSD 方式一： (可选)手动节点上挂载lvm存储到某个目录下，作为node节点上OSD的数据存储目录 yum install -y lvm2 ; disk=/dev/vdc ; pvcreate ${disk} ; vgcreate ${disk} ; vgcreate -s 16m ceph-osd ${disk} ; PE_Number=`vgdisplay|grep \"Free PE\"|awk '{print $5}'` ; lvcreate -l ${PE_Number} -n ceph-osd ceph-osd ; mkfs.xfs /dev/ceph-osd/ceph-osd ; mkdir -p /data/ceph/osd ; chown -R ceph:ceph /data/ceph/osd ; echo \"/dev/ceph-osd/ceph-osd /data/ceph/osd xfs defaults 0 0\" &gt;&gt;/etc/fstab ; mount -a ; df -mh #LV的文件系统格式注意要xfs,CentOS推荐使用xfs的文件系统.如果是ext4，需要在/etc/ceph/ceph.conf 中添加参数用来限制文件名的长度 osd max object name len = 256 osd max object namespace len = 64 # 之后重启osd服务 systemctl restart ceph-osd.target 准备并激活node节点上的OSD #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/data/ceph/osd #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/data/ceph/osd #查看OSD状态 ceph osd tree 方式二：(不是以目录为OSD数据存储设备，而是直接以硬盘。其实就是省去手动在硬盘上创建分区的操作) #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/dev/vdc #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/dev/vdc1 #查看OSD状态 ceph osd tree 五、安装验证 #集群健康状态检查 $> ceph health HEALTH_OK $> ceph -s $> systemctl is-enabled ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 六、其他信息 Ceph相关的SystemD Units ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 13:49:20 "},"origin/ceph-filesystem单节点安装.html":{"url":"origin/ceph-filesystem单节点安装.html","title":"Ceph FileSystem单节点安装","keywords":"","body":"Context Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 一个cephfs至少要求两个librados存储池，一个为data，一个为metadata。当配置这两个存储池时，注意： 为metadata pool设置较高级别的副本级别，因为metadata的损坏可能导致整个文件系统不用 建议metadata pool使用低延时存储，比如SSD，因为metadata会直接影响客户端的响应速度 Preflight 一个 clean+active 的cluster（Ceph RBD单节点安装） cluster fb506b4e-43b8-4634-acb9-ea3ee5a97b91 health HEALTH_OK monmap e1: 1 mons at {allinone=192.168.1.96:6789/0} election epoch 29, quorum 0 allinone fsmap e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} osdmap e113: 1 osds: 1 up, 1 in flags sortbitwise,require_jewel_osds pgmap v61453: 192 pgs, 3 pools, 2639 MB data, 985 objects 2730 MB used, 94500 MB / 97231 MB avail 192 active+clean 一、操作 部署元数据服务器MDSceph-deploy mds create $HOSTNAME 创建cephfs需要的两个存储池：一个pool用来存储数据，一个pool用来存储元数据ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 创建CephFS ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls 二、验证 $ ceph mds stat e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} 三、客户端挂载 Kernel方式 #加载rbd内核模块 modprobe rbd lsmod | grep rbd # 获取client.admin用户的秘钥 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" mkdir /mnt/mycephfs mount -t ceph allinone.okd311.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== FUSE方式 yum -y install ceph-fuse ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m 192.168.197.154:6789 ~/mycephfs/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 13:46:21 "}}