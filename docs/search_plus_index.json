{"./":{"url":"./","title":"简介","keywords":"","body":"Curiouser's Devops Roadmap This gitbook records the technical roadmap of Devops Curiouser. Link GitBook Access URL: https://gitbook.curiouser.top/docs GitHub: https://github.com/RationalMonster What I had done at Openshift or Kubernetes CI/CD Flow 1. Gitlab Webhook + Jenkins SharedLibraries/Kubernetes + SonarScanner Maven Plugin The technical MindMap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-29 10:57:31 "},"origin/openshift-allinone安装.html":{"url":"origin/openshift-allinone安装.html","title":"Allinone","keywords":"","body":"搭建Allinone全组件Openshift 3.11 一、Overviews Prerequisite IP地址：192.168.1.86 CentOS：7.5.1804 硬盘划分 系统盘60G / 数据盘100G /var/lib/docker ; 100G /data/nfs 开启Selinuxsed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/selinux/config && \\ setenforce 0 Context Docker：版本 1.13，Overlay2(执行Ansible准备脚本时会进行安装) Openshift：版本 3.11 Kubernetes：版本 v1.11.0 二、使用Ansible安装部署 设置主机名并在本地Host文件中添加IP地址域名映射关系ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 配置中科大Openshift的YUM 源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ bash -c ' cat > /etc/yum.repos.d/all.repo 安装基础软件yum install -y git vim net-tools lrzsz unzip bind-utils yum-utils bridge-utils python-passlib wget java-1.8.0-openjdk-headless httpd-tools lvm2 安装Ansible 2.6.5 yum install -y https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm 获取openshift ansible部署脚本代码，禁用ansible脚本中的指定repo git clone https://github.com/openshift/openshift-ansible.git -b release-3.11 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2 (可选)将附件中定制化的OKD登陆页面文件放置/etc/origin/master/custom路径下（自定义的登陆首页）# 路径需要新建 mkdir -p /etc/origin/master/custom 配置Ansible部署Openshift的主机清单/etc/ansible/hosts [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] openshift_ip=192.168.1.86 openshift_public_ip=192.168.1.86 ansible_default_ipv4.address=192.168.1.86 ansible_ssh_user=root openshift_deployment_type=origin deployment_type=origin openshift_release=3.11 openshift_image_tag=v3.11.0 ansible_ssh_pass=**Root用户SSH密码** ######################### Components Cert and CA Expire Days ################# openshift_hosted_registry_cert_expire_days=36500 openshift_ca_cert_expire_days=36500 openshift_node_cert_expire_days=36500 openshift_master_cert_expire_days=36500 etcd_ca_default_days=36500 ####################### Multitenant Network ####################### os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant ####################### OKD ####################### openshift_clock_enabled=true openshift_enable_unsupported_configurations=True openshift_node_groups=[{'name': 'allinone', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage ####################### OKD master config ####################### openshift_master_api_port=8443 openshift_master_cluster_public_hostname=allinone.okd311.curiouser.com openshift_master_cluster_hostname=allinone.okd311.curiouser.com openshift_master_default_subdomain=apps.okd311.curiouser.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin':'$apr1$eG8zNL.C$fvACBzDJ7.N7KdJORT12E0'} openshift_master_oauth_template=custom/login.html openshift_master_session_name=ssn openshift_master_session_max_seconds=3600 ####################### Docker ####################### container_runtime_docker_storage_setup_device=/dev/sdb container_runtime_docker_storage_type=overlay2 openshift_examples_modify_imagestreams=true openshift_docker_options=\"--selinux-enabled -l warn --ipv6=false --insecure-registry=0.0.0.0/0 --log-opt max-size=10M --log-opt max-file=3 --registry-mirror=https://zlsoueh7.mirror.aliyuncs.com\" ####################### Web Console ####################### openshift_web_console_extension_script_urls=[\"https://xhua-static.sh1a.qingstor.com/allinone/allinone-webconsole.js\"] openshift_web_console_extension_stylesheet_urls=[\"https://hermes-uat.mbcloud.com/mbcloud/M00/00/3A/rBACF1vz8NyALOS3AAApT8C9PDY549.css\"] ####################### Registry ####################### openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/data/nfs openshift_hosted_registry_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=10Gi ####################### metrics ####################### openshift_metrics_install_metrics=true openshift_metrics_image_version=v3.11.0 openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/data/nfs openshift_metrics_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=10Gi ####################### logging ####################### openshift_logging_install_logging=true openshift_logging_image_version=v3.11.0 openshift_logging_es_ops_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_es_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_elasticsearch_pvc_size=5Gi openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/data/nfs openshift_logging_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=10Gi ################### Prometheus Cluster Monitoring ################### openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_prometheus_storage_enabled=true openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi ##################### Disable Components ############# openshift_enable_service_catalog=false ansible_service_broker_install=false [masters] allinone.okd311.curiouser.com [etcd] allinone.okd311.curiouser.com [nfs] allinone.okd311.curiouser.com [nodes] allinone.okd311.curiouser.com openshift_node_group_name='allinone' 创建NFS挂载目录 pvcreate /dev/sdc && \\ vgcreate -s 4m data /dev/sdc && \\ lvcreate --size 45G -n nfs data && \\ mkfs.xfs /dev/data/nfs && \\ echo \"/dev/data/nfs /data/nfs xfs defaults 0 0\" >> /etc/fstab && \\ mkdir /data/nfs -p && \\ mount -a && \\ df -mh （可选）预先拉取安装过程中可能使用的镜像docker pull docker.io/openshift/origin-node:v3.11.0 && \\ docker pull docker.io/openshift/origin-control-plane:v3.11.0 && \\ docker pull docker.io/openshift/origin-haproxy-router:v3.11.0 && \\ docker pull docker.io/openshift/origin-deployer:v3.11.0 && \\ docker pull docker.io/openshift/origin-pod:v3.11.0 && \\ docker pull docker.io/openshift/origin-docker-registry:v3.11.0 && \\ docker pull docker.io/openshift/origin-console:v3.11.0 && \\ docker pull docker.io/openshift/origin-service-catalog:v3.11.0 && \\ docker pull docker.io/openshift/origin-web-console:v3.11.0 && \\ docker pull docker.io/cockpit/kubernetes:latest && \\ docker pull docker.io/openshift/oauth-proxy:v1.1.0 && \\ docker pull docker.io/openshift/origin-docker-builder:v3.11.0 && \\ docker pull docker.io/openshift/prometheus-alertmanager:v0.15.2 && \\ docker pull docker.io/openshift/prometheus-node-exporter:v0.16.0 && \\ docker pull docker.io/openshift/prometheus:v2.3.2 && \\ docker pull docker.io/grafana/grafana:5.2.1 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker tag quay-mirror.qiniu.com/coreos/etcd:v3.2.22 quay.io/coreos/etcd:v3.2.22 && \\ docker rmi quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 quay.io/coreos/kube-state-metrics:v1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker tag quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 quay.io/coreos/configmap-reload:v0.0.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker pull quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker tag quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-config-reloader:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 执行OKD Ansible Playbook先执行安装检查的Playbook ansible-playbook /root/openshift-ansible/playbooks/prerequisites.yml 再执行安装Playbook ansible-playbook /root/openshift-ansible/playbooks/deploy_cluster.yml 授予admin用户以管理员权限 oc adm policy add-cluster-role-to-user cluster-admin admin 三、配置Openshift的后端存储 使用Ceph RBD作为后端存储 搭建单节点的Ceph，详见（Ceph RBD单节点安装） 创建Storageclass 使用NFS作为后端存储：详见 参考文章链接 当主机有多网卡时指定组件监听的网卡IP地址：https://github.com/ViaQ/Main/blob/master/README-install.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-Kubernetes的持久化存储.html":{"url":"origin/openshift-Kubernetes的持久化存储.html","title":"数据持久化","keywords":"","body":"一、Kubernetes持久化存储简介 通常情况下，我们可以认为容器或者Pod的生命周期时短暂的，当容器被销毁时，容器内部的数据也同时被清除。 对于容器，数据持久化存储的重要性不言而喻。Docker有存储卷的概念，用来将磁盘上的或另一个容器中的目录挂载到容器的某一个路径下。即使容器挂掉了，挂载Volume中的数据依旧存在。然而没有对其生命周期进行管理。而Kubernetes提供了多种不同类型资源的Volume存储卷，供POD挂载到容器的不同路径下,常见的有： emptyDir：pod被调度到某个宿主机上的时候才创建，而同一个pod内的容器都能读写EmptyDir中的同一个文件。删除容器并不会对它造成影响，只有删除整个Pod时，它才会被删除，它的生命周期与所挂载的Pod一致 hostPath：将宿主机的文件系统的文件或目录挂接到Pod中 secret：将Kubernetes中secret对象资源挂载到POD中 configMap：将Kubernetes中config对象资源挂载到POD中 persistentVolumeClaim：将PersistentVolume挂接到Pod中作为存储卷。使用此类型的存储卷，用户不需要关注存储卷的详细信息。 nfs glusterfs cephfs vspherevolume iscsi .... 对于以上大部分的volume类型，对使用用户是极其不友好的。理解他们体系中的概念配置是一件复杂的事情，有时我们其实并不关心他们的各种存储实现，只希望能够简单安全可靠地存储数据。所以K8S对存储的供应和使用做了抽象，以API形式提供给管理员和用户使用。因此引入了两个新的API资源：Persistent Volume（持久卷PV）和Persistent Volume Claim（持久卷申请PVC）。 PVC负责定义使用多大的存储空间，什么样的读写方式等常见要求即可，而PV负责抽象各种存储系统的技术细节（例如存储系统IP地址端口，客户端证书密钥等），满足PVC的存储需求，继而作为Kubernetes集群的存储对象资源。 apiVersion: \"v1\" kind: \"PersistentVolumeClaim\" metadata: name: \"ceph-pvc-test\" namespace: \"default\" spec: accessModes: - \"ReadWriteMany\" resources: requests: storage: \"2Gi\" volumeName: \"pv-nfs-test\" # 指定PV PersistentVolumesClaim的属性 Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 Volume Modes：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Resources：指定使用多大的存储空间 Selector：PVC可以指定标签选择器进行更深度的过滤PV，只有匹配了选择器标签的PV才能绑定给PVC。选择器包含两个字段： matchLabels（匹配标签） - PV必须有一个包含该值得标签 matchExpressions（匹配表达式） - 一个请求列表，包含指定的键、值的列表、关联键和值的操作符。合法的操作符包含In，NotIn，Exists，和DoesNotExist。 　　所有来自matchLabels和matchExpressions的请求，都是逻辑与关系的，它们必须全部满足才能匹配上。 Class apiVersion: v1 kind: PersistentVolume metadata: name: ceph-pv-test spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - 192.168.122.133:6789 pool: rbd image: ceph-image user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Retain claimRef: name: \"pvc-test\" namespace: \"default\" PersistentVolumes的属性 Capacity：指定存储容量大小 Volume Mode：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Class: 一个PV可以有一种class，通过设置storageClassName属性来选择指定的StorageClass。有指定class的PV只能绑定给请求该class的PVC。没有设置storageClassName属性的PV只能绑定给未请求class的PVC(过去，使用volume.beta.kubernetes.io/storage-class注解，而不是storageClassName属性。该注解现在依然可以工作，但在Kubernetes的未来版本中已经被完全弃用了) Reclaim Policy Mount Options Node Affinity Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 PersistentVolumes的周期状态 Available: 空闲的，未绑定给PVC Bound: 绑定上了某个PVC Released: PVC已经删除了，但是PV还没有被回收 Failed: PV在自动回收中失败了 PV支持的存储系统: GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CSI FC (Fibre Channel) Flexvolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) Portworx Volumes ScaleIO Volumes StorageOS PV和PVC 之间的关联遵循如下的生命周期： Provisioning-供应: PV的创建阶段，有以下两种创建方式 静态手工：集群管理员通过手工的方式创建pv 动态自动：通过PersistentVolume Controller动态调度，Kubernetes将能够按照用户的需要，根据PVC的资源请求，寻找StorageClasse定义的符合要求的底层存储自动创建其需要的存储卷。 Binding-绑定: PV分配绑定到PVC Using-使用： POD挂载使用PVC类型的Volume Reclaiming-回收：PV释放后的回收利用策略 Retain保留: 保留现场，人工回收 Delete删除: 自动删除，动态删除后端存储。需要IaaS层的支持，目前只有Ceph RBD和OpenStack Cinder支持 Recycle复用：通过rm -rf删除卷上的所有数据。目前只有NFS和HostPath支持（逐渐在抛弃该方式，建议使用） 二、使用StorageClass提供动态存储供应 通常情况下，Kubernetes集群管理员需要手工创建所需的PV存储资源。从Kubernetes 1.2以后可以使用Storageclass实现动态自动地根据用户需求创建某种存储系统类型的PV。同时，可以定义多个 StorageClass ，给集群提供不同存储系统类型的PV资源。 1. 定义创建StorageClass 每一个存储类都必须包含以下参数 provisioner: 决定由哪个Provisioner来创建PV parameters: Provisioner需要的参数,可选项：Delete(Default),Retain reclaimPolicy: PV的回收策略 可选参数： Mount Options Volume Binding Mode Allowed Topologies Note: StorageClass一旦被创建，将不能被更新 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd # 指定Provisioner provisioner: kubernetes.io/rbd parameters: monitors: 10.20.30.40:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" Kubernetes支持的Provisioner Provisioner 是否内置插件 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local StorageClas可以支持第三方的Provisioner，只要该插件符合Kubernetes的规范 内置的Provisioner名称带有“kubernetes.io”前缀 Github仓库：https://github.com/kubernetes-incubator/external-storage 有官方支持的第三方Provisioner 2. 指定StorageClass动态创建PV 在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储； 在Kubernetes v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce # 指定所使用的存储类，此存储类将会自动创建符合要求的PV storageClassName: ceph-rbd resources: requests: storage: 30Gi 3. 指定默认的StorageClass 创建StorageClass时可添加添加storageclass.kubernetes.io/is-default-class注解来指定为默认的存储类。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" # 将此storageclass设置为默认 name: nfs-client-storageclass provisioner: fuseim.pri/ifs parameters: archiveOnDelete: \"true\" 一个集群中，最多只能有一个默认的存储类 如果没有默认的存储类，在PersistentVolumeClaim中也没有显示指定storageClassName，将无法创建PersistentVolume。 参考链接 https://kubernetes.io/docs/concepts/storage/volumes/ https://kubernetes.io/docs/concepts/storage/storage-classes/ https://www.kubernetes.org.cn/4078.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-Kubernetes-provisioner-nfs-client.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-client.html","title":"NFS Client provisioner","keywords":"","body":"一、NFS Client Provisioner https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client https://www.kubernetes.org.cn/3894.html Provisioner的定义原理: openshift-Kubernetes的持久化存储 二、安装部署 1. 创建NFS服务端 yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ mkdir -p /data/nfs/appstorage-nfs-client-provisioner && \\ echo \"/data/nfs/appstorage-nfs-client-provisioner *(rw,no_root_squash,sync)\" >> /etc/exports && \\ exportfs -a && \\ showmount -e $HOSTNAME 2. 创建RBAC kind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 3. 修改Deployment并以此部署POD 先拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest quay.io/external_storage/nfs-client-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: $HOSTNAME # NFS Server的地址 - name: NFS_PATH value: /data/nfs/appstorage-nfs-client-provisioner # NFS Server要挂载的路径 volumes: - name: nfs-client-root nfs: server: $HOSTNAME #指定NFS Server的地址 path: /data/nfs/appstorage-nfs-client-provisioner #指定NFS Server要挂载的路径 三、使用 1. 创建StorageClass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. =======================================================补充内容========================================================= #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\" 删除PVC时不会保留数据，\"true\"将保留PVC的数据，形成以\"archived-\"开头的文件夹 2. 创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc #当默认storageclass就是nfs-client-storageclass，可不要该注解 annotations: volume.beta.kubernetes.io/storage-class: \"nfs-client-storageclass\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 1. 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX nfs-client-storageclass 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX Delete Bound default/test nfs-client-storageclass 10m 2. 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 3. 查看NFS目录 /data/nfs/k8s-app-nfs-storage/ └── [drwxrwxrwx 32] default-test-pvc-e8a15786-5a09-11e9-ad53-000c296286d8 ├── [-rw-r--r-- 947] 1.log └── [-rw-r--r-- 1.0K] 2.log Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-Kubernetes-provisioner-nfs-server.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-server.html","title":"NFS Server Provisioner","keywords":"","body":"一、NFS Server Provisioner Github项目地址：https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/nfs Provisioner的定义原理：openshift-Kubernetes的持久化存储 NFS Provisioner的部署文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/deployment.md NFS Provisioner的使用文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/usage.md 二、在Kubernetes上部署 1、（可选）预拉取镜像 docker pull quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest quay.io/kubernetes_incubator/nfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest 2、创建PodSecurityPolicy apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: nfs-provisioner spec: fsGroup: rule: RunAsAny allowedCapabilities: - DAC_READ_SEARCH - SYS_RESOURCE runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - secret - hostPath 3、创建RBAC kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-provisioner apiGroup: rbac.authorization.k8s.io 4、使用deployment创建POD（推荐） apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: # 定义提供者的名称，存储类通过此名称指定提供者 - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 5、（可选）使用StatefulSet创建POD apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: StatefulSet apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner serviceName: \"nfs-provisioner\" replicas: 1 template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner terminationGracePeriodSeconds: 10 containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 三、使用 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" ​ =======================================================补充内容========================================================= ​ #其他参数： gid: # \"none\" or a supplemental group like \"1001\". NFS shares will be created with permissions such that pods running with the supplemental group can read & write to the share, but non-root pods without the supplemental group cannot. Pods running as root can read & write to shares regardless of the setting here, unless the rootSquash parameter is set true. If set to \"none\", anybody root or non-root can write to the share. Default (if omitted) \"none\". rootSquash: # \"true\" or \"false\". Whether to squash root users by adding the NFS Ganesha root_id_squash or kernel root_squash option to each export. Default \"false\". mountOptions: # a comma separated list of mount options for every PV of this class to be mounted with. The list is inserted directly into every PV's mount options annotation/field without any validation. Default blank \"\". ​ #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: #注解 annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" 创建PVC时指定storageclass kind: PersistentVolumeClaim apiVersion: v1 metadata: name: nfs annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX example-nfs 5m3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX Delete Bound default/test example-nfs 5m9s 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 查看/srv目录 /srv ├── [-rw-r--r-- 4.4K] ganesha.log ├── [-rw------- 36] nfs-provisioner.identity ├── [drwxrwsrwx 32] pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 │ ├── [-rw-r--r-- 5.1K] 1.log │ └── [-rw-r--r-- 5.7K] 2.log └── [-rw------- 1.1K] vfs.conf 注意：删除掉PVC，PV也会自动删除，底层的NFS目录也会跟着删除 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-Kubernetes-provisioner-glusterfs.html":{"url":"origin/openshift-Kubernetes-provisioner-glusterfs.html","title":"Glusterfs Provisioner","keywords":"","body":"一、OKD集群中添加容器化的GlusteFS Prerequisite OKD集群（3.11）至少有三个节点 OKD官方操作指南：https://docs.okd.io/3.11/install_config/persistent_storage/persistent_storage_glusterfs.html#install-config-persistent-storage-persistent-storage-glusterfs GlusterFS官方操作指南：https://docs.gluster.org/en/latest/Administrator%20Guide/overview/ heketi-cli官方操作指南：https://github.com/heketi/heketi 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false ​ [glusterfs] allinone311.okd.curiouser.com glusterfs_devices='[ \"/dev/vdf\" ]' node1.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' node2.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' #至少是三个节点 glusterfs节点上安装软件 yum install glusterfs-fuse && \\ yum update glusterfs-fuse 配置glusterfs节点上的Selinux setsebool -P virt_sandbox_use_fusefs on && \\ setsebool -P virt_use_fusefs on 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 二、向OKD集群中添加集群外的GlusteFS 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false openshift_storage_glusterfs_is_native=false openshift_storage_glusterfs_heketi_is_native=true openshift_storage_glusterfs_heketi_executor=ssh openshift_storage_glusterfs_heketi_ssh_port=22 openshift_storage_glusterfs_heketi_ssh_user=root openshift_storage_glusterfs_heketi_ssh_sudo=false openshift_storage_glusterfs_heketi_ssh_keyfile=\"/root/.ssh/id_rsa\" ​ [glusterfs] gluster1.example.com glusterfs_ip=192.168.10.11 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster2.example.com glusterfs_ip=192.168.10.12 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster3.example.com glusterfs_ip=192.168.10.13 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 三、卸载 ansible-playbook -e \"openshift_storage_glusterfs_wipe=true\" /root/openshift-ansible/playbooks/openshift-glusterfs/uninstall.yml 四、OKD中通过storage动态使用glusterfs作为PVC的后端存储 1. 创建storage class 创建storage class(ansible playbook执行过程中会自动创建storageclass) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: 'http://heketi-storage.app-storage.svc:8080' restuser: admin secretName: heketi-storage-admin-secret secretNamespace: app-storage reclaimPolicy: Delete volumeBindingMode: Immediate 如果使用的集群外的Glusterfs集群，需要手动创建storage class。 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: \"http://10.42.0.0:8080\" restauthenabled: \"false\" 2. 创建PVC时使用Glusterfs的storage class apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gluster1 spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi storageClassName: glusterfs-storage 五、主机上mount挂载使用容器化的GlusterFS 挂载命令格式： mount -t glusterfs GlusterFS容器化pod所在的节点IP地址:/volume_name /mnt/glusterfs 示例： $ mount -t glusterfs 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 /mnt/glusterfs && \\ df -mh 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 10G 136M 9.9G 2% /mnt/glusterfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 18:37:56 "},"origin/openshift-Kubernetes-provisioner-cephfs.html":{"url":"origin/openshift-Kubernetes-provisioner-cephfs.html","title":"Ceph FileSystem Provisioner","keywords":"","body":"相关链接 官方文档： https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs Provisioner的定义原理：Kubernetes的存储--> StorageClass provisioner 姊妹篇： Preflight 1. Openshift创建cephfs命名空间 oc new-project cephfs --display-name=\"Ceph FileSystem Provisioner\" 2. 拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest quay.io/external_storage/cephfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest 一、安装部署 1. 获取Ceph Filesystem Client.admin用户的密钥环 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" 2. 创建Secrets oc create secret generic cephfs-secret-admin --from-literal=key='AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ==' --namespace=cephfs 3. 创建RBAC --- apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 4. 使用Deployment创建Ceph-FileSystem-provisioner的POD apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 二、使用 1. 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cephfs provisioner: ceph.com/cephfs parameters: monitors: allinone.okd311.curiouser.com:6789 adminId: admin adminSecretName: cephfs-secret-admin adminSecretNamespace: \"cephfs\" claimRoot: /pvc-volumes 2、创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-test spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 18:45:00 "},"origin/openshift-Kubernetes-provisioner-cephrbd.html":{"url":"origin/openshift-Kubernetes-provisioner-cephrbd.html","title":"Ceph RBD Provisioner","keywords":"","body":"一、获取ceph client admin用户的密钥环keyring 查看Ceph集群Admin节点的集群配置文件夹my-cluster下的ceph.client.admin.keyring文件来获取key值 $> cat ceph.client.admin.keyring [client.admin] key = AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA== caps mds = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" ​ $ ceph auth list #获取所有客户端用户 $ ceph auth get client.admin #获取客户端指定用户 二、使用admin的keyring在openshift上创建secret CLI $> oc create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQAil11anEPOORAArxzRkH9iS1IOGKQfK87+Ag==' --namespace=default YAML kind: Secret apiVersion: v1 metadata: name: ceph-secret namespace: default selfLink: /api/v1/namespaces/default/secrets/ceph-secret data: key: QVFDcFNlMWJ0Y3VxSFJBQWlST25zY1VDMWpnTWRwZkRJMFd0THc9PQ== type: kubernetes.io/rbd 三、创建storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd-sc provisioner: kubernetes.io/rbd parameters: monitors: 192.168.0.26:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret #说明:adminId默认值为admin,pool默认值为rbd, userId默认值与adminId一样.所以这三个值可以不填写。 四、可以在console界面创建，也可以通过PVC的YAML配置文件中指定使用Ceph $> cat ceph-rbd-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd-sc 结果如下图： Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-资源对象常见操作.html":{"url":"origin/openshift-资源对象常见操作.html","title":"常见资源对象操作","keywords":"","body":"常用资源对象操作 1、登录 oc project oc login -u 用户名 集群master的URL oc whoami #查看当前登录的用户，加-t参数可查看当前用户的token 2、切换Project oc project 3、查看集群节点 oc get node/no oc get node/no node1.test.openshift.com 4、查看集群节点的详细信息 oc describe node node1.test.openshift.com 5、查看某个节点上的所有Pods oc adm manage-node node1.test.openshift.com --list-pods 6、使节点禁止参与调度 oc adm manage-node router1.test.openshift.com --schedulable=false 7、疏散某个节点上的所有POD oc adm drain router1.test.openshift.com --ignore-daemonsets 8、清除旧的Build和Deployments历史版（所有namespace） 统计要清除的资源个数 #oc adm prune deployments --keep-younger-than=24h --keep-complete=5 --keep-failed=5|wc -l 确认清除动作 # oc adm prune [deployments|builds|images] --confirm --keep-younger-than=24h --keep-complete=5 --keep-failed=5 参数详解 --confirm 确认操作 --keep-younger-than=1h0m0s Specify the minimum age of a Build for it to be considered a candidate for pruning. --keep-complete=5 Per BuildConfig, specify the number of builds whose status is complete that will be preserved. --keep-failed=1 Per BuildConfig, specify the number of builds whose status is failed, error, or cancelled that will be preserved. --orphans=false If true, prune all builds whose associated BuildConfig no longer exists and whose status is complete, failed, error, or cancelled. 示例： 清理images（在admin用户下执行） # oc adm prune images --keep-younger-than=400m --keep-tag-revisions=10 --registry-url=docker-registry.default.svc:5000 --certificate-authority=/etc/origin/master/registry.crt --confirm 9、删除所有Namespace中非Running的pods for i in `oc get po --all-namespaces|grep -v \"Running\"|grep -v \"NAMESPACE\"|awk '{print $1}'|sort -u` ; do echo \"===================Namespace $i===================\"; oc -n $i delete po `oc get po -n $i |grep -v \"Running\"|grep -v \"NAME\"|awk 'BEGIN{ORS=\" \"}{print $1}'`; done 10、强制删除POD oc delete po gitlab-ce-16-ntzst --force --grace-period=0 11、资源的查看 #查看当前项目的所有资源 oc get all #查看当前项目的所有资源，外加输出label信息 oc get all --show-labels # 查看指定资源 oc get pod/po oc get service/svc oc get persistentvolumes/pv 12、通过label选择器删除namespace下所有的资源 #如果namespace下所有的资源都打上了“name=test”标签 oc delete all -l name=test 13、项目的管理 #创建项目 oc new-project --display-name=显示的项目名 --description=项目描述 project_name #删除项目 oc delete project 项目名 #查看当前处于哪个项目下 oc project #查看所有项目 oc projects 14、模板的管理 #创建模板(模板文件格式为YAML/JSON.也可以在Openshift的web页面上直接导入) oc create -f #查看模板 oc get templates #编辑模板 oc edit template #删除模板 oc delete template 附录 buildconfigs (aka 'bc') #构建配置 builds #构建版本 certificatesigningrequests (aka 'csr') clusters (valid only for federation apiservers) clusterrolebindings clusterroles componentstatuses (aka 'cs') configmaps (aka 'cm') daemonsets (aka 'ds') deployments (aka 'deploy') deploymentconfigs (aka 'dc') endpoints (aka 'ep') events (aka 'ev') horizontalpodautoscalers (aka 'hpa') imagestreamimages (aka 'isimage') imagestreams (aka 'is') imagestreamtags (aka 'istag') ingresses (aka 'ing') groups jobs limitranges (aka 'limits') namespaces (aka 'ns') networkpolicies nodes (aka 'no') persistentvolumeclaims (aka 'pvc') persistentvolumes (aka 'pv') poddisruptionbudgets (aka 'pdb') podpreset pods (aka 'po') podsecuritypolicies (aka 'psp') podtemplates policies projects replicasets (aka 'rs') replicationcontrollers (aka 'rc') resourcequotas (aka 'quota') rolebindings roles routes secrets serviceaccounts (aka 'sa') services (aka 'svc') statefulsets users storageclasses thirdpartyresources Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-05 10:00:52 "},"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html":{"url":"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html","title":"将Secret和ConfigMap以文件的形式挂载到容器","keywords":"","body":"将Secret和ConfigMap以文件的形式挂载到容器 一、Context ConfigMap或者Secret在默认挂载到容器是以Volumes的形式，如果挂载路径下原有的其他文件，则会覆盖掉。 如果将挂载路径直接写成文件的绝对路径，这会在挂载路径下创建以文件名为名字的文件夹，文件会在这个文件夹下 containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret 二、操作 挂载Secret或Config类型的volume时，添加一个subPath字段即可，可将其以文件的形式挂载，而不是以目录的形式。如下： containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume readOnly: true subPath: test.txt volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret secret apiVersion: v1 kind: Secret metadata: name:test-secret type: Opaque data: test.txt: >- ************************ 此时，secret中的test.txt文件将会单个文件的形式挂载到/etc/test/目录下 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-集群节点管理.html":{"url":"origin/openshift-集群节点管理.html","title":"节点管理","keywords":"","body":"一、集群添加Node节点 Ansible脚本有新增节点的Playbook脚本，准备好新增节点的基础环境，在集群的ansible管理节点上执行该Playbook就行。 　Context OKD版本 OS版本 Docker版本 Ansible版本 3.11 CentOS 7.5.1804 1.13.1 2.6.5 1. 新增节点Prerequisite 新增node节点IP地址及主机名：192.168.1.23 node6.okd.curiouser.com 开启seLinux sed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/sysconfig/selinux && \\ setenforce 1 安装docker，jdk及基础软件 yum install -y docker vim lrzsz wget unzip net-tools telnet bind-utils && \\ systemctl enable docker && \\ systemctl start docker && \\ systemctl status docker && \\ yum localinstall -y jdk-8u191-linux-x64.rpm && \\ docker info && \\ java -version 配置DNS，发现集群其他节点的IP地址与域名的映射关系.(注意DNSMasq服务端的iptables是否放行DNS的53 UDP端口) 由于集群内有DNSMasq服务端，配置/etc/resolv.conf echo \"nameserver 192.168.1.22\" >> /etc/resolv.conf && \\ ping allinone311.okd.curiouser.com Note: #DNSMasq服务端放行DNS的53 UDP端口 iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 配置Openshift的YUM源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ scp allinone311.okd.curiouser.com:/etc/yum.repos.d/all.repo /etc/yum.repos.d/ && \\ yum clean all && \\ yum makecache 2. ansible管理节点 打通ansible管理节点到新增node节点的SSH免密通道 ssh-copy-id -i root@node6.okd.curiouser.com && \\ ssh root@node1.okd.curiouser.com ansible管理节点的ansible主机清单文件inventory中添加新增节点相关信息 [OSEv3:children] ... new_nodes [new_nodes] node1.okd.curiouser.com openshift_node_group_name=\"node-config-all-in-one\" ansible管理节点执行新增节点的Ansible Playbookansible-playbook /root/openshift-ansible/playbooks/openshift-node/scaleup.yml 注意1： 当执行脚本时tower主机会把它的dnsmasq配置/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下。由于tower主机的/etc/dnsmasq.d/origin-upstream-dns.conf设置的上游DNS服务器为外网的。不希望新增节点的上游DNS服务器走外网，而是走tower主机，形成集群只有Tower主机一个节点的dns对外，其他主机作为Tower主机dns服务的客户端。所以当tower主机/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下的时候，及时修改上游dns服务器为tower主机。然后重启dnsmasq。有两个明显的坑: ① 无法重启dnsmasq，报以下错误： DBus error: Connection \":1.50\" is not allowed to own the service \"uk.org.thekelleys.dnsmasq\" due to security policies in the configuration file 解决方案：重启dbus，再重启dnsmasq systemctl restart dbus && \\ systemctl restart dnsmasq ②tower主机的iptables服务开启，dns的53端口没有放开，导致新增节点的dns无法连接上游dns服务器（即Tower主机的dns服务） 解决方案：tower主机放行dns服务的UDP 53端口。（可在新增节点尝试nslookup解析域名试一下） iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 注意2： 如果出现收集allinone节点facts超时的报错，出现一下错误提示 The full traceback is: Traceback (most recent call last): File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/basic.py\", line 2853, in run_command cmd = subprocess.Popen(args, **kwargs) File \"/usr/lib64/python2.7/subprocess.py\", line 711, in __init__ errread, errwrite) File \"/usr/lib64/python2.7/subprocess.py\", line 1308, in _execute_child data = _eintr_retry_call(os.read, errpipe_read, 1048576) File \"/usr/lib64/python2.7/subprocess.py\", line 478, in _eintr_retry_call return func(*args) File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/facts/timeout.py\", line 37, in _handle_timeout raise TimeoutError(msg) TimeoutError: Timer expired after 10 seconds TimeoutError: Timer expired after 10 seconds 请在/etc/ansible/ansible.cfg 设置\"gather_subset = !all\"或者\"gather_timeout=300\"。原因可能是已经运行allinone节点上的facts（特别是docker images layer的挂载信息）过多，造成收集facts超时，默认收集facts超时时间是10。 相关连接：https://github.com/ansible/ansible/issues/43884 二、删除节点 疏散要删除节点上的POD oc adm drain [--pod-selector=] --force=true --grace-period=-1 --timeout=5s --delete-local-data=true 删除Node oc delete node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-使用Cockpit监控集群节点的系统状态.html":{"url":"origin/openshift-使用Cockpit监控集群节点的系统状态.html","title":"节点状态监控","keywords":"","body":"一、Cockpit简介 Cockpit 是一个自由开源的服务器管理软件，它使得我们可以通过它好看的 web 前端界面轻松地管理我们的 GNU/Linux 服务器。Cockpit 使得 linux 系统管理员、系统维护员和开发者能轻松地管理他们的服务器并执行一些简单的任务，例如管理存储、检测日志、启动或停止服务以及一些其它任务。它的报告界面添加了一些很好的功能使得可以轻松地在终端和 web 界面之间切换。另外，它不仅使得管理一台服务器变得简单，更重要的是只需要一个单击就可以在一个地方同时管理多个通过网络连接的服务器。它非常轻量级，web 界面也非常简单易用。在这篇博文中，我们会学习如何安装 Cockpit 并用它管理我们的运行着 Fedora、CentOS、Arch Linux 以及 RHEL 发行版操作系统的服务器。下面是 Cockpit 在我们的 GNU/Linux 服务器中一些非常棒的功能： 它包含 systemd 服务管理器。 有一个用于故障排除和日志分析的 Journal 日志查看器。 包括 LVM 在内的存储配置比以前任何时候都要简单。 用 Cockpit 可以进行基本的网络配置。 可以轻松地添加和删除用户以及管理多台服务器。 二、Cockpit安装 所有集群节点安装cockpit并启动服务 yum install -y cockpit cockpit-docker cockpit-kubernetes ;\\ systemctl enable cockpit ;\\ systemctl start cockpit ;\\ netstat -lanp |grep 9090 iptables放行端口 vi /etc/sysconfig/iptables #-A INPUT -p tcp -m state --state NEW -m tcp --dport 9090 -j ACCEPT systemctl restart iptables Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 17:32:45 "},"origin/openshift-集群组件TLS证书管理.html":{"url":"origin/openshift-集群组件TLS证书管理.html","title":"集群组件TLS证书管理","keywords":"","body":"Openshift组件:Master、Node、Etcd、Router、Registry之间的TLS证书管理 一、安装时指定证书的有效期 默认情况下，etcd证书、openshift证书的有效期为5年，kubelet证书、私有镜像仓库registry证书、Route证书的有效期为2年。在集群安装时可以通过设置ansible/hosts中的参数来指定证书的有效期 [OSEv3:vars] openshift_hosted_registry_cert_expire_days=730 openshift_ca_cert_expire_days=1825 openshift_node_cert_expire_days=730 openshift_master_cert_expire_days=730 etcd_ca_default_days=1825 二、使用openshift的ansible playbook查看当前集群所有证书的有效期 在/etc/ansible/hosts中添加变量 [OSEv3:vars] ... openshift_is_atomic=false ansible_distribution=centos openshift_certificate_expiry_config_base=/etc/origin openshift_certificate_expiry_warning_days=30 openshift_certificate_expiry_show_all=no # 可选项 # openshift_certificate_expiry_generate_html_report=no # openshift_certificate_expiry_html_report_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.html # openshift_certificate_expiry_save_json_results=no # openshift_certificate_expiry_json_results_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.json ... 检查 $ ansible-playbook playbooks/openshift-checks/certificate_expiry/easy-mode.yaml #执行完成后可在roles/openshift_certificate_expiry/defaults/main.yml中的openshift_certificate_expiry_html_report_path变量指定路径下看到证书检查报告文件。分别是HTML格式和JSON格式的文件。 # （默认证书检查报告文件路径是：当前用户家目录下~/cert-expiry-report.时间戳.html和cert-expiry-report.时间戳.JSON）查看所有证书的过期时间 它将会展示出所有Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书的过期时间 三、更新证书 更新证书方法可以只针对Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书中的一种进行更新，也可以全部进行更新。 确保ansible/hosts中的参数有如下信息 openshift_master_cluster_hostname=master.example.com openshift_master_cluster_public_hostname=master.example.com 重新生成证书进行更新 ①全部一次性更新 ansible-playbook playbooks/redeploy-certificates.yml ②只更新master CA证书 ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml ③只更新etcd CA证书 ansible-playbook playbooks/openshift-etcd/redeploy-ca.yml ④只更新master Certificates证书 ansible-playbook playbooks/openshift-master/redeploy-certificates.yml ⑤只更新etcd Certificates证书 ansible-playbook playbooks/openshift-etcd/redeploy-certificates.yml ⑥只更新node Certificates证书 ansible-playbook playbooks/openshift-node/redeploy-certificates.yml ⑦只更新私有镜像仓库Rgistry Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-registry-certificates.yml ⑧只更新Router Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-router-certificates.yml 四、安装时使用自定义Master CA证书（以Master的CA证书为例） 将证书的路径写在inventory的配置参数中 ... [OSEv3.vars] ... openshift_master_ca_certificate={'certfile': '', 'keyfile': ''} ... 执行正常部署 ansible-playbook playbooks/deploy_cluster.yml 五、已运行的集群，更新自定义证书 同步骤四，将证书的路径写在inventory的配置参数中，运行更新Master CA证书的playbook ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml 六、更新完成后可能遇到的问题 The installer detected the wrong host names and the issue was identified too late The certificates are expired and you need to update them You have a new CA and want to create certificates using it instead allinone的集群下更新所有证书时，在重启docker那一步中，容易卡住 参考连接 https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html#advanced-install-custom-certificates https://docs.openshift.com/container-platform/3.11/install_config/redeploying_certificates.html#install-config-cert-expiry https://www.jianshu.com/p/ffc4d6369d4e Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-WebConsole定制化.html":{"url":"origin/openshift-WebConsole定制化.html","title":"定制WebConsole界面","keywords":"","body":"一、定制WebConsole中左上角的logo 制作图标 使用Windows 10自带的Paint 3D。制作高度40pixel，宽度为logo字体宽的透明画布（建议logo字体宽度为100-300pixel之间）。保存为PNG格式。 将PNG图片转成SVG格式 http://www.bejson.com/convert/image_to_svg/ 将SVG文件进行Base64加密 https://www.css-js.com/tools/base64.html 将下面CSS文件上传到一个HTTPS的静态资源服务器上 #header-logo { background-image: url('data:image/svg+xml;base64,base64加过密的SVG图片源码'); width: 230px; height: 40px; } # 参考 #header-logo{ background-image:url('data:image/svg+xml;base64,77u/PD******'); width: 230px; height: 40px; } 或者 #header-logo{ background-image: url(\"logo图片的访问UTRL（必须是HTTPS）\"); width: 300px; height: 40px; } 修改WebConsole的配置文件 待Webconsole的容器重启过后（等待约5分钟），再次刷新页面可见修改过后的效果。可使用F12调出浏览器开发者模式，查看页面渲染的元素。 二、汉化项目左侧导航栏 创建js (function() { window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[0].label=\"概览\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[1].label=\"应用\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[2].label=\"构建\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[3].label=\"资源\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[4].label=\"存储\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[5].label=\"监控\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[6].label=\"商店\" window.OPENSHIFT_CONSTANTS.APP_LAUNCHER_NAVIGATION = [ { title: \"Sharing Videos\", iconClass: \"fa fa-video-camera\", href: \"https://yun.baidu.com/s/1xIwYILHQebEHZOcW4yvsAw\", tooltip: \"一键部署Openshift相关视频\" }]; }()); 上传到https服务器上 修改WebConsole的配置文件 三、定制登陆页面 导出login模板文件 oc adm create-login-template > login.html 修改该HTML文件，然后放到master节点上的/etc/origin/master/login-template/路径下（示例可见附件） 修改Master节点的/etc/origin/master/master-config.yaml文件 oauthConfig: ... templates: login: login-template/login.html #login-template/login.html是相对于/etc/origin/master/master-config.yaml文件路径的相对位置 重启master节点上的OKD的api进程 # 使用okd3.11新命令：master-restart master-restart api Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html":{"url":"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html","title":"集群管理遇到的问题","keywords":"","body":"问题一 描述 正在运行的Openshift Allinone 3.11集群创建POD突然报出 network: failed to allocate for range 0: no IP addresses available in range set 现象 整个Allinone集群所有的POD不超多100个，但是/var/lib/cni/networks/openshift-sdn/的IP地址文件却有252个。造成当前节点的容器网络无法再为POD分配IP地址 解决方案 将对应节点标记为不可调用 oc adm manage-node node1.test.openshift.com --schedulable=false 驱散对应节点上的POD oc adm drain node1.test.openshift.com --ignore-daemonsets 停止docker和origin-node服务 systemctl stop docker origin-node.service 删除/var/lib/cni/networks/openshift-sdn/路径下所有文件 rm -rf /var/lib/cni/networks/openshift-sdn/* 重启docker和origin-node服务 systemctl start docker origin-node.service 将节点标记为可调度 oc adm manage-node node1.test.openshift.com --schedulable=true 相关链接 https://access.redhat.com/solutions/3328541 https://github.com/debianmaster/openshift-examples/issues/59 https://github.com/cloudnativelabs/kube-router/issues/383 https://github.com/jsenon/api-cni-cleanup/blob/master/k8s/deployment.yml#L42 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 18:37:53 "},"origin/openshift-openshift的用户认证.html":{"url":"origin/openshift-openshift的用户认证.html","title":"用户认证","keywords":"","body":"一、用户认证 Openshift通过OAuth进行用户的认证。在Openshift的master节点上运行着一个内置的OAuth服务对用户的请求进行认证检查。一旦OAuth服务器通过登录信息确认了用户的信息，OAuth服务器就返回用户的访问Token。通过这个Token，用户可以在有效的时间内对系统进行访问。 #登录命令 $ oc login -u 用户名 ​ #查看以哪个用户登录的 $ oc whoami $ oc whoami -t 查看当前用户当前Session的Token ​ #system:admin是集群默认的管理员，该用户是一个特殊用户，它不能通过用户名密码登录，它也没有Token。 作为身份验证的登录信息，如用户名密码，并非保存在Openshift集群中，而是保存在用户信息管理系统中，这些用户信息管理系统在Openshift中被称为Identity Provider。但Openshift并不提供用户信息管理系统，而是提供了不同的适配器连接不同的用户信息管理系统。通过配置，Openshift可以连接到以下用户信息管理系统： LADP（Lightweight Directory Access Protocol） 微软的活动目录（Active Directory） AllowALL DenyAll HTPasswd Github #查看当前Openshift集群支持的用户信息管理系统 cat /etc/origin/master/master-config.yaml|grep provider -A 3 provider: apiVersion: v1 file: /etc/origin/master/htpasswd kind: HTPasswdPasswordIdentityProvider #Htpasswd是Apache提供的一个基于文本文件管理用户名密码的用户信息管理工具 Openshift的用户管理，在后台创建用户时，会同时创建一个User对象和Identity对象（该对象保存了用户来源哪一个Identity Provider及用户信息）。 #查看集群中所有用户 $oc get user NAME UID FULL NAME IDENTITIES admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:admin dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 htpasswd_auth:dev #查看用户的Identity对象 $oc get identity NAME IDP NAME IDP USER NAME USER NAME USER UID htpasswd_auth:admin htpasswd_auth admin admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:dev htpasswd_auth dev dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 Openshift的用户组管理。用户组的信息来源有两个：一个是Identity Provider，二是通过用户在Openshift中定义的。 #通过oadm groups命令在Openshift中对组及组成员进行管理 $> oadm groups #添加用户到用户组 $> oadm groups add-users group_name user_name #查看用户组 $> oc get group #创建用户组 $> oadm groups new group_name #删除组 $> oc delete group group_name 二、用户权限管理 用户角色权限管理 授予及撤销用户某种角色 oc policy add-role-to-user view test oc policy remove-role-from-user view test 查看项目的角色绑定关系 oc get rolebinding -n 项目名 授予某用户对某项目的某角色 oc policy add-role-to-user view test -n test 查看角色绑定的规则 oc describe clusterrole registry-viewer 用户管理 新增用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd test test\" 查看已创建的用户 oc get user 或 cat /etc/origin/master/htpasswd 删除用户 oc delete user test ansible masters -m shell -a \"htpasswd -D /etc/origin/master/htpasswd ha\" 用户组管理 创建用户组、添加用户到用户组 oc adm groups new test oc adm groups add-users test 用户1 用户2 用户3 查看创建的用户组及组内的成员用户 oc get group 删除用户组 oc delete group test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 20:41:50 "},"origin/openshift-openshift用户权限管理实例.html":{"url":"origin/openshift-openshift用户权限管理实例.html","title":"用户权限管理实例","keywords":"","body":"Openshift用户权限管理实例 由于公司的日常项目开发测试环境都迁移到openshift上了。有众多开发测试人员需要登陆到openshift上进行操作，如果直接给admin权限，肯定是不行的。而openshift是支持多租户的权限管理。所以，就在创建普通用户的基础上赋予各种不同的权限限制来自控制对openshift上project的操作。 一、Prerequisite 开发人员对CI环境有操作权限，对SIT、UAT环境只有查看权限 测试人员对SIT环境有操作权限，对CI环境只有查看权限 所有人员有自己的登录账户，均可见openshift上所有的业务项目，不可见系统项目 二、实现过程 创建登录用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev1 dev1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev2 dev2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev3 dev3\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester1 tester1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester2 tester2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester3 tester3\" 创建用户组 oc adm groups new developer oc adm groups new tester 将用户添加到用户组中 oc adm groups add-users developer dev1 dev2 dev3 oc adm groups add-users tester tester1 tester2 tester3 针对项目，给用户组赋予系统角色 oc adm policy add-role-to-group edit developer -n aci oc adm policy add-role-to-group edit developer -n bci oc adm policy add-role-to-group edit developer -n cci oc adm policy add-role-to-group view developer -n asit oc adm policy add-role-to-group view developer -n auat oc adm policy add-role-to-group view developer -n bsit oc adm policy add-role-to-group view developer -n buat oc adm policy add-role-to-group view developer -n csit oc adm policy add-role-to-group view developer -n cuat ​ oc adm policy add-role-to-group edit tester -n asit oc adm policy add-role-to-group edit tester -n auat oc adm policy add-role-to-group edit tester -n bsit oc adm policy add-role-to-group edit tester -n buat oc adm policy add-role-to-group edit tester -n csit oc adm policy add-role-to-group edit tester -n cuat oc adm policy add-role-to-group view tester -n aci oc adm policy add-role-to-group view tester -n bci oc adm policy add-role-to-group view tester -n cci 实际操作过程中，在以某以开发人员登录过程openshift过程中，依旧会看到openshift 其他一些项目的namespace。例如base namespace，该namespace项目是在registry镜像注册仓库中创建镜像项目时自动创建的openshift namespace（在registry镜像注册仓库中创建base镜像项目是为了存放一些自定义的s2i镜像）。为了使其他openshift namespace使用其中的s2i镜像，特别在registry镜像注册仓库中是镜像项目的访问策略设置为共享的。种种以上，导致openshift上的base namespace是能被所有的已认证的用户查看到。 在openshift中查看base项目的membership 可以发现，凡是在registry镜像注册仓库中设置问访问策略设置为共享的，都会在openshift 项目中添加一个系统用户system:authenticated 。这个系统用户上绑定的是这个角色registry-viewer。在openshift后台查看该角色的详细信息 # oc describe clusterrole registry-viewer Name: registry-viewer Created: About an hour ago Labels: Annotations: authorization.openshift.io/system-only=true openshift.io/reconcile-protect=false Verbs Non-Resource URLs Resource Names API Groups Resources [get list watch] [] [] [ image.openshift.io] [imagestreamimages imagestreammappings imagestreams imagestreamtags] [get] [] [] [ image.openshift.io] [imagestreams/layers] [get] [] [] [] [namespaces] [get] [] [] [project.openshift.io ] [projects] 发现该角色有对namespace资源拥有get动作。仔细想想，该system:authenticated用户只是让openshift其他项目的系统用户能够拉取其下的镜像流。而在Kubernetes中使用命名空间的概念来分隔资源。在同一个命名空间中，某一个对象的名称在其分类中必须唯一，但是分布在不同命名空间中的对象则可以同名。OpenShift中继承了Kubernetes命名空间的概念，而且在其之上定义了Project对象的概念。每一个Project会和一个Namespace相关联，甚至可以简单地认为，Project就是Namespace。所以，该用户对project资源有获取权限，那就把对namespace的权限给去掉试试。 先导出角色registry-viewer的bindding配置文件 oc export clusterrole registry-viewer > registry-viewer.yml 然后修改配置文件，注释掉get namespace的动作 apiVersion: v1 kind: ClusterRole metadata: annotations: authorization.openshift.io/system-only: \"true\" openshift.io/reconcile-protect: \"false\" creationTimestamp: null name: registry-viewer rules: - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreamimages - imagestreammappings - imagestreams - imagestreamtags verbs: - get - list - watch - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreams/layers verbs: - get #- apiGroups: # - \"\" # attributeRestrictions: null # resources: # - namespaces # verbs: # - get - apiGroups: - project.openshift.io - \"\" attributeRestrictions: null resources: - projects verbs: - get 再将集群中角色删掉（此时特别注意:从集群中删掉registry-viewer角色后会导致已有镜像注册仓库中镜像的访问策略从共有变成私有,base项目的membership中会删掉system:authenticated该用户） oc delete clusterrole registry-viewer 接着再从配置文件中创建角色 oc create -f registry-viewer.yml 最后再次修改镜像注册仓库中镜像的访问策略从私有变成共有。再次查看base项目中membership. 最有再以测试人员账户登录查看。不再显示base项目。测试其他项目去拉取base项目中的镜像，看去掉registry-viewer角色中role是否有影响。 实际使用过程中，测试人员需要以openshift上的用户名密码登录openshift上的jenkins，还要对jenkins做操作，比如在jenkins上做构建操作，查看构建日志等。需要对测试人员分组tester赋予对jenkins的编辑权限。初步思路是直接给tester分组服务系统角色clusterrole edit（oc adm policy add-cluster-role-to-group edit tester）。但是再以测试人员登录时还是能看到jenkins的项目，甚至能操作openshift上jenkins pod的重新部署。这是不可接受的。 那就换个思路。自己创建一个集群角色clusterrole，在角色上绑定若干规则，再将这个集群角色赋予测试组，相应的测试组成员能登录jenkins，并对jenkins做操作。 具体过程如下： 先查看集群角色edit的配置，看edit都对那些资源都有什么动作 oc describe clusterrole edit ​ Name: edit Created: 7 months ago Labels: Annotations: openshift.io/description=A user that can create and edit most objects in a project, but can not update the project's membership. Verbs Non-Resource URLs Resource Names API Groups Resources [create delete deletecollection get list patch update watch] [] [] [] [pods pods/attach pods/exec pods/portforward pods/proxy] [create delete deletecollection get list patch update watch] [] [] [] [configmaps endpoints persistentvolumeclaims replicationcontrollers replicationcontrollers/scale secrets serviceaccounts services services/proxy] [get list watch] [] [] [] [bindings events limitranges namespaces namespaces/status pods/log pods/status replicationcontrollers/status resourcequotas resourcequotas/status] [impersonate] [] [] [] [serviceaccounts] [create delete deletecollection get list patch update watch] [] [] [autoscaling] [horizontalpodautoscalers] [create delete deletecollection get list patch update watch] [] [] [batch] [cronjobs jobs scheduledjobs] [create delete deletecollection get list patch update watch] [] [] [extensions] [deployments deployments/rollback deployments/scale horizontalpodautoscalers jobs replicasets replicasets/scale replicationcontrollers/scale] [get list watch] [] [] [extensions] [daemonsets] [create delete deletecollection get list patch update watch] [] [] [apps] [deployments deployments/scale deployments/status statefulsets] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildconfigs buildconfigs/webhooks builds] [get list watch] [] [] [build.openshift.io ] [builds/log] [create] [] [] [build.openshift.io ] [buildconfigs/instantiate buildconfigs/instantiatebinary builds/clone] [update] [] [] [build.openshift.io ] [builds/details] [edit view] [] [] [build.openshift.io] [jenkins] [create delete deletecollection get list patch update watch] [] [] [apps.openshift.io ] [deploymentconfigs deploymentconfigs/scale generatedeploymentconfigs] [create] [] [] [apps.openshift.io ] [deploymentconfigrollbacks deploymentconfigs/instantiate deploymentconfigs/rollback] [get list watch] [] [] [apps.openshift.io ] [deploymentconfigs/log deploymentconfigs/status] [create delete deletecollection get list patch update watch] [] [] [image.openshift.io ] [imagestreamimages imagestreammappings imagestreams imagestreams/secrets imagestreamtags] [get list watch] [] [] [image.openshift.io ] [imagestreams/status] [get update] [] [] [image.openshift.io ] [imagestreams/layers] [create] [] [] [image.openshift.io ] [imagestreamimports] [get] [] [] [project.openshift.io ] [projects] [get list watch] [] [] [quota.openshift.io ] [appliedclusterresourcequotas] [create delete deletecollection get list patch update watch] [] [] [route.openshift.io ] [routes] [create] [] [] [route.openshift.io ] [routes/custom-host] [get list watch] [] [] [route.openshift.io ] [routes/status] [create delete deletecollection get list patch update watch] [] [] [template.openshift.io ] [processedtemplates templateconfigs templateinstances templates] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildlogs] [get list watch] [] [] [] [resourcequotausages] 导出集群角色edit的配置文件到本地文件，在其上做修改 oc export clusterrole edit > jenkins-clusterrole.yml 编辑 jenkins-clusterrole.yml（只保留相重要的，其他的都删掉） apiVersion: v1 kind: ClusterRole metadata: annotations: openshift.io/description: A user that can view jenkins project, and edit jenkins job. #添加clusterrole角色的说明简介 creationTimestamp: null name: jenkins #修改clusterrole名字为jenkins rules: - apiGroups: - \"\" attributeRestrictions: null resources: #clusterrole edit中有好多对其他资源的操作。例persistentvolumeclaims、replicationcontrollers、replicationcontrollersscale。对这些资源没有什么用处，就可以删掉啦。 - configmaps - endpoints - secrets - serviceaccounts - services - services/proxy verbs: - get - list - apiGroups: - \"\" attributeRestrictions: null resources: - serviceaccounts verbs: - impersonate - apiGroups: - build.openshift.io attributeRestrictions: null resources: - jenkins verbs: - edit - view 导入jenkins clusterrole oc create -f jenkins-clusterrole.yml 将jenkins clusterrole赋予测试组 oc adm policy add-cluster-role-to-group jenkins tester 以测试组成员登录openshift，jenkins项目不可见了。再以测试组成员登录jenkins，发现登录 出现以下界面 点击\"Allow selected permissions\"，发现也能正常登录jenkins。然后进行一次构建触发。发现一切正常。Bazinga！Everything is ok ! Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-开启router的haproxy-statisc.html":{"url":"origin/openshift-开启router的haproxy-statisc.html","title":"openshift开启router的haproxy-statisc","keywords":"","body":" 设置router POD 所在节点的iptables对1936端口的放行 iptables -I OS_FIREWALL_ALLOW -p tcp -m tcp --dport 1936 -j ACCEPT 获取访问router haproxy statics 页面的用户名密码。 删除掉router dc中的环境变量”ROUTER_METRICS_TYPE“ 这个环境变量默认值为“haproxy”。不删除的话，访问的时候会报一下错误 Forbidden: User \"system:anonymous\" cannot get routers/metrics.route.openshift.io at the cluster scope 将健康检查readiness的HTTP GET URL由“/healthz/ready”改为\"/healthz\"。（不然router POD无法通过健康检查） 验证监听端口80，443，1936 ss -ntl|grep 80 ss -ntl|grep 443 ss -ntl|grep 1936 访问router haproxy statistics 页面。 访问方式是：http://:@router所在节点IP地址:1936 例如：http://admin:MJbJFvODhP@allinone.curiouser.com:1936 相关链接 https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#using-wildcard-routes https://bugzilla.redhat.com/show_bug.cgi?id=1579054 https://github.com/openshift/origin/issues/17025 https://blog.chmouel.com/2016/09/27/how-to-view-openshift-haproxy-stats/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-多租户网络.html":{"url":"origin/openshift-多租户网络.html","title":"openshift的多租户网络","keywords":"","body":"一、Openshift容器网络简介 Openshift容器网络默认是基于Open vSwitch（OVS）实现的。 Openshift提供两种网络方案： ovs-subnet(子网模式)：为集群节点上的容器提供一个扁平化的二层虚拟网路，所有在这个二层网路中容器可直接通信。 ovs-multitenet(多租户模式)：基于项目的网络隔离，即不同项目间的容器之间不能直接通信。启动多租户网络隔离后，每个项目创建后都会被分配一个虚拟网络ID（Virtual Network ID ,VNID）.OVS网桥会为该项目的所有数据流量标记上VNID，在默认情况下，只有数据包上的VNID与目标容器所在项目的VNID匹配上后，数据包才允许被转发到目标容器中。当有些项目的容器应用是通过公共服务的，后期可通过配置将多个项目见的网络连通，或者将项目设置为全局可访问。 二、启动多租户网络 需要将集群中所有的master节点配置文件/etc/origin/master/master-config.yaml和node节点配置文件/etc/origin/node/node-config.yaml中的networkPluginName的属性值从redhat/openshift-ovs-subnet修改为redhat/openshift-ovs-multitenant，然后重启Openshift集群Master节点的origin-master-controllers.service服务和Node节点的origin-node.service服务 三、测试，查看网络隔离 在一个项目中的一个pod的终端中ping/telnet/curl/nslook另一个项目中的pod的ip地址或者对应svc的FQDN（..svc.cluster.local） 查看namespace的Netid是否一致 $ oc get netnamespaces NAME NETID EGRESS IPS default 0 [] kube-public 5899696 [] kube-service-catalog 0 [] demo 13843039 [] dubbo 11344186 [] jenkins 13843039 [] 当NETID相同时，表示这个两个project的网络是相通的 当NETID为0时，表示这个Project的网络全局可访问 四、连通隔离的网络 # project 1,2,3中所有的pod，service可以通过容器IP相互访问（通过service的FQDN不能相互访问） oc adm pod-network join-projects --to= #将某个project中所有的pod和service设置为全局可访问 oc adm pod-network make-projects-global 参考链接 https://docs.okd.io/3.11/admin_guide/managing_networking.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-kubernetes的审计日志功能.html":{"url":"origin/openshift-kubernetes的审计日志功能.html","title":"Kubernetes的审计日志功能","keywords":"","body":"一、Overviews kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题： 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？ kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有： RequestReceived ：apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted ：在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete ：当响应 body 发送完并且不再发送数据。 Panic：内部服务器出错，请求未完成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成 日志记录级别，当前支持的日志记录级别有： None: 符合这条规则的日志将不会记录。 Metadata: 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request: 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse: 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。 输出的审计日志格式 json{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1beta1\", \"metadata\": { \"creationTimestamp\": \"2019-07-23T09:02:19Z\" }, \"level\": \"Request\", \"timestamp\": \"2019-07-23T09:02:19Z\", \"auditID\": \"eb481add-fdac-48a3-a302-1c33d73bfdbf\", \"stage\": \"RequestReceived\", \"requestURI\": \"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\", \"verb\": \"update\", \"user\": { \"username\": \"system:openshift-master\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.1.96\" ], \"objectRef\": { \"resource\": \"configmaps\", \"namespace\": \"kube-system\", \"name\": \"openshift-master-controllers\", \"apiVersion\": \"v1\" }, \"requestReceivedTimestamp\": \"2019-07-23T09:02:19.148057Z\", \"stageTimestamp\": \"2019-07-23T09:02:19.148057Z\" } legacy 2019-07-23T23:50:06.223368641+08:00 AUDIT: id=\"3574e2e0-06b1-44d8-bc6c-5983c402d55e\" stage=\"ResponseComplete\" ip=\"192.168.1.96\" method=\"update\" user=\"system:openshift-master\" groups=\"\\\"system:masters\\\",\\\"system:authenticated\\\"\" as=\"\" asgroups=\"\" namespace=\"kube-system\" uri=\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\" response=\"200\" 审计后端可以将审计事件导出到外部存储。 Kube-apiserver 提供两个后端： Log 后端: 将事件写入到磁盘 Webhook 后端: 将事件发送到外部 API Note: 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 二、openshift开启自定义策略的审计功能 创建审计日志的存储路径 mkdir /etc/origin/master/audit # 注意：审计日志文件的存储路径必须是kube-system命名空间下apiservser pod挂载目录下的子路径。 # ocp 3.11版本的apiserver是以pod的形式运行在kube-system命名空间下的，它所需要的配置文件等Volume资源都是以hostpath的形式挂载上去的，例如ocp节点上的/etc/origin/master目录 编辑/etc/origin/master/master-config.yaml ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 ****省略******** 创建自定义的审计策略配置文件 kind: Policy omitStages: - \"ResponseStarted\" rules: - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"configmaps\",\"secrets\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the metadata level. - level: None verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the request level. - level: None resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # Log login failures from the web console or CLI. Review the logs and refine your policies. - level: Metadata nonResourceURLs: - /login* - /oauth* - level: Metadata userGroups: [\"system:authenticated:oauth\"] verbs: [\"create\", \"delete\"] resources: - group: \"project.openshift.io\" resources: [\"projectrequests\", \"projects\"] omitStages: - RequestReceived 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 三、使用openshift集群的Fluentd收集审计日志到集群内的elasticsearch 配置OCP集群中的Fluentd挂载审计日志的存储目录(OCP集群中日志系统的fluentd是以DaemonSet形式收集节点上容器的日志到elasticsearch的，它是将节点的/var/lib/docker目录以hostpath形式挂载到容器中的) oc set volume ds/logging-fluentd --add --mount-path=/etc/origin/master/audit --name=audit --type=hostPath --path=/etc/origin/master/audit -n openshift-logging 配置OCP集群中的Fluentd监控审计日志目录下的日志 oc edit cm/logging-fluentd -n openshift-logging *****省略******* ## sources *****省略******* @include configs.d/user/input-audit.conf *****省略******* input-audit.conf: | @type tail @id audit-ocp path /etc/origin/master/audit/audit-ocp.log pos_file /etc/origin/master/audit/audit.pos tag audit.requests format json @type copy @type elasticsearch log_level debug host \"#{ENV['OPS_HOST']}\" port \"#{ENV['OPS_PORT']}\" scheme https ssl_version TLSv1_2 index_name .audit user fluentd password changeme client_key \"#{ENV['OPS_CLIENT_KEY']}\" client_cert \"#{ENV['OPS_CLIENT_CERT']}\" ca_file \"#{ENV['OPS_CA']}\" type_name com.redhat.ocp.audit reload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'false'}\" reload_after \"#{ENV['ES_RELOAD_AFTER'] || '100'}\" sniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::ElasticsearchSimpleSniffer'}\" reload_on_failure false flush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '5s'}\" max_retry_wait \"#{ENV['ES_RETRY_WAIT'] || '300'}\" disable_retry_limit true buffer_type file buffer_path '/var/lib/fluentd/buffer-output-es-auditlog' buffer_queue_limit \"#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }\" buffer_chunk_limit \"#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }\" buffer_queue_full_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}\" request_timeout 2147483648 *****省略******* 重启Fluentd oc delete po -l component=fluentd -n openshift-logging 在ocp集群系统的Kibana上添加\".audit*\"的Index Pattern,并在\"Discover\"查看、筛选审计日志 四、将审计日志通过WebHook 发送到OCP外部的Logstash或者Fluentd 接受后端 可使用Logstash或者Fluentd作为后端来接受Api-Server通过web hook方式发送的审计日志。Logstash和Fluentd可以是ocp集群外二进制方式安装运行的，也可以是原生Docker运行的，甚至可以是另外一个集群中容器化的。一个原则就是不要放到审计日志产生集群的内部。防止apiserver启动起来了，有了一些操作，logstash还没有启动起来，丢失审计日志。再者审计日志后端最好选择适合自己的，审计日志落一份，重复记录也没多大意义。 方式一：使用OCP集群外二进制方式安装的Logstash来接受ApiServer通过web hook方式发送过来的审计日志并过滤、存储到本地文件中 安装logstash bash -c 'cat > /etc/yum.repos.d/elasticsearch.repo 设置logstash，/etc/logstash/logstash.yml # ------------ Pipeline Configuration Settings -------------- # Where to fetch the pipeline configuration for the main pipeline path.config: /etc/logstash/conf.d/ *************************省略****************************** # ------------ Data path ------------------ # Which directory should be used by logstash and its plugins for any persistent needs. Defaults to LOGSTASH_HOME/data path.data: /data/logs/logstash/data/ *************************省略****************************** # ------------ Debugging Settings ------------- # Options for log.level: fatal/error/warn/info (default)/debug/trace log.level: info path.logs: /data/logs/logstash/logs 创建监听HTTP 8081端口的pipeline cat /etc/logstash/conf.d/accept-audit-log.conf input{ http{ host => \"0.0.0.0\" port => 8081 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=>[items] # We only need event subelement, remove others. remove_field=>[headers, metadata, apiVersion, kind, \"@version\", host] } mutate{ rename => {items=>event} } } output{ file{ # Audit events from different users will be saved into different files. path=>\"/data/logs/logstash/ocp-audit-logs/ocp-audit-%{[event][user][username]}/audit-%{+YYYY-MM-dd}.log\" } } EOF 启动logstash mkdir -p /data/logs/logstash/{data,logs,ocp-audit-logs} chown -R logstash:logstash /data/logs/logstash system start logstash # 或者 /usr/share/logstash/bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/ 测试logstash的联通性。一是看logstash pipeline监听的HTTP端口是否开启。二是尝试发送一个带有模拟数据的POST请求，看其是否会pipeline指定的数据目录生成日志文件 ss -ntl |grep 8081 curl -X POST \\ http://192.168.1.96:8081 \\ -H 'Accept: */*' \\ -H 'Cache-Control: no-cache' \\ -H 'Connection: keep-alive' \\ -H 'Content-Type: application/json' \\ -H 'accept-encoding: gzip, deflate' \\ -d '{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1beta1\",\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:27:54Z\"},\"level\":\"Request\",\"timestamp\":\"2019-07-23T14:27:54Z\",\"auditID\":\"29bf32ba-4bea-4b4f-a1fb-cd091b2188ff\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"verb\":\"update\",\"user\":{\"username\":\"system:openshift-master\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"resource\":\"configmaps\",\"namespace\":\"kube-system\",\"name\":\"openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"apiVersion\":\"v1\",\"resourceVersion\":\"8285989\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ConfigMap\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-master-controllers\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"resourceVersion\":\"8285989\",\"creationTimestamp\":\"2019-03-09T11:31:08Z\",\"annotations\":{\"control-plane.alpha.kubernetes.io/leader\":\"{\\\"holderIdentity\\\":\\\"allinone.okd311.curiouser.com\\\",\\\"leaseDurationSeconds\\\":15,\\\"acquireTime\\\":\\\"2019-03-09T11:31:01Z\\\",\\\"renewTime\\\":\\\"2019-07-23T14:27:54Z\\\",\\\"leaderTransitions\\\":0}\"}}},\"requestReceivedTimestamp\":\"2019-07-23T14:27:54.894767Z\",\"stageTimestamp\":\"2019-07-23T14:27:54.899643Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}}' 创建audit的webhook配置文件/etc/origin/master/audit-policy.yaml cat /etc/origin/master/audit-policy.yaml apiVersion: v1 clusters: - cluster: server: http://192.168.1.96:8081 name: logstash contexts: - context: cluster: logstash user: \"\" name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF 编辑/etc/origin/master/master-config.yaml，添加webhook相关的参数 ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 #==========以下配置项为添加的webhook参数=========================================================================== webHookKubeConfig: /etc/origin/master/audit-webhook-config.yaml # 指定WebHook的配置文件（同样路径要指定在ApiServer POD已挂载的路径下） webHookMode: batch # 可选参数\"batch\"和\"blocking\" ****省略******** 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 验证，用除\"system:admin\"用户外的其他用户创建project，然后再删除project，最后查看logstash配置的审计日志存储目录下是否生成对应的文件 oc login -u admin -p oc new-project test oc delete project test $ tree -L 2 /data/logs/logstash/ocp-audit-logs/ /data/logs/logstash/ocp-audit-logs/ ├── ocp-audit-admin │ └── audit-2019-07-23.log └── ocp-audit-system:openshift-master └── audit-2019-07-23.log $ cat /data/logs/logstash/ocp-audit-logs/ocp-audit-admin/audit-2019-07-23.log 产生以下内容。显示一次创建成功，另一次创建失败，原因是project已经存在（特意测试），一次删除project等日志。 {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:03Z\"},\"stageTimestamp\":\"2019-07-23T14:45:03.019249Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:02Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:02.964347Z\",\"auditID\":\"eec24884-b70a-4b27-80c1-431111d2f4f5\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:10Z\"},\"stageTimestamp\":\"2019-07-23T14:45:10.842999Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:10Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"reason\":\"AlreadyExists\",\"code\":409},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:10.836487Z\",\"auditID\":\"a7b64f47-94eb-4723-b101-24e112cd0735\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"self-provisioners\\\" of ClusterRole \\\"self-provisioner\\\" to Group \\\"system:authenticated:oauth\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:19Z\"},\"stageTimestamp\":\"2019-07-23T14:45:19.813911Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:19Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projects\",\"namespace\":\"test\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Success\",\"code\":200},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:19.805940Z\",\"auditID\":\"9d3260ca-3bff-49da-9fe9-346043a29991\",\"verb\":\"delete\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projects/test\"}} 方式二：复用ocp集群内日志系统的Fluentd接受ApiServer通过web hook方式发送过来的审计日志并过滤、存储到挂载的文件中 整得太晚了，后续更新。 五、审计策略配置详解 整得太晚了，后续更新。 参考链接 https://austindewey.com/2018/10/17/integrating-advanced-audit-with-aggregated-logging-in-openshift-3-11/#test-it-outhttps://www.outcoldsolutions.com/docs/monitoring-openshift/v4/audit/https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-config-advanced-audithttps://docs.openshift.com/container-platform/3.11/security/monitoring.htmlhttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6chttps://www.jianshu.com/p/8117bc2fb966https://cloud.google.com/kubernetes-engine/docs/concepts/audit-policy?hl=zh-cnhttps://github.com/rbo/openshift-examples/tree/master/efk-auditloghttps://github.com/openshift/origin-aggregated-logging/issues/1226 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/openshift-elasticsearch容器化部署.html":{"url":"origin/openshift-elasticsearch容器化部署.html","title":"Elasticsearch容器化部署","keywords":"","body":"一、拉取镜像 docker pull docker.io/elasticsearch/elasticsearch:6.6.1 #或者 docker pull docker.elastic.co/elasticsearch/elasticsearch:6.6.1 二、Docker部署 修改系统 echo \"vm.max_map_count=262144\" >> /etc/sysctl.conf sysctl -w vm.max_map_count=262144 Docker单节点部署 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.io/elasticsearch/elasticsearch:6.6.1 Docker compose集群部署 version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch2 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnet volumes: esdata1: driver: local esdata2: driver: local networks: esnet: 三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: elasticsearch name: elasticsearch spec: replicas: 1 selector: app: elasticsearch deploymentconfig: elasticsearch strategy: type: Recreate template: metadata: labels: app: elasticsearch deploymentconfig: elasticsearch spec: containers: - env: - name: discovery.type value: single-node - name: cluster.name value: curiouser - name: bootstrap.memory_lock value: 'true' - name: path.repo value: /usr/share/elasticsearch/snapshots-repository - name: TZ value: Asia/Shanghai - name: ES_JAVA_OPTS value: '-Xms1g -Xmx2g' - name: xpack.monitoring.collection.enabled value: 'true' - name: xpack.security.enabled value: 'true' - name: ELASTIC_USERNAME value: \"elastic\" - name: \"ELASTIC_PASSWORD\" value: \"elastic\" image: 'docker.elastic.co/elasticsearch/elasticsearch:7.1.1' imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 name: elasticsearch ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 80 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 resources: limits: cpu: '2' memory: 3Gi requests: cpu: '1' memory: 2Gi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-data - mountPath: /usr/share/elasticsearch/snapshots-repository name: elasticsearch-snapshots-repository dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: elasticsearch name: elasticsearch spec: ports: - name: 9200-tcp port: 9200 protocol: TCP targetPort: 9200 - name: 9300-tcp port: 9300 protocol: TCP targetPort: 9300 selector: deploymentconfig: elasticsearch sessionAffinity: None type: ClusterIP 数据目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi snapshot repository存储目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-snapshots-repository spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 四. Kubernetes部署 Deployment kind: Deployment apiVersion: apps/v1 metadata: labels: elastic-app: elasticsearch role: master name: elasticsearch-master namespace: elk spec: replicas: 1 revisionHistoryLimit: 10 strategy: type: Recreate selector: matchLabels: elastic-app: elasticsearch role: master template: metadata: labels: elastic-app: elasticsearch role: master spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository && chown -R 1000.0 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository'] volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data - name: elasticsearch-snapshots-repository mountPath: /usr/share/elasticsearch/snapshots-repository containers: - name: elasticsearch-master-data image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: \"cluster.name\" value: \"Curiouser\" - name: \"bootstrap.memory_lock\" value: \"false\" - name: discovery.type value: single-node - name: \"node.master\" value: \"true\" - name: \"node.data\" value: \"true\" - name: \"node.ingest\" value: \"false\" - name: xpack.monitoring.collection.enabled value: \"true\" - name: \"xpack.monitoring.elasticsearch.collection.enabled\" value: \"true\" - name: \"xpack.security.enabled\" value: \"true\" - name: \"path.repo\" value: \"/usr/share/elasticsearch/snapshots-repository\" - name: \"ES_JAVA_OPTS\" value: \"-Xms2048m -Xmx2048m\" - name: TZ value: Asia/Shanghai - name: \"xpack.monitoring.exporters.my_local.type\" value: \"local\" - name: \"xpack.monitoring.exporters.my_local.use_ingest\" value: \"false\" resources: requests: memory: \"2Gi\" cpu: \"2\" limits: memory: \"4096Mi\" cpu: \"3\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 volumeMounts: - name: elasticsearch-data mountPath: \"/usr/share/elasticsearch/data\" - name: elasticsearch-snapshots-repository mountPath: \"/usr/share/elasticsearch/snapshots-repository\" restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository PersistentVolumeClaim --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-data namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-snapshots-repository namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi Service kind: Service apiVersion: v1 metadata: labels: elastic-app: elasticsearch-service name: elasticsearch namespace: elk spec: ports: - port: 9200 targetPort: 9200 protocol: TCP selector: elastic-app: elasticsearch type: ClusterIP Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/openshift-Kibana容器化部署.html":{"url":"origin/openshift-Kibana容器化部署.html","title":"Kibana容器化部署","keywords":"","body":"一、拉取镜像 docker pull docker.io/kibana/kibana:6.6.1 #或者 docker pull docker.elastic.co/kibana/kibana:6.6.1 二、Docker部署 单机Docker部署 docker run -p 5601:5601 \\ -e \"ELASTICSEARCH_HOST=http://ElasticSearch_HostIP:9200\" \\ -e \"SERVER_NAME=Curiouser\" \\ docker.elastic.co/kibana/kibana:6.6.1 三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: kibana name: kibana spec: replicas: 1 selector: app: kibana deploymentconfig: kibana strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: kibana deploymentconfig: kibana spec: containers: - env: - name: ELASTICSEARCH_USERNAME value: kibana - name: ELASTICSEARCH_PASSWORD value: uLAWAfW1b7UHZdHEigCW - name: TZ value: Asia/Shanghai image: docker.elastic.co/kibana/kibana:7.1.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 name: kibana ports: - containerPort: 5601 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 resources: limits: cpu: \"1\" memory: 1500Mi requests: cpu: 500m memory: 800Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: kibana name: kibana spec: ports: - name: 5601-tcp port: 5601 protocol: TCP targetPort: 5601 selector: deploymentconfig: kibana sessionAffinity: None type: ClusterIP Route apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: apache-kibana name: kibana spec: port: targetPort: 5601-tcp to: kind: Service name: kibana weight: 100 wildcardPolicy: None 四. Kubernetes上部署 Deployment apiVersion: apps/v1beta2 kind: Deployment metadata: labels: app: kibana name: kibana namespace: elk spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: kibana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: kibana spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com containers: - image: kibana/kibana:7.2.0 imagePullPolicy: IfNotPresent name: kibana envFrom: - secretRef: name: kibana-config-env env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOSTS value: '[\"http://elasticsearch.elk.svc:9200\"]' ports: - containerPort: 5601 name: web protocol: TCP resources: requests: memory: \"1Gi\" cpu: \"0.5\" limits: memory: \"2Gi\" cpu: \"1\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: {} privileged: false procMount: Default readOnlyRootFilesystem: false runAsNonRoot: false stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsConfig: {} dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Secret apiVersion: v1 kind: Secret metadata: labels: app: kibana name: kibana-config-env namespace: elk stringData: ELASTICSEARCH_USERNAME: kibana ELASTICSEARCH_PASSWORD: kibana Service apiVersion: v1 kind: Service metadata: name: kibana namespace: elk labels: app: kibana spec: ports: - port: 5601 name: web selector: app: kibana Ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: elk spec: rules: - host: kibana.apps.k8s.curiouser.com http: paths: - path: / backend: serviceName: kibana servicePort: 5601 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 10:44:07 "},"origin/kubernetes-集群角色及插件.html":{"url":"origin/kubernetes-集群角色及插件.html","title":"Kubernetes的集群角色及插件","keywords":"","body":"一、Master节点上的组件 kube-apiserver：对外暴露了Kubernetes API。它是的 Kubernetes 前端控制层。它被设计为水平扩展，即通过部署更多实例来缩放。 kube-controller-manager：运行控制器，它们是处理集群中常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成独立的可执行文件，并在单个进程中运行。这些控制器包括: 节点控制器: 当节点移除时，负责注意和响应。 副本控制器: 负责维护系统中每个副本控制器对象正确数量的 Pod。 端点控制器: 填充端点(Endpoints) 对象(即连接 Services & Pods)。 服务帐户和令牌控制器: 为新的命名空间创建默认帐户和 API 访问令牌 kube-scheduler：监视没有分配节点的新创建的 Pod，选择一个节点供他们运行。 etcd：用于 Kubernetes 的后端存储。存储所有集群数据。 cloud-controller-manager：用于与底层云提供商交互的控制器。云控制器管理器可执行组件是 Kubernetes v1.6 版本中引入的 Alpha 功能。仅运行云提供商特定的控制器循环。您必须在 - kube-controller-manager 中禁用这些控制器循环，您可以通过在启动 kube-controller-manager 时将 --cloud-provider 标志设置为external来禁用控制器循环。允许云供应商代码和 Kubernetes 核心彼此独立发展，在以前的版本中，Kubernetes 核心代码依赖于云提供商特定的功能代码。在未来的版本中，云供应商的特定代码应由云供应商自己维护，并与运行 Kubernetes 的云控制器管理器相关联。以下控制器具有云提供商依赖关系: 节点控制器: 用于检查云提供商以确定节点是否在云中停止响应后被删除 路由控制器: 用于在底层云基础架构中设置路由 服务控制器: 用于创建，更新和删除云提供商负载平衡器 数据卷控制器: 用于创建，附加和装载卷，并与云提供商进行交互以协调卷 二、Node节点上的组件 kubelet：是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能: 挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过 rkt)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建镜像Pod（Mirror Pod） 将 Pod 的状态报告回系统的其余部分。 将节点的状态报告回系统的其余部分。 kube-proxy：通过维护主机上的网络规则并执行连接转发，实现了Kubernetes服务抽象 Container Runtime：运行容器的底层平台。Kubernetes支持的容器平台：Docker、containerd、cri-o、rktlet 三、插件 网络插件 ACI: provides integrated container networking and network security with Cisco ACI. Calico is a secure L3 networking and network policy provider. Canal unites Flannel and Calico, providing networking and network policy. Cilium is a L3 network and network policy plugin that can enforce HTTP/API/L7 - policies transparently. Both routing and overlay/encapsulation mode are - supported. CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,such as Calico, Canal, Flannel, Romana, or Weave. Contiv provides configurable networking (native L3 using BGP, overlay using - vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy - framework. Contiv project is fully open sourced. The installer provides both - kubeadm and non-kubeadm based installation options. Contrail, based on Tungsten Fabric, is a open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads. Flannel is an overlay network provider that can be used with Kubernetes. Knitter is a network solution supporting multiple networking in Kubernetes. Multus is a Multi plugin for multiple network support in Kubernetes to support - all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, - DPDK, OVS-DPDK and VPP based workloads in Kubernetes. NSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and - container orchestrators such as Kubernetes, as well as integration between - NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service - (PKS) and OpenShift. Nuage is an SDN platform that provides policy-based networking between - Kubernetes Pods and non-Kubernetes environments with visibility and security - monitoring. Romana is a Layer 3 networking solution for pod networks that also supports the - NetworkPolicy API. Kubeadm add-on installation details available here. Weave Net provides networking and network policy, will carry on working on both - sides of a network partition, and does not require an external database 服务发现插件 CoreDNS is a flexible, extensible DNS server which can be installed as the in-cluster DNS for pods 可视化及控制插件 Dashboard is a dashboard web interface for Kubernetes. Weave Scope is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a Weave Cloud account or host the UI yourself 参考链接 https://kubernetes.io/docs/concepts/cluster-administration/addons/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-27 14:31:57 "},"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html":{"url":"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html","title":"Kubeadm安装单机版Kubernetes","keywords":"","body":"使用Kubeadm安装单机版Kubernetes kubeadm是Kubernetes官方提供的用于快速安装 Kubernetes 集群的工具，通过将集群的各个组件进行容器化安装管理，通过kubeadm的方式安装集群比二进制的方式安装要方便 Prerequisite Hostname IP 地址 硬件 Kubernetes版本 Docker版本 allinone.k8s114.curiouser.com 172.16.1.12 最低2C2G v1.14.0 18.09.4 关闭防火墙 关闭Selinux 关闭Swap 加载br_netfilter 添加配置内核参数 hosts文件添加主机名与IP的映射关 #关闭防火墙 \\ #关闭Swap \\ #关闭Selinux \\ #加载br_netfilter \\ #添加配置内核参数 \\ #加载配置 \\ systemctl disable firewalld && systemctl stop firewalld \\ swapoff -a && sed -i 's/.\\\\*swap.\\\\*/#&/' /etc/fstab \\ setenforce 0 \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config && \\ modprobe br_netfilter && \\ bash -c 'cat > /etc/sysctl.d/k8s.conf > /etc/hosts 一、安装 安装docker kubeadm kubelet kubectl yum -y install yum-utils && \\ yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo && \\ bash -c 'cat > /etc/yum.repos.d/kubernetes.repo (可选)添加dockers日志相关配置 --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5 (可选)预拉取镜像 docker pull mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker pull mirrorgooglecontainers/pause:3.1 && \\ docker pull mirrorgooglecontainers/etcd:3.3.10 && \\ docker pull coredns/coredns:1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 && \\ docker pull calico/cni:v3.3.6 && \\ docker pull calico/node:v3.3.6 && \\ docker tag mirrorgooglecontainers/kube-proxy:v1.14.0 k8s.gcr.io/kube-proxy:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-scheduler:v1.14.0 k8s.gcr.io/kube-scheduler:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-apiserver:v1.14.0 k8s.gcr.io/kube-apiserver:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.0 k8s.gcr.io/kube-controller-manager:v1.14.0 && \\ docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 && \\ docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10 && \\ docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 && \\ docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker rmi mirrorgooglecontainers/pause:3.1 && \\ docker rmi mirrorgooglecontainers/etcd:3.3.10 && \\ docker rmi coredns/coredns:1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 初始化apiserver kubeadm init --apiserver-advertise-address=0.0.0.0 --kubernetes-version=v1.14.0 --pod-network-cidr=192.168.0.0/16 配置常规用户或root用户如何使用kubectl访问集群 mkdir -p $HOME/.kube && \\ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && \\ chown $(id -u):$(id -g) $HOME/.kube/config 设置Master节点可被调度 kubectl taint nodes --all node-role.kubernetes.io/master- #该参数node-role.kubernetes.io/master会污染所有节点，包括master节点,这意味着调度器可以调度POD到所有节点。 (可选)设置Kubectl命令别名及命令补全 yum install -y bash-completion && \\ echo \"alias k='kubectl'\" >> /etc/bashrc && \\ source 二、安装容器网络插件 Calico kubeadm初始化apiserver时添加\"--pod-network-cidr=192.168.0.0/16\" 网络工作在amd64，arm64，ppc64le kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml && \\ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml Flannel Prerequisite 设置 /proc/sys/net/bridge/bridge-nf-call-iptables为1，将桥接的IPv4流量传递到iptables的链 sysctl net.bridge.bridge-nf-call-iptables=1 kubeadm初始化apiserver时添加\"--pod-network-cidr=10.244.0.0/16\" flannel网络工作在amd64, arm, arm64, ppc64le,s390x 安装 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 三、验证 查看所有namespace下的POD kubectl get pods --all-namespaces kubectl get pod -n kube-system 查看集群Node节点 kubectl get node systemctl status kubelet.service 查看版本 kubectl version 显示集群信息 kubectl cluster-info 四、添加Node节点 kubeadm join 172.16.1.12:6443 --token i7xcb9.sz5t4sa8xx3ntc2h --discovery-token-ca-cert-hash sha256:487275a22ea4af5a1ea30ee4b0f21f8c27104d17f6a259bf4990f1569a3301cd 查看Master的Token kubeadm token list Master创建Token kubeadm token create Master节点创建\"--discovery-token-ca-cert-hash\"值 openssl x509 -pubkey -in /etc/kubernet 五、安装UI管理界面 DashBoard 项目GitHub：https://github.com/kubernetes/dashboard.git # ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role & Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"create\"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics from heapster. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 #===修改原rolebind类型RoleBinding为ClusterRoleBinding kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io #修改原role类型Role为ClusterRole kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 ports: #====修改原容器端口8443为9090 - containerPort: 9090 protocol: TCP args: #==== #- --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTP path: / #修改原健康检查端口8443为9090 port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # #使用Nodeport的方式访问Dashboard kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-external namespace: kube-system spec: ports: - port: 9090 targetPort: 9090 nodePort: 30090 type: NodePort selector: k8s-app: kubernetes-dashboard 拉取Template中使用的Image docker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 && \\ docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 && \\ docker rmi mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 Weave Scope 官方文档： https://www.weave.works/docs/scope/latest/installing/#k8s 安装部署 kubectl apply -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" （可选）修改svc使用NodePort访问 $ kubectl edit service/weave-scope-app -n weave apiVersion: v1 kind: Service #.........省略........ spec: externalTrafficPolicy: Cluster ports: - name: app #===== nodePort: 30040 port: 80 protocol: TCP targetPort: 4040 selector: app: weave-scope name: weave-scope-app weave-cloud-component: scope weave-scope-component: app sessionAffinity: None #====== type: NodePort Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/kubernetes-kubectl多集群上下文配置.html":{"url":"origin/kubernetes-kubectl多集群上下文配置.html","title":"kubectl多集群上下文配置","keywords":"","body":"kubectl命令行配置多Kubernetes集群 一. 下载kubectl kubectl github下载地址：https://github.com/kubernetes/kubectl/releases 二. 创建配置文件夹 Linux mkdir ~/.kube Windows CMD mkdir %USERPROFILE%\\.kube # %USERPROFILE% 当前用户目录 三. 创建编辑kubectl配置文件 apiVersion: v1 # 集群信息 clusters: - cluster: certificate-authority-data: **CA证书*** server: https://开发k8s环境APIServer的IP地址:6443 name: k8s-dev - cluster: certificate-authority-data: **CA证书*** server: https://测试k8s环境APIServer的IP地址:8443 name: k8s-test - cluster: certificate-authority-data: **CA证书*** server: https://UAT k8s环境APIServer的IP地址:8443 name: k8s-uat - cluster: certificate-authority-data: **CA证书*** server: https://生产k8s环境APIServer的IP地址:8443 name: k8s-pro # 集群上下文环境 contexts: - context: cluster: k8s-dev user: k8s-dev-admin name: k8s-dev - context: cluster: k8s-test user: k8s-test-admin name: k8s-test - context: cluster: k8s-uat user: k8s-uat-admin name: k8s-uat - context: cluster: k8s-pro user: k8s-pro-readonly name: k8s-pro # 当前使用的上下文环境 current-context: k8s-dev kind: Config preferences: {} #集群用户信息及证书信息 users: - name: k8s-dev user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-test user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-uat user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-pro user: client-certificate-data: **用户证书** client-key-data： **用户私钥** 四. 切换Kubernetes集群上下文 #切换至开发k8s环境上下文 kubectl config use-context k8s-dev #切换至开发k8s环境上下文 kubectl config use-context k8s-test #切换至开发k8s环境上下文 kubectl config use-context k8s-uat #切换至开发k8s环境上下文 kubectl config use-context k8s-pro 五. kubectl命令的别名和快速切换集群上下文的别名 设置别名快速使用kubectl命令 Windows doskey k=kubectl $* # $*表示这个命令还可能有其他参数 Linux alias k='kubectl' 设置别名快速切换Kubectl集群上下文 Windows doskey k2d=kubectl config use-context k8s-dev doskey k2t=kubectl config use-context k8s-test doskey k2u=kubectl config use-context k8s-uat doskey k2p=kubectl config use-context k8s-pro Linux alias k2d='kubectl config use-context k8s-dev' alias k2t='kubectl config use-context k8s-test' alias k2u='kubectl config use-context k8s-uat' alias k2p='kubectl config use-context k8s-pro' Windows和Linux下设置别名永久生效 Windows ①创建bat脚本cmdalias.cmd @doskey k=kubectl $* @doskey k2d=kubectl config use-context k8s-dev @doskey k2t=kubectl config use-context k8s-test @doskey k2u=kubectl config use-context k8s-uat @doskey k2p=kubectl config use-context k8s-pro # @表示执行这条命令时不显示这条命令本身 ②修改注册表 方式1：手动在注册HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor下添加一项AutoRun，把值设为bat脚本的路径 方式2：创建编写一个注册表修改文件，名为：add-regkey.reg，双击行这个文件,导入注册表添加的值 Windows Registry Editor Version 5.00 [HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor] \"AutoRun\"=\"%USERPROFILE%\\\\.kube\\\\cmdalias.cmd\" Linux echo \"alias k='kubectl'\" >> /etc/profile && \\ echo \"alias k2d='kubectl config use-context k8s-dev'\" >> /etc/profile && \\ echo \"alias k2t='kubectl config use-context k8s-test'\" >> /etc/profile && \\ echo \"alias k2u='kubectl config use-context k8s-uat'\" >> /etc/profile && \\ echo \"alias k2p='kubectl config use-context k8s-pro'\" >> /etc/profile && \\ source /etc/profile 六. Kubectl Config命令详解 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used a list of paths (normal path delimitting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Usage: kubectl config SUBCOMMAND [options] Available Commands: current-context Displays the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts rename-context Renames a context from the kubeconfig file. set Sets an individual value in a kubeconfig file set-cluster Sets a cluster entry in kubeconfig set-context Sets a context entry in kubeconfig set-credentials Sets a user entry in kubeconfig unset Unsets an individual value in a kubeconfig file use-context Sets the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file 参考连接 https://blog.csdn.net/u013360850/article/details/83315188 https://www.awaimai.com/2445.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 17:03:02 "},"origin/kubernetes-NetworkPolicy.html":{"url":"origin/kubernetes-NetworkPolicy.html","title":"Network Policy容器流量管理","keywords":"","body":"Kubernetes Network Policy容器网络流量限制 一、Overview Kubernetes使用命名空间namesapce做多租户隔离，但是如果不配置网络策略，namespace的隔离也仅仅是作用于在kubernetes编排调度时的隔离，实际上不同namespace下的pod还是可以相互联通的。此时就需要使用Kubernetes提供的networkPolicy,用于隔离不同租户应用的网络流量来减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。Kubernetes中的Network Policy只定义了规范，并没有提供实现，而是把实现留给了网络插件 Kubernetes v1.7+版本GA，API版本为http://networking.k8s.io/v1 二、使用Calico做Network policy Network Policy的实现仰赖CNI插件的支持，目前已经支持的cni插件包括： Calico Kube-router Romana Weave Net trireme Calico的NetworkPolicy功能支持以下特性： 支持多种endpoint: pods/containers, VMs 支持限制入站和出站的流量访问 规则策略支持: 动作: allow, deny, log, pass 源和目标的匹配标准: 端口: numbered, ports in a range, and Kubernetes named ports 协议: TCP, UDP, ICMP, SCTP, UDPlite, ICMPv6, protocol numbers (1-255) HTTP attributes (if using Istio service mesh) ICMP attributes IP version (IPv4, IPv6) IP or CIDR Endpoint selectors (using label expression to select pods, VMs, host - interfaces, and/or network sets) Namespace selectors Service account selectors Optional packet handling controls: disable connection tracking, apply before DNAT, apply to forwarded traffic and/or locally terminated traffic Preflight k8s集群版本大于v1.3.0 calico-cni网络插件的二进制文件 kubelet添加配置Flag 需要配置kubelet 让pod启动时使用calico网络插件，kubelet可以配置使用calico在启动时配置参数： --network-plugin=cni --cni-conf-dir=/etc/cni/net.d # CNI插件的配置文件目录,该目录下的配置文件内容需要符合CNI规范 --cni-bin-dir=/opt/cni/bin # CNI插件的可执行文件目录，默认为/opt/cni/bin API Server添加配置Flag --allow-privileged=true # calico-node的POD需要以特权模式运行在各node上 三、策略规则的声明配置 Network Policy策略规则是用来定义命名空间有哪些POD，能被谁访问，能访问谁的。相应地，在Network Policy声明文件中的字段有： spec.podSelector: 定义该命名空间有哪些POD遵循本networkpolicy约束 spec.ingress.from: 定义受本networkpolicy约束的POD的入站规则 spec.egress.to：定义受本networkpolicy约束的POD的出站规则 spec.ingress.from的选择器有： podSelector namespaceSelector ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ...下文省略... namespaceSelector和podSelector（注意YAML语法的区别） ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ...下文省略... ipBlock 四、示例说明 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: # POD选择器，选择遵循本networkpolicy约束的POD podSelector: matchLabels: role: db # 流量访问策略类型 policyTypes: - Ingress - Egress # 流量访问入站规则 ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 # POD流量访问出站规则 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 上述例子，网络流量访问策略规则可解释为： \"default\"命名空间下标签为\"role=db\"的POD的入站规则： 允许被\"default\"命名空间下，所有带标签\"role=frontend\"的POD访问TCP 6379端口 允许被标签为\"project=myproject\"的命名空间下所有的POD访问TCP 6379端口 允许被IP地址为172.17.0.0–172.17.0.255或172.17.2.0–172.17.255.255的POD访问TCP 6379端口 \"default\"命名空间下标签为\"role=db\"的POD的出站规则： 允许访问10.0.0.0/24网段的POD的5978端口 五、默认策略 默认情况下，如果namespace下没有network policy,则该namespace下所有POD的入站规则和出站规则都是开放的。network policy只影响命名空间下被Pod Selector选择的POD，其他依旧是默认规则。 namespace下的所有pod，入站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Ingress namespace下的所有pod，入站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} ingress: - {} policyTypes: - Ingress namespace下的所有pod，出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Egress namespace下的所有pod，出站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} egress: - {} policyTypes: - Egress 同namespace的pod，入站和出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress 六、场景测试 限制服务只能被带有特定label的应用访问 $ kubectl create ns test1 $ kubectl -n test1 run nginx --image=nginx $ kubectl -n test1 expose deployment nginx --port=80 kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: test1 name: access-nginx spec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: \"true\" # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ (无法访问) # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --labels=\"access=true\" --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ Connecting to nginx.test1.svc:80 (10.68.86.216:80) saving to 'index.html' index.html 100% | *********************************************************************************************************************| 612 0:00:00 ETA 'index.html' saved 限制带\"run=busybox\"标签的Pod只能访问www.baidu.com kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: busybox-egress-baidu-a-policy namespace: test1 spec: podSelector: matchLabels: run: busybox egress: - to: - ipBlock: cidr: 180.101.49.11/32 - to: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: UDP port: 53 $ kubectl -n test1 run busybox --labels=\"run=busybox\" --rm -ti --image=busybox /bin/sh / # wget www.baidu.com Connecting to www.baidu.com (180.101.49.11:80) saving to 'index.html' index.html 100% |*********************************************************************************************************************| 2381 0:00:00 ETA 'index.html' saved / # wget www.sohu.com Connecting to www.sohu.com (101.227.172.11:80) ^C / # wget https://github.com Connecting to github.com (13.250.177.223:443) ^C / # 参考链接 https://kubernetes.io/docs/concepts/services-networking/network-policies/#sctp-support https://docs.projectcalico.org/v3.8/security/kubernetes-network-policy#best-practice-create-deny-all-default-network-policy https://yq.aliyun.com/articles/640190 https://www.jianshu.com/p/c0d2618d2849 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-28 20:14:46 "},"origin/kubernetes-helm详解.html":{"url":"origin/kubernetes-helm详解.html","title":"Kubernetes应用管理工具helm详解","keywords":"","body":"Helm简介、安装、配置、使用 一、简介 二、原理 三、安装 Tiller服务端安装 Helm客户端安装 四、配置 五、命令行参数 helm template -f values-dev.yaml . helm del --purge logstash-producer-mysql-slowlog helm install --namespace logger --name logstash-producer-mysql-slowlog -f values.yaml . helm upgrade logstash-producer-mysql-slowlog -f values.yaml . Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-16 09:23:02 "},"origin/jenkins-SharedLibraries.html":{"url":"origin/jenkins-SharedLibraries.html","title":"Jenkins共享库Shared Libraries","keywords":"","body":"Jenkins Pipeline共享库Shared Libraries 一、简介 随着Job的增多和pipeline的功能越来越复杂，Pipeline代码冗余度高。所以可以将一些公共的pipeline抽象做成模块代码，在各种项目pipeline之间共享核心实现，同时可以放到SVM中进行版本控制。以减少冗余并保证所有job在构建的时候会调用最新的共享库代码。这时就可以用到pipline的共享库Shared Libraries功能。 模块化 可重用性 二、共享库的目录结构 共享库根目录 |-- vars |-- test1.groovy |-- src |-- test2.groovy |-- resources vars: 依赖于Jenkins运行环境的Groovy脚本。其中的Groovy脚本被称之为全局变量。 src: 标准的Java源码目录结构,其中的Groovy脚本被称为类库(Library class)。该目录所有下的所有类都一次性静态被添加到类路径classpath下 resources: 目录允许从外部库中使用 libraryResource 步骤来加载有关的非 Groovy 文件。 目前，内部库不支持该特性 三、配置全局共享库 可在Jenkins中的Manage Jenkins –> Configure System　–> Global Pipeline Libraries 添加一个或多个全局的共享库，同时也可以在构建过程中的任何位置使用library step动作动态地配置引用共享库，详见动态引用共享库 四、引用共享库 1. 引用全局共享库 格式：@Library('my-shared-library-1@$Branch/Tag','my-shared-library-1@$Branch/Tag') _ #!groovy // 引用默认配置的共享库 @Library('demo-shared-library') _ // 引用指定分支、tag的共享库代码 @Library('demo-shared-library@1.0') _ // 引用多个指定分支tag的共享库 @Library('demo-shared-library@$Branch/Tag','demo-shared-library-test@$Branch/Tag') _ @Library('utils') import org.foo.Utilities @Library('utils') import static org.foo.Utilities.* 2. 动态引用共享库 2.7版本后的Shared Groovy Libraries插件，增加了一个library的setp,可以随时在构建过程中引用共享库 #!groovy library 'demo-shared-library@$BRANCH_NAME' library \"demo-shared-library@${params.LIB_VERSION}\" library('demo-shared-library').com.mycorp.pipeline.Utils.someStaticMethod() // 此时共享库的版本必须指定 library identifier: 'custom-lib@master', retriever: modernSCM( [$class: 'GitSCMSource', remote: 'git@git.mycorp.com:my-jenkins-utils.git', credentialsId: 'my-private-key']) 3. 调用第三方Java库 @Grab('org.apache.commons:commons-math3:3.4.1') import org.apache.commons.math3.primes.Primes 引用完的第三方Java库后会缓存在Jenkins Master节点的~/.groovy/grapes/ 目录下 五、全局变量和类库的编写规则和调用方法 1. /var下定义的全局变量 全局变量必须以全小写或驼峰（camelCased）命名以便于能够在流水线中正确的加载 /vars目录中的脚本根据需求以单例的方式实例化，这允许在单个.groovy` 文件中定义多个方法 /vars/*.groovy若实现call()方法，直接引用时默认执行其中的方法，该方法可以让全局变量以一种以类似于step的方式被调用 /vars/log.groovy #!groovy def call(String name = 'human') { echo \"Hello, ${name}.\" } def info(message) { echo \"INFO: ${message}\" } def warning(message) { echo \"WARNING: ${message}\" } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ log() // 输出\"Hello, human.\" log.info 'Starting' // 输出\"INFO: Starting\" log.warning 'Nothing to do!' // 输出\"WARNING: Nothing to do!\" 从2017年9月下旬发布的声明式 1.2开始，可以在全局变量中直接定义声明式流水线 /vars/evenOrOdd.groovy #!groovy def call(int buildNumber) { if (buildNumber % 2 == 0) { pipeline { agent any stages { stage('Even Stage') { steps { echo \"The build number is even\" } } } } } else { pipeline { agent any stages { stage('Odd Stage') { steps { echo \"The build number is odd\" } } } } } } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ evenOrOdd(currentBuild.getNumber()) 全局变量的传参 /vars/buildPlugin.groovy #!groovy def call(Map config) { node { git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\" sh 'mvn install' mail to: '...', subject: \"${config.name} plugin build\", body: '...' } } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ buildPlugin name: 'git' 声明式流水线不允许在script指令之外使用全局变量 Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ pipeline { agent none stage ('Example') { steps { script { log.info 'Starting' log.warning 'Nothing to do!' } } } } 2. /src下定义的类库 类库不能直接调用 sh或 git这样的步骤。 但是他们可以在封闭的类的范围之外实现方法，从而调用流水线步骤 /src/org/foo/Zot.groovy #!groovy package org.foo; def checkOutFrom(repo) { git url: \"git@github.com:jenkinsci/${repo}\" } return this Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ def z = new org.foo.Zot() z.checkOutFrom(repo) 类库中使用”class“声明父类 /src/org/foo/Utilities.groovy package org.foo class Utilities implements Serializable { def steps Utilities(steps) {this.steps = steps} def mvn(args) { steps.sh \"${steps.tool 'Maven'}/bin/mvn -o ${args}\" } } Jenkinsfile @Library('utils') import org.foo.Utilities def utils = new Utilities(this) node { utils.mvn 'clean package' } 类库中的方法访问流水线中的变量 /src/org/foo/Utilities.groovy package org.foo class Utilities { static def mvn(script, args) { script.sh \"${script.tool 'Maven'}/bin/mvn -s ${script.env.HOME}/jenkins.xml -o ${args}\" } } Jenkinsfile @Library('utils') import static org.foo.Utilities.* node { mvn this, 'clean package' } 六、Jenkins Pipeline生成器生成动态引用共享库的代码 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-29 09:25:23 "},"origin/jenkins-声明式Declarative-pipeline语法.html":{"url":"origin/jenkins-声明式Declarative-pipeline语法.html","title":"声明式Declarative语法","keywords":"","body":"Jenkins声明式Declarative Pipeline 一、语法结构 Jenkins 2.5新加入的pipeline语法 声明式pipeline 基本语法和表达式遵循 groovy语法，但是有以下例外： 声明式pipeline 必须包含在固定格式的pipeline{}中 每个声明语句必须独立一行， 行尾无需使用分号 块(Blocks{}) 只能包含章节(Sections),指令（Directives）,步骤(Steps),或者赋值语句 属性引用语句被视为无参数方法调用。 如input() 一个声明式Pipeline中包含的元素 pipeline：声明这是一个声明式的pipeline脚本 agent：指定要执行该Pipeline的节点（job运行的slave或者master节点） stages：阶段集合，包裹所有的阶段（例如：打包，部署等各个阶段） stage：阶段，被stages包裹，一个stages可以有多个stage steps：步骤,为每个阶段的最小执行单元,被stage包裹 post：执行构建后的操作，根据构建结果来执行对应的操作 示例： pipeline{ // 指定pipeline在哪个slave节点上允许 agent { label 'jdk-maven' } // 指定pipeline运行时的一些配置 option { timeout(time: 1, unit: 'HOURS') } // 自定义的参数 parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } // 自定义的环境变量 environment { Gitlab_Deploy_KEY = credentials('gitlab-jenkins-depolykey') } // 定义pipeline的阶段任务 stages { stage (\"阶段1任务：拉代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段2任务：编译代码\") { steps { // 编译代码的具体命令 } } stage (\"阶段3任务：扫描代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段4任务：打包代码\") { steps { // 打包代码的具体命令 } } stage (\"阶段5任务：构建推送Docker镜像\") { steps { // 构建推送Docker镜像的具体命令 } } stage (\"阶段6任务：部署镜像\") { steps { // 部署镜像的具体命令 } } } post { success { // 当pipeline构建状态为\"success\"时要执行的事情 } always { // 无论pipeline构建状态是什么都要执行的事情 } } } 二、章节Sections 1、agent（必须） 指定整个Pipeline或特定阶段是在Jenkins Master节点还是Jenkins Slave节点上运行。可在顶级pipeline块和每个stage块中使用（在顶层pipeline{}中是必须定义的 ，但在阶段Stage中是可选的） 参数（以下参数值在顶层pipeline{}和stage{}中都可使用）： any：在任何可用的节点上执行Pipeline或Stage none：当在顶层pipeline{}中应用时，将不会为整个Pipeline运行分配全局代理，并且每个stage部分将需要包含其自己的agent部分 label node docker dockerfile kubernetes 公用参数： label customWorkspace reuseNode args 2、post 定义在Pipeline运行或阶段结束时要运行的操作。具体取决于Pipeline的状态 支持pipeline运行状态: always：无论Pipeline运行的完成状态如何都要运行 changed：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能运行 fixed：整个pipeline或者stage相对于上一次失败或不稳定Pipeline的状态有改变。才能运行 regression： aborted：只有当前Pipeline处于“中止”状态时，才会运行，通常是由于Pipeline被手动中止（通常在具有灰色指示的Web UI 中表示） failure：仅当当前Pipeline处于“失败”状态时才运行（通常在Web UI中用红色指示表示） success：仅当当前Pipeline在“成功”状态时才运行（通常在具有蓝色或绿色指示的Web UI中表示） unstable：只有当前Pipeline在不稳定”状态，通常由测试失败，代码违例等引起，才能运行（通常在具有黄色指示的Web UI中表示） unsuccessful： cleanup：无论Pipeline或stage的状态如何，在跑完所有其他的post条件后运行此条件下 的post步骤。 3、stages（必须） 至少包含一个用于执行任务的stage指令 pipeline{ }中只能有一个stages{} 4、steps（必须） 在stage指令中至少包含一个用于执行命令的steps 三、指令Directives 1、Environment环境变量 environment{…},使用键值对来定义一些环境变量并赋值。它的作用范围，取决environment{…}所写的位置。写在顶层环境变量，可以让所有stage下的step共享这些变量；也可以单独定义在某一个stage下，只能供这个stage去调用变量，其他的stage不能共享这些变量。一般来说，我们基本上上定义全局环境变量，如果是局部环境变量，我们直接用def关键字声明就可以，没必要放environment{…}里面。 同时，environment{…}支持credentials() 方法来访问预先在Jenkins保存的凭据，并赋值给环境变量 credentials() 支持的凭据类型： Secret Text Secret File Username and password：使用变量名_USR and 变量名_PSW 来获取其中的用户名和Password pipeline { agent any stages { stage('Example Username/Password') { environment { SERVICE_CREDS = credentials('my-prefined-username-password') } steps { sh 'echo \"Service user is $SERVICE_CREDS_USR\"' sh 'echo \"Service password is $SERVICE_CREDS_PSW\"' sh 'curl -u $SERVICE_CREDS https://myservice.example.com' } } } } SSH with Private Key pipeline { agent any stages { stage('Example Username/Password') { environment { SSH_CREDS = credentials('my-prefined-ssh-creds') } steps { sh 'echo \"SSH private key is located at $SSH_CREDS\"' sh 'echo \"SSH user is $SSH_CREDS_USR\"' sh 'echo \"SSH passphrase is $SSH_CREDS_PSW\"' } } } } 2、Parameters参数 pipeline{ }中只能有一个parameters{} 参数定义格式 parameters { 参数类型(name: '参数名', defaultValue: '默认值', description: '描述') } 参数类型 string text boobleanParam choice password 参数调用格式：${params.参数名} 示例： pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" echo \"Biography: ${params.BIOGRAPHY}\" echo \"Toggle: ${params.TOGGLE}\" echo \"Choice: ${params.CHOICE}\" echo \"Password: ${params.PASSWORD}\" } } } } 3、Options选项 pipeline{ }中只能有一个options{} buildDiscarder checkoutToSubdirectory disableConcurrentBuilds disableResume newContainerPerStage overrideIndexTriggers preserveStashes quietPeriod retry skipDefaultCheckout skipStagesAfterUnstable timeout timestamps parallelsAlwaysFailFast 4、Triggers触发器 pipeline{ }中只能有一个triggers {} 触发器类型 cron pollSCM upstream Jenkins的Cron语法 5、Stage阶段(至少有一个) 包含在stages{}中 至少有一个 6、Tools工具 包含在pipeline{}或stage{} 支持的工具： Maven JDK Gradle 7、Input用户输入 8、When条件 内置条件： branch Execute the stage when the branch being built matches the branch pattern given, for example: when { branch 'master' }. Note that this only works on a multibranch Pipeline. buildingTag Execute the stage when the build is building a tag. Example: when { buildingTag() } changelog Execute the stage if the build’s SCM changelog contains a given regular expression pattern, for example: when { changelog '.*^\\[DEPENDENCY\\] .+$' } changeset Execute the stage if the build’s SCM changeset contains one or more files matching the given string or glob. Example: when { changeset \"*/.js\" } By default the path matching will be case insensitive, this can be turned off with the caseSensitive parameter, for example: when { changeset glob: \"ReadMe.*\", caseSensitive: true } changeRequest Executes the stage if the current build is for a \"change request\" (a.k.a. Pull Request on GitHub and Bitbucket, Merge Request on GitLab or Change in Gerrit etc.). When no parameters are passed the stage runs on every change request, for example: when { changeRequest() }. By adding a filter attribute with parameter to the change request, the stage can be made to run only on matching change requests. Possible attributes are id, target, branch, fork, url, title, author, authorDisplayName, and authorEmail. Each of these corresponds to a CHANGE_* environment variable, for example: when { changeRequest target: 'master' }. The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison (the default), GLOB for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. Example: when { changeRequest authorEmail: \"[\\w_-.]+@example.com\", comparator: 'REGEXP' } environment Execute the stage when the specified environment variable is set to the given value, for example: when { environment name: 'DEPLOY_TO', value: 'production' } equals Execute the stage when the expected value is equal to the actual value, for example: when { equals expected: 2, actual: currentBuild.number } expression Execute the stage when the specified Groovy expression evaluates to true, for example: when { expression { return params.DEBUG_BUILD } } Note that when returning strings from your expressions they must be converted to booleans or return null to evaluate to false. Simply returning \"0\" or \"false\" will still evaluate to \"true\". tag Execute the stage if the TAG_NAME variable matches the given pattern. Example: when { tag \"release-*\" }. If an empty pattern is provided the stage will execute if the TAG_NAME variable exists (same as buildingTag()). The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison, GLOB (the default) for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. For example: when { tag pattern: \"release-\\d+\", comparator: \"REGEXP\"} not Execute the stage when the nested condition is false. Must contain one condition. For example: when { not { branch 'master' } } allOf Execute the stage when all of the nested conditions are true. Must contain at least one condition. For example: when { allOf { branch 'master'; environment name: 'DEPLOY_TO', value: 'production' } } anyOf Execute the stage when at least one of the nested conditions is true. Must contain at least one condition. For example: when { anyOf { branch 'master'; branch 'staging' } } triggeredBy Execute the stage when the current build has been triggered by the param given. For example: when { triggeredBy 'SCMTrigger' } when { triggeredBy 'TimerTrigger' } when { triggeredBy 'UpstreamCause' } when { triggeredBy cause: \"UserIdCause\", detail: \"vlinde\" } 未完待整理更新 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-19 13:15:20 "},"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html":{"url":"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html","title":"Kubernetes Plugin","keywords":"","body":"Jenkins在Kubernetes上使用Kubernetes插件动态创建Slave节点 一、Context 插件GIthub地址：https://github.com/jenkinsci/kubernetes-plugin Jenkins 分布式架构是由一个 Master 和多个 Slave Node组成的分布式架构。在 Jenkins Master 上管理你的项目，可以把你的一些构建任务分担到不同的 Slave Node 上运行，Master 的性能就提高了。Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行构建。一个master（jenkins服务所在机器）可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 传统的 Jenkins Slave 一主多从式会存在一些痛点。比如： 主 Master 发生单点故障时，整个流程都不可用了； 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致 管理起来非常不方便，维护起来也是比较费劲； 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态； 资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 而使用Kubernetes插件可以在Kubernetes上动态创建slave POD作为Slave节点。Jenkins Master 和 Slave 节点以 Docker Container 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。 这种方式的工作流程大致为：当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Docker Container 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且 Docker Container 也会自动删除，恢复到最初状态。这种方式带来的好处有很多： 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。 二、Jenkins与Slave的连接方式 Jenkins的Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。当job被分配到slave上运行的时候，此时master和slave其实是建立的双向字节流的连接，其中连接方法主要有如下几种： SSH：Jenkins内置有ssh客户端实现，可以用来与远程的sshd通信，从而启动slave agent。这是对unix系统的slave最方便的方法，因为unix系统一般默认安装有sshd。在创建ssh连接的slave的时候，你需要提供slave的host名字，用户名和ssh证书。创建public/private keys，然后将public key拷贝到slave的~/.ssh/authorized_keys中，将private key 保存到master上某ppk文件中。jenkins将会自动地完成其他的配置工作，例如copy slave agent的binary，启动和停止slave。 Java web start（JNLP：Java Network Lancher Protocol）：jnlp连接方式有个好处就是不用master和slave之间能够ssh连接，只需要能够ping即可。并且如果slave的机器是windows的话，也是可以的这个其实是非常实用的 WMI+DCOM：对于Windows的Slave，Jenkins可以使用Windows2000及以后内置的远程管理功能（WMI+DCOM），你只需要提供对slave有管理员访问权限的用户名和密码，jenkins将远程地创建windows service然后远程地启动和停止他们。对于windows的系统，这是最方便的方法，但是此方法不允许运行有显示交互的GUI程序。 在Kubernetes上的Jenkins通过Kubernetes插件动态创建的Slave POD节点是通过JNLP的方式与Jenkins Master进行通信的！ 三、Jenkins Kubernetes插件的安装配置 安装 配置 四、定制Slave镜像 Slave镜像中安装的软件信息 工具 版本 说明 Oracel JDK 1.8.0_171 Maven编译打包时使用 Apache Maven 3.6.1 在Slave容器中使用MAVEN编译打包源代码 helm v2.13.1 helm客户端 git 1.8.3.1 git命令 docker client 1.13.1 Dockers客户端，用于在Slave容器中构建应用镜像 sonar-scanner 3.3.0.1492 用于扫描源代码 FROM centos:7.4.1708 ENV TZ=Asia/Shanghai \\ LANG=en_US.UTF-8 \\ JDK_VERSION=Oracle_1.8.0_171 \\ MAVEN_VERSION=Apache_3.6.1 \\ HOME=/home/jenkins \\ MAVEN_HOME=/opt/apache-maven-3.6.1 \\ JAVA_HOME=/opt/jdk1.8.0_171 \\ SONARSCANNER_HOME=/opt/sonar-scanner-3.3.0.1492-linux COPY jdk1.8.0_171 /opt/jdk1.8.0_171 COPY apache-maven-3.6.1 /opt/apache-maven-3.6.1 COPY helm /usr/bin/helm COPY sonar-scanner-3.3.0.1492-linux /opt/sonar-scanner-3.3.0.1492-linux RUN curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo \\ && yum makecache \\ && yum install -y git docker-ce-cli make \\ && yum clean all \\ && groupadd -g 1000 jenkins \\ && useradd -c \"Jenkins user\" -d /home/jenkins -u 1000 -g 0 -m jenkins \\ && mkdir /home/jenkins/.m2 \\ && chown -R 1000.0 /home/jenkins \\ && ln -s /opt/jdk1.8.0_171/bin/java /usr/bin/java \\ && ln -s /opt/apache-maven-3.6.1/bin/mvn /usr/bin/mvn \\ && ln -s /opt/sonar-scanner-3.3.0.1492-linux/bin/sonar-scanner /usr/bin/sonar-scanner USER jenkins WORKDIR /home/jenkins COPY dumb-init /usr/bin/dumb-init ADD run-jnlp-client /usr/bin/ ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/usr/bin/run-jnlp-client\"] 五、Pipeline或者Job中使用验证 Job 创建一个自由风格的Job 点击构建后，会自动创建一个Slave POD，并通过JNLP协议与Jenkins Master的Agent端口5000进行通通信 Declarative Pipeline pipeline { agent { label 'maven' } stages { stage (\"代码编译\") { steps { configFileProvider([configFile(fileId: 'nexus-maven-settings', targetLocation: 'settings.xml')]){ sh 'mvn -s settings.xml compile' } } } stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.projectName=demo-springboot2 \\ -Dsonar.projectKey=demo-springboot2 \\ -Dsonar.sources=src \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=****** \\ -Dsonar.java.binaries=. \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.issuesReport.html.enable=true \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.gitlab.user_token=***** \\ -Dsonar.gitlab.url=http://gitlab.apps.okd311.curiouser.com/ \\ -Dsonar.gitlab.ignore_certificate=true \\ -Dsonar.gitlab.comment_no_issue=true \\ -Dsonar.gitlab.max_global_issues=1000 \\ -Dsonar.gitlab.unique_issue_per_inline=true\" } } stage (\"代码打包\") { steps { sh \"mvn -s settings.xml package\" } } stage(\"上传制品\"){ steps{ script{ def pomfile = readMavenPom file: 'pom.xml' sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u devops:**** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://nexus.apps.okd311.curiouser.com/repository/jenkins-product-repo/${pomfile.artifactId}-${pomfile.version}-${env.GIT_COMMIT}.${pomfile.packaging}\" } } } stage(\"构建应用镜像\"){ steps{ sh 'docker login -p ********** -u unused docker-registry-default.apps.okd311.curiouser.com' sh 'make' } } } post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 参考博客： https://github.com/easzlab/kubeasz/blob/master/docs/guide/jenkins.md https://www.qikqiak.com/k8s-book/docs/36.Jenkins%20Slave.html https://jenkins.io/blog/2018/09/14/kubernetes-and-secret-agents/ https://www.jianshu.com/p/1440b5b4b980 https://www.cnblogs.com/guguli/p/7827435.html https://blog.csdn.net/felix_yujing/article/details/78725142 https://jicki.me/kubernetes/2018/02/08/kubernetes-jenkins/ https://testerhome.com/topics/17251 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/jenkins-pipeline-utility-steps.html":{"url":"origin/jenkins-pipeline-utility-steps.html","title":"Pipeline Utility Steps","keywords":"","body":"Jenkins Pipeline-Utility-steps插件 一、Pipeline-Utility-steps插件简介 Jekins中的Pipeline-Utility-steps插件能让你在pipeline的Step中直接使用它的API方法进行某些操作，例如查找文件，读取YAML/JSON/Properties文件、读取Maven工程POM文件等。这些方法有一个前提，任何文件都需要放在jenkins的workspace下，执行的job才能去找到文件。 Github地址：https://github.com/jenkinsci/pipeline-utility-steps-plugin 相关文档：https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/ 二、Pipeline-Utility-steps插件的方法 File System findFiles：根据一些字符串规则去查找文件，如果有匹配的查找，返回是一个fille数组对象。（文档） 参数 excludes(可选，参数类型为String) glob(可选，参数类型为String) 示例 def files = findFiles(glob: '**/TEST-*.xml') echo \"\"\"${files[0].name} ${files[0].path} ${files[0].directory} ${files[0].length} ${files[0].lastModified}\"\"\" touch：创建文件（如果文件不存在的话）并设置时间戳. Returns a FileWrapper representing the file that was touched. (文档) 参数 file(参数类型为String)：The path to the file to touch. timestamp(可选，参数类型为long)：The timestamp to set (number of ms since the epoc), leave empty for current system time. sha1：计算指定文件的SHA1 (文档) 参数 file(参数类型为String): The path to the file to hash. tee：将输出重定向到文件 参数 file(参数类型为String) Zip Files zip：创建Zip文件. (文档) 参数 zipFile(参数类型为String): The name/path of the zip file to create. archive(可选，参数类型为boolean): If the zip file should be archived as an artifact of the current build. The file will still be kept in the workspace after archiving. dir(可选，参数类型为String): The path of the base directory to create the zip from. Leave empty to create from the current working directory. glob(可选，参数类型为String): Ant style pattern of files to include in the zip. Leave empty to include all files and directories. unzip：解压或读取Zip文件 (文档) 参数 zipFile(参数类型为String): The name/path of the zip file to extract. charset(可选，参数类型为String): Specify which Charset you wish to use eg. UTF-8 dir(可选，参数类型为String): The path of the base directory to extract the zip to. Leave empty to extract in the current working directory. glob(可选，参数类型为String): Ant style pattern of files to extract from the zip. Leave empty to include all files and directories. quiet(可选，参数类型为boolean): Suppress the verbose output that logs every single file that is dealt with. E.g. unzip zipFile: 'example.zip', quiet: true read(可选，参数类型为boolean): Read the content of the files into a Map instead of writing them to the workspace. The keys of the map will be the path of the files read. E.g. def v = unzip zipFile: 'example.zip', glob: '*.txt', read: true String version = v['version.txt'] test(可选，参数类型为boolean): Test the integrity of the archive instead of extracting it. When this parameter is enabled, all other parameters (except for zipFile) will be ignored. The step will return true or false depending on the result instead of throwing an exception. Configuration Files readProperties：Reads a file in the current working directory or a String as a plain text Java Properties file. The returned object is a normal Map with String keys. The map can also be pre loaded with default values before reading/parsing the data. (文档) 参数 defaults (可选，Nested Choice of Objects): An Map containing default key/values. These are added to the resulting map first. file(可选，参数类型为String): path to a file in the workspace to read the properties from. These are added to the resulting map after the defaults and so will overwrite any key/value pairs already present. interpolate (可选，参数类型为boolean): Flag to indicate if the properties should be interpolated or not. In case of error or cycling dependencies, the original properties will be returned. text (可选，参数类型为String): An String containing properties formatted data. These are added to the resulting map after file and so will overwrite any key/value pairs already present. 示例 def d = [test: 'Default', something: 'Default', other: 'Default'] def props = readProperties defaults: d, file: 'dir/my.properties', text: 'other=Override' assert props['test'] == 'One' assert props['something'] == 'Default' assert props.something == 'Default' assert props.other == 'Override' def props = readProperties interpolate: true, file: 'test.properties' assert props.url = 'http://localhost' assert props.resource = 'README.txt' // if fullUrl is defined to ${url}/${resource} then it should evaluate to http://localhost/README.txt assert props.fullUrl = 'http://localhost/README.txt' readManifest：Reads a Jar Manifest file or text and parses it into a set of Maps. The returned data structure has two properties: main for the main attributes, and entries containing each individual section (except for main). (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): path to a file to read. It could be a plain text, .jar, .war or .ear. In the latter cases the manifest will be extracted from the archive and then read. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): text containing the manifest data. 示例 def man = readManifest file: 'target/my.jar' assert man.main['Version'] == '6.15.8' assert man.main['Application-Name'] == 'My App' assert man.entries['Section1']['Key1'] == 'value1-1' assert man.entries['Section2']['Key2'] == 'value2-2' readYaml：Reads a file in the current working directory or a String as a plain text YAML file. It uses SnakeYAML as YAML processor. The returned objects are standard Java objects like List, Long, String, ...: bool: [true, false, on, off] int: 42 float: 3.14159 list: ['LITE', 'RES_ACID', 'SUS_DEXT'] map: {hp: 13, sp: 5}. (文档) 参数 file(可选，参数类型为String) text(可选，参数类型为String) 示例 // 读取单个YAML文件 def datas = readYaml text: \"\"\" something: 'my datas' size: 3 isEmpty: false \"\"\" assert datas.something == 'my datas' assert datas.size == 3 assert datas.isEmpty == false // 读取多个YAML文件 def datas = readYaml text: \"\"\" --- something: 'my first document' --- something: 'my second document' \"\"\" assert datas.size() == 2 assert datas[0].something == 'my first document' assert datas[1].something == 'my second document' // With file dir/my.yml containing something: 'my datas' : def datas = readYaml file: 'dir/my.yml', text: \"something: 'Override'\" assert datas.something == 'Override' writeYaml：Write a YAML file from an object. (文档) 参数 file(参数类型为String): Mandatory path to a file in the workspace to write the YAML datas to. data: A Mandatory Object containing the data to be serialized. charset: Optionally specify the charset to use when writing the file. Defaults to UTF-8 if nothing else is specified. What charsets that are available depends on your Jenkins master system. The java specification tells us though that at least the following should be available: [ US-ASCII、ISO-8859-1、UTF-8、UTF-16BE、UTF-16LE、UTF-16] 示例 ```groovy def amap = ['something': 'my datas', 'size': 3, 'isEmpty': false] writeYaml file: 'datas.yaml', data: amap def read = readYaml file: 'datas.yaml' assert read.something == 'my datas' assert read.size == 3 assert read.isEmpty == false ``` readJSON：Reads a file in the current working directory or a String as a plain text JSON file. The returned object is a normal Map with String keys or a List of primitives or Map. (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the JSON data. Data could be access as an array or a map. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the JSON formatted data. Data could be access as an array or a map. 示例 ```groovy def props = readJSON file: 'dir/input.json' assert props['attr1'] == 'One' assert props.attr1 == 'One' def props = readJSON text: '{ \"key\": \"value\" }' assert props['key'] == 'value' assert props.key == 'value' def props = readJSON text: '[ \"a\", \"b\"]' assert props[0] == 'a' assert props[1] == 'b' ``` writeJSON：Write a JSON file in the current working directory. That for example was previously read by readJSON. (文档) 参数 file(可选，参数类型为String): Path to a file in the workspace to write to. json(Nested Choice of Objects): The JSON object to write. pretty (可选，参数类型为int): Prettify the output with this number of spaces added to each level of indentation. 示例 def input = readJSON file: 'myfile.json' //Do some manipulation writeJSON file: 'output.json', json: input //or pretty print it, indented with a configurable number of spaces writeJSON file: 'output.json', json: input, pretty: 4 readCSV：Reads a file in the current working directory or a String as a plain text. A List of CSVRecord instances is returned. (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the CSV data. Data is accessed as a List of String Array. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the CSV formatted data. Data is accessed as a List of String Arrays. format(可选，org.apache.commons.csv.CSVFormat) 示例 def records = readCSV file: 'dir/input.csv' assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b' assert records[0][0] == 'key' assert records[1][1] == 'b' // 进阶示例 def excelFormat = CSVFormat.EXCEL def records = readCSV file: 'dir/input.csv', format: excelFormat assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b', format: CSVFormat.DEFAULT.withHeader() assert records[1].get('key') == 'a' assert records[1].get('value') == 'b' writeCSV：Write a CSV file in the current working directory. That for example was previously read by readCSV. See CSVPrinter for details.(文档) 参数 file(参数类型为String): Path to a file in the workspace to write to. records(java.lang.Iterable): The list of CSVRecord instances to write. format(可选，org.apache.commons.csv.CSVFormat):See CSVFormat for details. 示例 def records = [['key', 'value'], ['a', 'b']] writeCSV file: 'output.csv', records: records, format: CSVFormat.EXCEL Maven Projects readMavenPom：读取Maven POM文件到一个Model数据结构中. (文档) 参数 file(可选，参数类型为String)：默认读取目前工作区下的POM.xml文件 示例 stage ('上传制品') { steps { script{ def pomfile = readMavenPom file: 'pom.xml' nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] } } } writeMavenPom：Writes a Maven project file. That for example was previously read by readMavenPom. (文档) 参数 model(参数类型为org.apache.maven.model.Model): The Model object to write. file(可选，参数类型为String): Optional path to a file in the workspace to write to. If left empty the step will write to pom.xml in the current working directory. 示例 def pom = readMavenPom file: 'pom.xml' //Do some manipulation writeMavenPom model: pom Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-17 09:16:14 "},"origin/jenkins-Nexus-Platform的使用.html":{"url":"origin/jenkins-Nexus-Platform的使用.html","title":"Nexus Platform Plugin","keywords":"","body":"Preflight 官方插件文档：https://help.sonatype.com/integrations/nexus-and-continuous-integration/nexus-platform-plugin-for-jenkins 安装插件：Pipeline Utility Steps 功能： 一、安装 二、配置 系统管理--> 系统设置--> Sonatype Nexus 三、使用 上传构建后的制品到Nexus的Hosted类型仓库中 Job Declarative Pipeline ```bash stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://nexus-nexus.apps.okd311.curiouser.com/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } ``` 四、注意 如果Job再次构建，产生相同的Jar，上传信息还是一样的，Nexus的Release仓库需要设置为\"允许Redeploy\"。不然，仓库中已经相同版本信息的制品，会造成上传失败 参考链接 https://support.sonatype.com/hc/en-us/articles/115009108987-Jenkins-Publish-Using-Maven-Coordinates-from-the-pom-xml https://www.jianshu.com/p/29403ecf7fc2 https://stackoverflow.com/questions/37603619/extract-version-id-from-pom-in-a-jenkins-pipeline Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/jenkins-配置SMTP邮箱服务.html":{"url":"origin/jenkins-配置SMTP邮箱服务.html","title":"Mail Plugin","keywords":"","body":"Jenkins配置SMTP邮箱服务 Prerequisite 自己邮箱运营商设置了开通SMTP服务 Jenkins 安装了Jenkins Mailers Plugin 一、Context Jenkins默认有个插件叫\"Mailer Plugin\"用来发送通知邮件。该插件使用的\"JavaMail \"来进行配置自定义个邮箱服务器 二、配置 系统管理-->系统设置 配置Jenkins的系统管理员邮箱地址 配置SMTP邮件服务器地址 三、使用 Job中 四、问题 当构建不成功时发送的邮件，内容包含构建的日志。 当初次构建成功时会发送邮件通知，当再次重复构建成功时，则不会发送邮件通知，得等到构建失败时才会再次发送通知邮件 功能太弱，可使用\"Mail Extension\"插件进行功能扩展。详见：jenkins-Mailer邮箱功能扩展插件Email-Extension 不知Jenkins的系统管理员邮箱时，发送会报错 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html":{"url":"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html","title":"Mail Extension","keywords":"","body":"Jenkins Mailer邮箱功能扩展插件Email-Extension 一、Context Jenkins自带的邮件插件功能太弱，有个邮箱扩展插件。 官方文档WIKI：https://wiki.jenkins.io/display/JENKINS/Email-ext+plugin 优势： 邮件格式改为HTML，更美观 使用模板来配置邮件内容 为不同的Job配置不一样的收件人 为不同的事件配置不一样的trigger 在Jenkins pipeline中集成发送邮件通知功能 二、插件安装配置 1、安装 2、配置 三、使用 1、Jobs中 2、Pipeline中 pipeline{ ...上文省略... post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 四、发送HTML格式的邮件 1、Pipeline中 Prerequisite 准备格式化好的HTML ${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志 Jenkins构建信息邮件，请勿回复！ 构建信息 项目名称： ${PROJECT_NAME} 构建编号： ${BUILD_NUMBER} 构建状态： ${BUILD_STATUS} 构建人员： ${GITLABUSERNAME} 构建日志： 见附件 Jenkins构建页面： ${BUILD_URL} 变更代码： ${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit} 单元测试报告： ${BUILD_URL}jacoco 测试报告 特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖 ${FILE,path=\"./target/site/jacoco/index.html\"} 使用pipeline语法生成器生成pipeline 压缩pipeline. (压缩HTML源代码的工具网站：http://tool.oschina.net/jscompress?type=2) pipeline{ ...上文省略... post { always { emailext attachLog: true, body: '''${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志Jenkins构建信息邮件，请勿回复！构建信息项目名称： ${PROJECT_NAME}构建编号： ${BUILD_NUMBER}构建状态： ${BUILD_STATUS}构建人员： ${GITLABUSERNAME}构建日志： 见附件Jenkins构建页面：${BUILD_URL}变更代码：${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit}单元测试报告：${BUILD_URL}jacoco测试报告特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖${FILE,path=\"./target/site/jacoco/index.html\"}''', mimeType: 'text/html', subject: '项目构建报告：$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-29 13:29:25 "},"origin/jenkins-gitlab插件的使用.html":{"url":"origin/jenkins-gitlab插件的使用.html","title":"Gitlab","keywords":"","body":"Jenkins Gitlab插件的使用 一、Overviews Gitlab插件能接受Gitlab的一些事件Webhook来触发Jenkins的构建，并且能将构建状态同步到Gitlab中. 插件Github：https://github.com/jenkinsci/gitlab-plugin 利用此插件可实现如下效果： 二、安装 Jenkins -> 系统管理 ->插件管理->可用插件->搜索\"gitlab\" 插件下载地址：https://plugins.jenkins.io/gitlab-plugin 三、配置 1、Gitlab发送Webhook到Jenkins时的安全认证配置 Gitlab代码仓库配置web hook时需要Jenkins Gitlab插件的Token,来加密验证webhook的安全性 方式一：全局性的认证Token ① Jenkins创建新用户（授予Job/Build权限即可） ② 获取新用户的User ID和API Token ③ Gitlab配置web hook填写URL时，使用http://USERID:APITOKEN@JENKINS_URL/project/YOUR_JOB即可（Secret Token可忽略） 方式二(推荐)：单独项目Jenkins Job的认证Token 在Gitlab中配置Webhook时，将图中的Token填入即可 方式三(不推荐)：不使用认证 ① Manage Jenkins -> Configure System -> GitLab section -> 取消勾选 \"Enable authentication for '/project' end-point\" 2、Jenkins回写构建状态到Gitlab时需要的认证授权配置 Jenkins通过Gitlab API将构建状态回写到对应代码仓库时，需要有权限在代码仓库中创建评论，更改状态等 步骤一 ① Gitlab创建新用户（建议命名用户名时尽量见名知意，例如：Jenkins） ② 登陆新用户，获取Access Tokens,Token权限只要api即可(及时复制Token,只显示一次) ③ 在Jenkins中创建GitLab API token类型的凭据，在API token字段中保存上一步获取的Token ④ 在Jenkins Gitlab插件中配置gitlab Server相关信息 步骤二 对应代码仓库配置Jenkins Job时要将该Token所属的用户加入members中，授予Developers角色或有更高权限的角色 五、Jenkins Job中配置Webhook 1、配置Gitlab Buid Trigger 2、配置构建后Gitlab回写信息的动作 添加构建后动作\"Pushlish build status to Gitlab\" 之后会在Gitlab代码仓库的CI/CD-->Pipeline和Merge Request中看到构建状态图标 添加构建后动作\"Add note with build status on Gitlab merge request\" 之后会根据构建状态的不同在Gitlab的Merge Request中添加不同的comment评论 添加构建后动作\"Add Vote for build status on Gitlab merge request\" 之后会根据构建状态的不同在Gitlab的Merge Request中显示不同的投票 添加构建后动作\"Accept Gitlab merge request on success\" 该动作会在build构建成功后在Gitla上自动同意接受Merge Request 不添加构建后动作 Gitlab的事件只会触发Jenkins构建，不会回写任何信息到gitlab对应代码仓库中 五、Gitlab 代码仓库配置Webhook 详见：gitlab-配置代码仓库事件触发器Webhook 六、获取Webhook HTTP请求信息的变量 对于Gitlab发送过来的Webhook HTTP POST请求信息，可直接通过以下变量来获取。 gitlabBranch gitlabSourceBranch gitlabActionType gitlabUserName gitlabUserEmail gitlabSourceRepoHomepage gitlabSourceRepoName gitlabSourceNamespace gitlabSourceRepoURL gitlabSourceRepoSshUrl gitlabSourceRepoHttpUrl gitlabMergeRequestTitle gitlabMergeRequestDescription gitlabMergeRequestId gitlabMergeRequestIid gitlabMergeRequestState gitlabMergedByUser gitlabMergeRequestAssignee gitlabMergeRequestLastCommit gitlabMergeRequestTargetProjectId gitlabTargetBranch gitlabTargetRepoName gitlabTargetNamespace gitlabTargetRepoSshUrl gitlabTargetRepoHttpUrl gitlabBefore gitlabAfter gitlabTriggerPhrase Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/gitlab-配置SMTP邮件服务.html":{"url":"origin/gitlab-配置SMTP邮件服务.html","title":"配置SMTP邮件服务","keywords":"","body":"1. 修改/etc/gitlab/gitlab.rb ### Email Settings gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = 'rationalmonster@163.com' gitlab_rails['gitlab_email_display_name'] = 'Curiouser163SMTPServer' gitlab_rails['gitlab_email_reply_to'] = 'rationalmonster@163.com' gitlab_rails['gitlab_email_subject_suffix'] = 'Gitlab' gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.163.com\" gitlab_rails['smtp_port'] = 25 gitlab_rails['smtp_user_name'] = \"******@163.com\" gitlab_rails['smtp_password'] = \"******\" gitlab_rails['smtp_domain'] = \"163.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = false gitlab_rails['smtp_tls'] = false 2. 测试发送邮件 # gitlab-rails console Loading production environment (Rails 4.2.10) # irb(main):001:0> Notify.test_email('******@163.com','gitlab send mail test','gitlab test mail').deliver_now Notify#test_email: processed outbound mail in 539.6ms Sent mail to ******@163.com (397.8ms) Date: Thu, 04 Jul 2019 15:07:33 +0000 From: Curiouser163SMTPServer Reply-To: Curiouser163SMTPServer To:******@163.com Message-ID: Subject: gitlab send mail test Mime-Version: 1.0 Content-Type: text/html; charset=UTF-8 Content-Transfer-Encoding: 7bit Auto-Submitted: auto-generated X-Auto-Response-Suppress: All gitlab test mail=> #, >, >, , >, , , , , , > irb(main):002:0> Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/gitlab-配置代码仓库事件触发器Webhook.html":{"url":"origin/gitlab-配置代码仓库事件触发器Webhook.html","title":"代码仓库配置事件触发器Webhook","keywords":"","body":"Gitlab 代码仓库配置事件Webhoook触发器 一、Overviews Webhook与异步编程中\"订阅-发布模型\"非常类似，一端触发事件，一端监听执行。 WebHook就是一个接收HTTP POST（或GET，PUT，DELETE）的URL 通常来说，WebHook通过HTTP协议将请求数据发送到外部系统后，就不再处理响应数据啦！ 二、Gitlab事件触发器配置Webhook 当Gitlab中代码仓库发生了一些事件后，可设置触发器发送http通知到外部系统。通常配置Jenkins上的Webhook 配置Gitlab的网络限制 否则后续发送webhook请求时会报错\"Requests to the local network are not allowed\",原因详见：附录 配置代码代码仓库的Webhook 配置完可以模拟一些事件进行测试 查看详细的Webhook请求信息 附录：gitlab发送webhook请求时报错原因 If you have non-GitLab web services running on your GitLab server or within its local network, these may be vulnerable to exploitation via Webhooks. With Webhooks, you and your project maintainers and owners can set up URLs to be triggered when specific things happen to projects. Normally, these requests are sent to external web services specifically set up for this purpose, that process the request and its attached data in some appropriate way. Things get hairy, however, when a Webhook is set up with a URL that doesn't point to an external, but to an internal service, that may do something completely unintended when the webhook is triggered and the POST request is sent. Because Webhook requests are made by the GitLab server itself, these have complete access to everything running on the server (http://localhost:123) or within the server's local network (http://192.168.1.12:345), even if these services are otherwise protected and inaccessible from the outside world. If a web service does not require authentication, Webhooks can be used to trigger destructive commands by getting the GitLab server to make POST requests to endpoints like \"http://localhost:123/some-resource/delete\". To prevent this type of exploitation from happening, starting with GitLab 10.6, all Webhook requests to the current GitLab instance server address and/or in a private network will be forbidden by default. That means that all requests made to 127.0.0.1, ::1 and 0.0.0.0, as well as IPv4 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 and IPv6 site-local (ffc0::/10) addresses won't be allowed. This behavior can be overridden by enabling the option \"Allow requests to the local network from hooks and services\" in the \"Outbound requests\" section inside the Admin area under Settings (/admin/application_settings): Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-26 13:50:40 "},"origin/nexus-简介.html":{"url":"origin/nexus-简介.html","title":"Nexus","keywords":"","body":"一、Nexus简介 Nexus是Sonatype公司出的一款目前最为流程的构件仓库管理软件，主要用于局域网内部的构件管理，代理访问外部仓库等。例如对于公司私有的Java制品Jar包，可上传至Nexus的Maven类型仓库中进行集中管理；代理访问阿里云Maven仓库，缓存加速获取互联网上的Java制品。Nexus使用Lucene提供了强大的构件搜索功能，拥有丰富的RestFul API接口用于管理控制，支持WebDAV和LDAP安全身份认证，基于RBAC的权限访问控制等功能。市面上同类产品有Apache的Archiva和JFrog的Artifatory。 Nexus分为免费开源版OSS和收费商业版Professional 二、仓库类型 Proxy 类型仓库主要用于代理缓存访问外网上其他公开的仓库，将每次从代理仓库拉取的制品缓存到nexus文件系统中，下次再拉取相同版本制品时就不需再次从外网拉取，起到代理访问缓存的功能 Hosted 类型的仓库主要用于存放各个项目组产出的、用于共享、不能放到公网上、私有的制品。有两种版本策略，一种是Snapshots版本策略类型的，对于相同版本制品的上传，nexus会自动追加时间戳加以区分；一种是Release版本策略类型的，对于相同的制品，要明确版本，不能存放相同版本。可以理解为snapshots仓库存放一些内容变更频繁的制品，这样不管上传还是使用时不用频繁变更版本号就能拉取到最新版本。而release仓库存放一些内容稳定变更少的制品，使用时指定好版本就行，无需经常变动 Group 类型仓库主要用于组合其他仓库，统一对外使用方式。可设置组仓库组合其他仓库的顺序。例如组合顺序为先拉取maven格式aliyun代理仓库中的制品，如果其中没有想要的制品，再去拉取maven格式Central代理仓库中的制品。如果还没有，就去maven格式hosted类型仓库中拉取，直到遍历完所有的组合仓库。同时，拉取使用时不需要配置那么多的仓库地址，只需要配置group仓库地址就行 三、官方高可用方案 官方收费版的高可用方案 四、Kubernetes部署 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: replicas: 1 selector: matchLabels: app: nexus3 strategy: type: Recreate template: metadata: labels: app: nexus3 spec: imagePullSecrets: - name: harbor-secrets initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /nexus-data'] volumeMounts: - name: nexus3-data mountPath: /nexus-data containers: - env: - name: CONTEXT_PATH value: / - name: TZ value: 'Asia/Shanghai' image: docker.io/sonatype/nexus3:3.15.2 imagePullPolicy: IfNotPresent readinessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 name: nexus3 ports: - containerPort: 8081 protocol: TCP resources: limits: memory: 4096Mi requests: memory: 2048Mi terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /nexus-data name: nexus3-data dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: nexus3-data persistentVolumeClaim: claimName: nexus3-pv --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: nexus3 name: nexus3-pv namespace: nexus3 spec: accessModes: - ReadWriteMany resources: requests: storage: 1000Gi --- apiVersion: v1 kind: Service metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: ports: - name: 8081-tcp port: 8081 protocol: TCP targetPort: 8081 selector: app: nexus3 sessionAffinity: None type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nexus3 namespace: nexus3 spec: rules: - host: nexus.curiouser.com http: paths: - path: / backend: serviceName: nexus3 servicePort: 8081 五、常见仓库配置 YUM格式制品 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库 yum-ansible：https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ yum-ustc： http://mirrors.ustc.edu.cn/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库 yum-hosted Maven格式制品 Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库 maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库 maven-snapshots maven-releases NPM格式制品 Group类型仓库 npm npm-taobao npm-cnpm npm-hosted Proxy类型仓库 npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库 npm-hosted（上传权限需修改realms) Docker格式制品 Group类型仓库 docker（设置http-port:8082） Proxy类型仓库 docker-io： https://registry-1.docker.io Hosted类型仓库 docker-hosted（设置http-port:8083） Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/nexus-使用OrientDB Console在DB层面修改配置.html":{"url":"origin/nexus-使用OrientDB Console在DB层面修改配置.html","title":"使用OrientDB Console在DB层面修改配置","keywords":"","body":"Nexus使用OrientDB Console在DB层面修改配置 一、Context 在配置Nexus对接LDAP过程中，原先的用户（admin和原来创建的）也是可以登录的，为了测试只让LDAP用户登录，将安全域中的Local Authorizing Realm和Local Authenticating Realm给去掉了，导致admin用户登陆不上，LDAP上的用户能登陆，但没有管理Nexus的权限。换句话说，这个Nexus成了僵尸。如果Nexus的admin密码忘了,情形也类似。都需要修改Nexus数据库中的用户数据才能修改相关配置（Nexus是将用户、配置等相关信息存放在OrientDB 数据库中，而不是在配置文件中，所以改密码或者在没有权限的情况下改配置都是要操作OrientDB 中的数据） 相关信息 Nexus是在Openshift(Kubernetes也一样)中容器化部署的 Nexus版本：3.15.2 Nexus的数据目录是单独使用NFS类型的PV挂载的 openshift上Nexus容器的执行用户没有权限操作除持久化目录之外的目录。（以root用户起的Nexus进程可直接使用Nexus容器中的”/opt/nexus/lib/support/nexus-orient-console.jar“连接OrientDB ） 问题解决思路 将Nexus的PV数据目录挂载到物理节点一个临时目录下 使用Nexus相同版本安装包中的OrientDB Console连接到临时数据目录中的DB，然后修改相关信息 Note: 如果容器中执行Nexus进程的用户和物理节点上的用户不一样导致权限不够的话，可暂时将临时数据目录中的所有文件权限设置为777，修改完再改回来 再将Nexus的PV数据目录挂载到openshift上Nexus容器中，重启Nexus 二、Preflight Nexus的PV数据目录已挂载到/roor/test 下载相同版本的Nexus到/opt/目录下 /roor/test下的所有文件及文件夹权限临时修改为777 三、操作 1. 使用官方的OrientDB Console连接到数据目录中的DB java -jar /opt/nexus-3.15.2-01/lib/support/nexus-orient-console.jar 此时会进入OrientDB 控制台 # OrientDB console v.2.2.36 (build d3beb772c02098ceaea89779a7afd4b7305d3788, branch 2.2.x) https://www.orientdb.com # Type 'help' to display all the supported commands. orientdb> #输入exit退出 2. 连接临时数据目录中的DB orientdb> connect plocal:/root/test/db/security admin admin 此时会连到一个security的Database 3. 重置Realm 如果从活动列表中删除了缺省安全域，则缺省管理员用户(即使用户名密码正确)也无法进行身份验证 orientdb> delete from realm Note: 重置后。默认的安全域将被激活，任何自定义的安全域都将被删除。后续再UI界面进行添加 4.(可选) 重置admin用户密码为默认值\"admin123\" orientdb> update user SET password=\"$shiro1$SHA-512$1024$NE+wqQq/TmjZMvfI7ENh/g==$V4yPw8T64UQ6GfJfxYq2hLsVrBY8D1v+bktfOxGdt4b/9BthpWPNUy/CBk6V9iA0nHpzYzJFWO8v/tZFtES8CA==\" UPSERT WHERE id=\"admin\" 5.(可选) 重置admin为管理员 如果admin用户不在是“nx-admin”管理员角色 orientdb> select * from user_role_mapping where userID = \"admin\" orientdb> update user_role_mapping set roles = [\"nx-admin\"] where userID = \"admin\" orientdb> select status from user where id = \"admin\" orientdb> update user set status=\"active\" upsert where id=\"admin\" 6. 退出OrientDB 控制台，修改回临时数据目录所有文件的原始权限，重新将数据目录挂载到容器上，然后重启Nexus Bazinga，admin用户能正常，又重新夺回Nexus的管理权限，所有的仓库配置和数据没有丢失，重新将LDAP的安全域加回去，一切恢复原样。 附录：Nexus使用的OrientDB 1. What is the OrientDB Console Nexus 3 uses several OrientDB databases. In very specific circumstances, these databases can be manipulated as advised by Sonatype support. This article describes how to open a special command-line interface for connecting to and working with the databases used by Nexus. This interface is known as the OrientDB Console. Caution: Using the console incorrectly can cause irreparable harm to the databases. 2. Launching the OrientDB Console on Nexus 3.2.1 and Newer Nexus 3.2.1+ includes a single jar executable which can launch the OrientDB console. As the operating system user account that typically owns the Nexus Repository Manager process, start a terminal session on the host where Nexus is installed. Change directories to your application directory. Launch the console using the same version of Java executable that Nexus is using : Unix java -jar ./lib/support/nexus-orient-console.jar Windows java -jar lib\\support\\nexus-orient-console.jar Mac .install4j/jre.bundle/Contents/Home/jre/bin/java -jar ./lib/support/nexus-orient-console.jar You should be presented with a command-line interface such as this, ready to accept your commands: # OrientDB console v.2.2.16 www.orientdb.com #Type 'help' to display all the supported commands. orientdb> # When you are done your commands, type exit to quit the console. The nexus-orient-console.jar sets up the correct classpath to successfully launch the console. Launching the jar from another location is not supported 参考连接 https://support.sonatype.com/hc/en-us/articles/115002930827-Accessing-the-OrientDB-Console https://support.sonatype.com/hc/en-us/articles/213467158-How-to-reset-a-forgotten-admin-password-in-Nexus-3-x Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-29 17:30:14 "},"origin/nexus-设置SMTP邮件服务.html":{"url":"origin/nexus-设置SMTP邮件服务.html","title":"设置SMTP邮件服务","keywords":"","body":" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/nexus-maven仓库的配置与使用.html":{"url":"origin/nexus-maven仓库的配置与使用.html","title":"Maven","keywords":"","body":"Maven仓库的配置与使用 一、Overview 有了Maven仓库之后，当 Maven 需要下载构件时，直接请求Nexus的maven仓库，仓库中存在则下载到本地仓库；否则，nexus请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载 有了Maven仓库之后，当Maven需要下载构件时，直接请求Nexus，Maven仓库上存在则下载到本地仓库；Maven仓库上不存在的话，Nexus请求外部的远程仓库，将构件下载到Maven仓库，再提供给本地仓库下载。 8e3e97dde794e5413f80df5a8b21234616159073 Maven格式制品仓库配置 三、 四、 ======= Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库 maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库 maven-snapshots maven-releases maven格式的仓库有两种典型的使用场景，一个是顺序拉取group组仓库中组合仓库里的制品，一个是上传制品到hosted类型的仓库中，例如maven-snapshots仓库。而maven配置nexus中的仓库有三个地方，配置最终生效的顺序为全局配置文件--->用户配置文件---->POM文件： maven的全局配置文件settings.xml中,该配置文件在maven安装目录conf文件夹下 maven的用户配置文件settings.xml中,该配置文件在用户目录.m2文件夹下 项目的POM文件 二、代理仓库的使用 在Maven用户配置文件setting.xml中添加maven格式group仓库的地址 .....上文省略...... curiouser-maven microservices microservices用户的密码 curiouser-maven * The Maven repository of curiouser http://nexus-ip地址:8081/repository/maven/ .....下文省略...... 三、发布制品到Maven的Hosted仓库 1、mvn deploy 在Maven用户配置文件setting.xml中添加snapshot、release仓库的id .....上文省略...... maven-releases microservices microservices用户的密码 maven-snapshots microservices microservices用户的密码 .....下文省略...... 在项目POM.xml文件中添加Snapshots仓库或Release仓库的地址 .....上文省略...... maven-releases User Project Release http://nexus-ip地址:8081/repository/maven-releases/ maven-snapshots User Project SNAPSHOTS http://nexus-ip地址:8081/repository/maven-snapshots/ .....下文省略...... 然后mvn deploy 2、Curl手动上传 curl -v -u microservices:microservices用户的密码 --upload-file springboot2-0.0.0.jar http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.jar curl -v -u microservices:microservices用户的密码 --upload-file pom.xml http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.pom 3、mvn命令手动上传 前提是Maven用户配置文件setting.xml中已经添加了snapshot、release仓库的id Linux mvn deploy:deploy-file \\ #要上传的Jar包路径 \\ -Dfile=/root/websocket-server-9.4.11.v20180605.jar \\ #Jar包Maven坐标的GroupID \\ -DgroupId=org.eclipse.jetty.websocket \\ #Jar包Maven坐标的ArtifactID \\ -DartifactId=websocket-server \\ #Jar包Maven坐标的Version \\ -Dversion=9.4 \\ #要上传到仓库的制品类型，该值还可以是pom -Dpackaging=jar \\ #maven私服hosted类型仓库的地址 -Durl=http://nexus-ip地址:8081/repository/maven-releases/ \\ #maven私服hosted类型仓库的repositoryid -DrepositoryId=maven-releases Windows mvn deploy:deploy-file ^ -Dfile=D:\\websocket-server-9.4.11.v20180605.jar ^ -DgroupId=org.eclipse.jetty.websocket ^ -DartifactId=websocket-server ^ -Dversion=9.4 ^ -Dpackaging=jar ^ -Durl=http://nexus-ip地址:8081/repository/maven-releases/ ^ -DrepositoryId=maven-releases 4、UI界面上传 5、使用Postman上传 注意事项 Jar包上传只能上传hosted类型的仓库中，Proxy和Group类型的无法上传。同时，注意有些仓库设置禁止了上传 hosted类型的Snapshot仓库默认设置的上传版本控制为只能上传以“-snapshot”结尾版本的制品。所以版本为非“-snapshot”结尾的Jar包无法上传到snapshot仓库中 如果Nexus禁止匿名用户访问时，匿名用户是被禁止拉取maven仓库的Jar包。所以可以创建一个对maven仓库类型有读取权限的用户，在settings.xml文件进行配置。 附录：如何在maven的配置文件settings.xml中使用加密过的用户密码 在Maven的settings.xml中，往往要配置访问远程库所在的服务器的username/password。但是明文的密码总是显得那么扎眼，必欲除之而后快。Apache Maven项目提供了便捷的密码加密机制，该机制的最近更新时间为2018-03-06。该机制目前只支持在命令行下的操作，如生成密码的密文。此外，用户还需要在${user.home}/.m2目录下配置settings-security.xml文件，其中包含： 用以加密其他密码的master password（此处也是密文） 或指向另一个保密文件的完整路径 在该加密机制中有两个概念，一个是master password，即用以加密其他密码的密码，另一个就是实际使用的服务器访问密码password。master password的密文配置在settings-security.xml文件中，而服务器访问密码password的密文就可以大大方方地配置在settings.xml中。具体用法如下： 生成Master password的密文 mvn --encrypt-master-password 根据提示输入Master password: 就可以生成密文{iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} 配置${user.home}/.m2/settings-security.xml文件(如果没有就手动创建) {iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} # 如果settings-security.xml文件被保存到U盘，则配置${user.home}/.m2/settings-security.xml文件如下： /my_u_volume/my_path/settings-security.xml 加密访问服务器的密码 mvn --encrypt-password 根据提示输入Password: ​ 就可以生成密文{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=} 配置settings.xml文件 my.server myfoo add_any_comment or \\{\\{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=\\}\\} add_any_comment 参考链接 https://blog.csdn.net/taiyangdao/article/details/79500507 8e3e97dde794e5413f80df5a8b21234616159073 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/nexus-npm仓库的配置与使用.html":{"url":"origin/nexus-npm仓库的配置与使用.html","title":"NPM","keywords":"","body":"NPM仓库的配置与使用 一、npm仓库的配置信息 Group类型仓库 NPM npm-taobao npm-cnpm npm-cnpm Proxy类型仓库 npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库 npm-hosted 二、使用NPM仓库 npm config set registry http://nexus-ip地址:8081/repository/NPM/ 三、发布制品到NPM的Hosted仓库 echo \"hello\" >> test npm init # package name: (test) sadsada # version: (1.0.0) # description: # entry point: (index.js) test # test command: # git repository: # keywords: # author: # license: (ISC) # About to write to /root/test/package.json: # { # \"name\": \"sadsada\", # \"version\": \"1.0.0\", # \"description\": \"\", # \"main\": \"f\", # \"scripts\": { # \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" # }, # \"author\": \"\", # \"license\": \"ISC\" # } npm login --registry http://Nexus-IP地址:8081/repository/NPM-Hosted/ # Username: admin #Password: ***** # Email: (this IS public) asdad@sada.com npm publish -registry http://Nexus-IP地址:8081/repository/NPM-Hosted/ 四、使用nrm工具切换npm仓库 Github地址：https://github.com/Pana/nrm 帮助快速切换npm仓库源。默认已经配置了npm、yarn、taobao、cnpm、nj、npmMirror、edunpm等常见的仓库源。 1. 安装 npm install nrm -g 2. 命令详解 $ nrm -h Usage: nrm [options] [command] Options: -V, --version output the version number -h, --help output usage information Commands: ls List all the registries current Show current registry name use Change registry to registry add [home] Add one custom registry set-auth [options] [value] Set authorize information for a custom registry with a base64 encoded string or username and pasword set-email Set email for a custom registry set-hosted-repo Set hosted npm repository for a custom registry to publish packages del Delete one custom registry home [browser] Open the homepage of registry with optional browser publish [options] [|] Publish package to current registry if current registry is a custom registry. if you're not using custom registry, this command will run npm publish directly test [registry] Show response time for specific or all registries help Print this help 3. 常用命令 查看默认支持的npm 仓库 $ nrm ls * npm -------- https://registry.npmjs.org/ yarn ------- https://registry.yarnpkg.com/ cnpm ------- http://r.cnpmjs.org/ taobao ----- https://registry.npm.taobao.org/ nj --------- https://registry.nodejitsu.com/ npmMirror -- https://skimdb.npmjs.com/registry/ edunpm ----- http://registry.enpmjs.org/ # \"*\"编注的仓库代表当前使用的仓库 添加私有的npm仓库 nrm add curiouser http://nexus.apps.okd311.curiouser.com/repository/NPM 切换npm仓库 nrm use 仓库名 删除仓库 nrm del 仓库名 测试仓库速度 nrm test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-02 09:37:33 "},"origin/nexus-yum仓库的配置与使用.html":{"url":"origin/nexus-yum仓库的配置与使用.html","title":"YUM","keywords":"","body":"YUM仓库的配置与使用 一、Overviews 之前搭建内网YUM源仓库都是使用HTTP和createrepo做的，每次还要手动去同步外网镜像源最新的RPM包，繁琐耗时。最近发现新版的Nexus已经有可以做YUM源的功能，能像Maven仓库那样，没有相关资源的时候去外网拉，有的话就不去外网啦。即节省存储，又加快速度，还能自动更新RPM版本。 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库 yum-ustc：http://mirrors.ustc.edu.cn/ yum-tuna：https://mirrors.tuna.tsinghua.edu.cn/ yum-163：http://mirrors.163.com/ yum-ansible：https://releases.ansible.com/ansible/rpm/releasepel-7-x86_64/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库 yum-hosted（\"Repodata Depth\"设置为“1”） 二、手动上传RPM包到Hosted仓库 上传RPM包到Hosted仓库，相关的RPM元信息文件夹repodata会自动生成，不再需要createrepo生成啦（如果没有自动生成，点击仓库的Rebuild Index，然后稍等即可） $ ls -l | grep ^[^d] | awk '{print $9}' mysql-community-client-5.7.19-1.el7.x86_64.rpm mysql-community-common-5.7.19-1.el7.x86_64.rpm mysql-community-devel-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-compat-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-devel-5.7.19-1.el7.x86_64.rpm mysql-community-libs-5.7.19-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.19-1.el7.x86_64.rpm mysql-community-minimal-debuginfo-5.7.19-1.el7.x86_64.rpm mysql-community-server-5.7.19-1.el7.x86_64.rpm mysql-community-server-minimal-5.7.19-1.el7.x86_64.rpm mysql-community-test-5.7.19-1.el7.x86_64.rpm #一次只能上传一个RPM文件， #为了一个Host仓库存放不同软件的不同版本，上传时需要指定上传到Hosted仓库的哪个目录下，以文件夹名来区分当前上传的RPM包是哪个版本的。 $ for i in `ls *rpm` ;do curl -v --user 'admin:admin123' --upload-file $i http://nexus-ip地址:8081/repository/yum-nexus-MySQL/mysql5.7.19/;done 三、YUM仓库的使用 [centos] name=Nexus Yum baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/os/$basearch/ enabled=1 gpgcheck=0 [centos-extras] name=Nexus Yum Extras baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/extras/$basearch/ enabled=1 gpgcheck=0 [epel] name=Nexus Epel baseurl=http://nexus-ip地址:8081/repository/yum-nexus/epel/$releasever/$basearch/ enabled=1 gpgcheck=0 [openshift] name=Nexus openshift 3.11 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/paas/$basearch/openshift-origin311/ enabled=1 gpgcheck=0 [mysql5.7] name=Nexus MySQL 5.7.19 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/mysql5.7.19 enabled=1 gpgcheck=0 注意 不要代理多个外网的YUM源。例如同时代理缓存网易的163镜像源和清华大学的镜像源，这些外网镜像源中的RPM都大差无几，配置多个的话，使用group去代理拉RPM的时候，会优先选择group中靠前的外网代理镜像源。有时（偶尔其中一个外网镜像源不能用）会出现相同的RPM存在于每一个proxy仓库中，浪费存储。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-02 09:49:05 "},"origin/nexus-数据的备份恢复.html":{"url":"origin/nexus-数据的备份恢复.html","title":"数据备份恢复","keywords":"","body":"Nexus的数据备份与恢复 Nexus的备份分为两个部分。一个是元信息和配置信息数据库的备份，一个是Blob存储的备份 一、配置DB的备份操作 二、Blob存储的备份操作 Nexus的blob stores可以简单的理解为一个文件夹，存放着各种制品的原始文件，例如原始的Java制品Jar包，POM文件，War包等文件。Blob的类型可以是文件系统的一个文件夹，也可以是S3对象存储的一个存储Buckets。（默认是文件系统类型，仅支持S3对象存储）一个blob存储可以被一个或者多个仓库组使用 三、Kubernetes上Nexus的数据备份 由于nexus在Kubernetes上的实例使用了CephFS类型的PVC存储作为Volume挂载在到数据目录下。所以，只要将Nexus容器持久化卷的PV对应Ceph路径挂载到某个目录下，拷贝其中的数据库DB目录和Blob目录进行数据备份。 mkdir test nexus3-k8s-2019-5-6-bak mount -t ceph jk1:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-25c75aeb-6f2b-11e9-ac5b-9209ee33fdea test -o name=admin,secret=AQCxFKtcDGd1AhAAvytw5KlaeuApSi1a3G2iwA== cp -r test/db test/blob nexus3-k8s-2019-5-6-bak umount test 四、数据恢复操作 数据恢复时使用的版本与旧版本的差异仅限于小版本号 暂停旧的Nexus POD 挂载旧的Nexus CephFS PV 到本地目录 将备份提拷贝新的CephFS PV中 修改备份目录的权限 重启新的Nexus POD 五、备份策略优化 hosted类型的仓库可使用单独的Blob存储，备份时只备份该Blob。Proxy类型的仓库可不用备份。 可添加执行脚本类型的定时任务做备份，将新增Blob同步到其他Nexus实例中 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/nexus-api.html":{"url":"origin/nexus-api.html","title":"API","keywords":"","body":"Nexus API 一、Context 官网API文档：https://help.sonatype.com/repomanager3/rest-and-integration-api 二、Search API Search API用于搜索component和asset。 GET /service/rest/v1/search 1、Search Components 例如在maven-central仓库中搜索\"group=org.osgi\"的component $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search?repository=maven-central&group=org.osgi' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDoyZTQ3ZGRhMGYxYjU1NWUwNzE1OWRjOWY5ZGQzZmVmNA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.osgi\", \"name\" : \"org.osgi.core\", \"version\" : \"4.3.1\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ] } ], \"continuationToken\" : null } 2、Search Assets 例如在maven-central仓库中搜索\"group=org.osgi\"的assets $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets?repository=maven-central&group=org.osgi' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ], \"continuationToken\" : null } 3、Search and Download Asset 用于搜索一个资产，然后将请求重定向到该资产的downloadUrl GET /service/rest/v1/search../assets/download 例如获取一个maven坐标为\"groupId=com.curiosuer，artifactId=SpringBoot2，version=0.0.0\"Jar包的下载链接 $ curl -v -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets/download?maven.groupId=com.curiosuer&maven.artifactId=SpringBoot2&maven.baseVersion=0.0.0&maven.extension=jar&maven.classifier' 浏览器中 三、Repositories API 1、List Repositories 获取用户能访问到的Repository仓库列表 GET /service/rest/v1/repositories $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/repositories' [ { \"name\" : \"YUM-Hosted\", \"format\" : \"yum\", \"type\" : \"hosted\", \"url\" : \"http://localhost:8081/repository/YUM-Hosted\" }, ... ] 此endpoint返回所有存储库，并且不允许分页。 注意，存储库的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 四、Assets API 1、List Assets 列出指定Repository仓库中包含的Assets GET /service/rest/v1../assets 例如列出maven-central仓库中的Assets $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets?repository=Maven-Releases' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/Maven-Releases/com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"path\" : \"com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"id\" : \"TWF2ZW4tUmVsZWFzZXM6MzZlM2RlYzhkZTUyOGM5YmRkYTdhZTNjZjlmYjFiNTY\", \"repository\" : \"Maven-Releases\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"c1ab61e9f407cbabaa8f3b377a76afa1f8afa4f1\", \"md5\" : \"5474cd7fc95a581eb6b6a3319c8aa6ba\" }, ... ], \"continuationToken\" : \"3f5cae01760233b6506547dc7be10e0b\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。 注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 2、Get Asset GET /service/rest/v1../assets/{id} This endpoint allows us to get the details of an individual asset. $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"path\" : \"org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a3bf672b3ea844575acba3b84790e76ed86a7c66\", \"md5\" : \"49e439c814c3098450dc4bbee952463f\" }} 3、Delete Asset DELETE /service/rest/v1../assets/{id} This endpoint can be used to delete an individual asset. $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:41:47 GMT ... 五、Components API 1、List Components 遍历仓库中的Components GET /service/rest/v1/components $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components?repository=Maven-Central' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjMyNjhmMjIwZTQ1ZDdkZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"com.google.guava\", \"name\" : \"guava\", \"version\" : \"21.0\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MzA4OThiZjZmZTFkOTE2NA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"3a3d111be1be1b745edfa7d91678a12d7ed38709\", \"md5\" : \"ddc91fd850fa6177c91aab5d4e4d1fa6\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5MDc0MDFlYzRjNjVlNjU5OQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a1ff60cb911e1f64801c03d03702044d10c9bdd3\", \"md5\" : \"e34b8695ede1677ba262411d757ea980\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlOWJjNDgzOGE1MzM2OGZlZg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"fe4fa08a8c0897f9896c7e278fb397ede4a2feed\", \"md5\" : \"5c10f97af2ce9db54fa6c2ea6997a8d7\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmZDA3NDdkNjlhZDNmZjI5Nw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"992b43ab7b3a061be47767e910cab58180325abc\", \"md5\" : \"33aed29aa0bb4e03ea7854066a5b4738\" } } ] }, ... ], \"continuationToken\" : \"88491cd1d185dd136f143f20c4e7d50c\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。 注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 2、Get Component 获取仓库中component的详细信息 GET /service/rest/v1/components/{id} $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.apache.httpcomponents\", \"name\" : \"httpcomponents-client\", \"version\" : \"4.3.5\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2YTFhOGUxOGQxZmFkOGM3Mw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"95d80a44673358a5dcbcc2f510770b9f93fe5eba\", \"md5\" : \"f4769c4e60799ede664414c26c6c5c9d\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5ZDU3YjFlYjE0MzM1ZTcwMQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"6b98f5cef5d7102f8f45215bdcf48dc843d060af\", \"md5\" : \"f3b3ac640853fcb887621d13029a1747\" } } ] } 3、Upload Component 上传Component到指定仓库中，一些格式的仓库允许上传的Component中包含多个Assets。 endpoint的上传参数取决于要上传Component到那个仓库的格式 POST /service/rest/v1/components $ curl -v -u admin:admin123 \\ -X POST 'http://localhost:8081/service/rest/v1/components?repository=maven-releases' \\ -F maven2.groupId=com.google.guava \\ -F maven2.artifactId=guava \\ -F maven2.version=24.0-jre \\ -F maven2.asset1=@guava-24.0-jre.jar \\ -F maven2.asset1.extension=jar \\ -F maven2.asset2=@guava-24.0-jre-sources.jar \\ -F maven2.asset2.classifier=sources \\ -F maven2.asset2.extension=jar HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... ①Maven Maven format allows multiple assets to be uploaded as part of a single component. To upload multiple assets just follow the information from a table describing the given format and replace assetN with multiple instances of it (e.g. asset1, asset2, etc.): Field name Field type Required? Description maven2.groupId String Yes, unless a POM asset is included in the upload Group ID of the component maven2.artifactId String Yes, unless a POM asset is included in the upload Artifact ID of the component maven2.version String Yes, unless a POM asset is included in the upload Version of the component maven2.generate-pom Boolean No Whether the Nexus Repository Manager should generate a POM file based on above component coordinates provided maven2.packaging String No Define component packaging (e.g. jar, ear) maven2.assetN File Yes, at least one Binary asset maven2.assetN.extension String Yes Extension of the corresponding assetN asset maven2.assetN.classifier String No Classifier of the corresponding assetN asset Examples：Uploading a jar and Automatically Creating a pom File $ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=true\" \\ -F \"maven2.groupId=com.example\" \\ -F \"maven2.artifactId=commercial-product\" \\ -F \"maven2.packaging=jar\" \\ -F \"version=1.0.0\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/product.jar;type=application/java-archive\" \\ -F \"maven2.asset1.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-third-party\" Upload a POM and associated JAR File $ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=false\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/pom.xml\" \\ -F \"maven2.asset1.extension=pom\" \\ -F \"maven2.asset2=@/absolute/path/to/the/local/file/product-1.0.0.jar;type=application/java-archive\" \\ -F \"maven2.asset2.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-releases\" ②Raw Raw supports multiple assets within a single component. Field name Field type Required? Description raw.directory String Yes Destination for upload files (e.g. /path/to/files) raw.assetN File Yes, at least one Binary asset raw.assetN.filename String Yes Filename to be used for the corresponding assetN asset ③PyPI Field name Field type Required? Description pypi.asset File Yes Binary asset ④RubyGems Field name Field type Required? Description rubygems.asset File Yes Binary asset ⑤NuGet Field name Field type Required? Description nuget.asset File Yes Binary asset ⑥NPM Field name Field type Required? Description npm.asset File Yes Binary asset 4、Delete Component：删除仓库中的component DELETE /service/rest/v1/components/{id} $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... 六、Metrics API 示例 https://raw.githubusercontent.com/OpenShiftDemos/nexus/master/scripts/nexus-functions ################################################################# # Functions for Managing Sonatype Nexus # # # # Authors: # # - Jorge Morales https://github.com/jorgemoralespou # # - Siamak Sadeghianfar https://github.com/siamaksade # # # ################################################################# ​ # # add_nexus2_repo [repo-id] [repo-url] [nexus-username] [nexus-password] [nexus-url] # ​ function add_nexus2_repo() { local _REPO_ID=$1 local _REPO_URL=$2 local _NEXUS_USER=$3 local _NEXUS_PWD=$4 local _NEXUS_URL=$5 ​ read -r -d '' _REPO_JSON Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html":{"url":"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html","title":"Jenkins相关插件","keywords":"","body":"使用jenkins插件上传CI流程制品到Nexus仓库 一、Overviews 现在Nexus各个格式仓库中的制品大多数都是在Jenkins的持续集成CI流水线中生成的，每次流水线构建都需要其制品上传到Nexus中进行管理。Nexus针对Jenkins有Nexus Platform的插件来简化上传步骤，该插件主要用来上传Maven格式制品到Hosted类型的仓库中。同时，Jenkins CI Pipeline中除了可以使用该插件来上传Maven制品到Maven格式仓库，原始Curl也是可以的。 插件Github：https://github.com/jenkinsci/nexus-platform-plugin 二、Jenkins使用Nexus Platform上传maven格式制品 1、安装 2、配置 系统管理--> 系统设置--> Sonatype Nexus 3、Jenkins Job 4、Jenkins Pipeline .....上文省略...... stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://Nexus-IP地址:8081/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } .....下文省略...... 5、Jenkins使用Curl命令手动上传maven制品到Nexus仓库中 stage(\"上传制品\"){ steps{ script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用curl命令通过Nexus API接口上传制品到RAW仓库。下载URL既是上传URL sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u admin:****** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://Nexus-IP地址:8081/repository/jenkins-product-repository/${pomfile.artifactId}-${pomfile.version}-${params.BUILD_VERSION}-${params.BUILD_ID}.${pomfile.packaging}\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/SonarQube静态代码扫描分析简介.html":{"url":"origin/SonarQube静态代码扫描分析简介.html","title":"SonarQube静态代码扫描分析","keywords":"","body":"一、静态代码分析 Why：在软件开发过程中，开发团队往往要花费大量的时间和精力发现并修改代码缺陷。而 发现BUG越晚，修复的成本越大 缺陷引入的大部分是在编码阶段，但发现更多的是在单元测试、集成测试、功能测试阶段 30% 至 70% 的代码逻辑设计和编码缺陷是可以通过静态代码分析来发现和修复的 How：在编码阶段，可以通过以下手段发现源代码问题，从源头及时规避，保证代码质量 静态代码扫描工具 Code Review What：Code Review往往要求大量的时间消耗和相关知识的积累，因此对于软件开发团队来说，使用静态代码分析工具自动化执行代码检查和分析，能够极大地提高软件可靠性并节省软件开发和测试成本。 帮助程序开发人员自动执行静态代码分析，快速定位代码隐藏错误和缺陷 助代码设计人员更专注于分析和解决代码设计缺陷 显著减少在代码逐行检查上花费的时间，提高软件可靠性并节省软件开发和测试成本 常见的一些静态分析工具 Checkstyle：SourceForge 的开源项目，通过检查对代码编码格式，命名约定，Javadoc，类设计等方面进行代码规范和风格的检查，从而有效约束开发人员更好地遵循代码编写规范 FindBugs：由马里兰大学提供的一款开源 Java 静态代码分析工具。FindBugs 通过检查类文件或 JAR 文件，将字节码与一组缺陷模式进行对比从而发现代码缺陷，完成静态代码分析 PMD：由 DARPA 在 SourceForge 上发布的开源 Java 代码静态分析工具。PMD 通过其内置的编码规则对 Java 代码进行静态检查，主要包括对潜在的 bug，未使用的代码，重复的代码，循环体创建新对象等问题的检验 二、Sonar简介 Sonar是一个用于代码质量管理的开源平台，可以从 七个维度检测代码质量 Sonar可以通过PMD、CheckStyle、Findbugs等代码规则检测工具来检测你的代码，帮助你发现代码的漏洞，Bug，异味等信息。 Sonar最大的特点就是插件化，可以根据不同的场景需求进行插件化安装，可以同时可以检测Python、C++等多种语言。 Sonar客户端可以采用IDE插件、Sonar-Scanner插件、Ant插件和Maven插件等多种方式，并通过各种不同的分析机制对项目源代码进行分析和扫描，并把分析扫描后的结果上传到sonar的数据库，通过sonar web界面对分析结果进行管理 Sonar的架构体系 Project：是需要被分析的源码 SonarQube Scanner：用于执行代码分析的工具，SonarQube Scanner分析完毕之后，会将结果上报到指定的SonarQube Server。 SonarQube Server：显示分析结果的Web Server，在SonarQube Scanner第一次将一个工程的分析结果上报给SonarQube Server后，Server上会自动创建一个工程显示分析的结果，可以在Server上设置代码质量管理相关的各种配置，如设置代码检查规则（Rule）和质量门限（Quality Gate）等。SonarQube Server包含三个子进程（web服务（界面管理）、搜索服务、计算引擎服务（写入数据库）） SonarQube Database：保存SonarQube服务端的权限配置，插件配置，项目快照，项目视图等 三、自动化扫描分析源代码的流程 官方推荐的自动化扫描流程 自动化静态代码扫描流程 本地开发：JetBrains Intellij IDEA 、Eclipse安装阿里巴巴的代码检查规范插件，可在编写代码时提示规范信息；安装使用sonarlint插件在本地运行代码扫描 Gitlab：Gitlab代码仓库可设置事件监听器，例如PUSH事件、Merge Request事件等。发送Web-hook到外部系统 Jenkins：Jenkins中可安装Gitlab插件，用于设置特定的Web-hook后端监听器来触发当前任务。 Jenkins Pipeline：在Jenkins Pipeline中获取Web-hook信息来拉取代码，然后编译、执行Sonar Scanner扫描源代码文件或二进制文件，最后将扫描的结果发送SnarQube进行存储、展示、管理等操作 SonarQube： 四、SonarQube服务端配置 1. 配置代码规则插件 2. 配置全局参数 3. 管理扫描结果 4. 质量门禁 五、Sonar体系中的配置参数生效优先级 UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数 例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数 例如sonar scanner Maven插件在settings.xml中配置的参数 项目分析客户端命令行运行时配置的参数，例如sonar-scanner二进制命令行运行时以“-D”开头的配置参数 六、扫描器SonarScanner 当SonarQube服务端搭建配置好了，Sonar提供了各种插件形式的Sonar Scanner扫描器供你选择来扫描你的源代码。 SonarScanner：下载二进制客户端进行扫描 SonarScanner for Maven：以Maven插件的形式扫描代码 SonarScanner for Jenkins：以Jenkins插件的形式配置扫描代码 SonarScanner for Gradle：以Gradle插件的形式配置扫描代码 SonarScanner for Ant：以Ant插件的形式配置扫描代码 SonarScanner项目扫描参数 官方文档说明 参数 描述 默认值 是否必要 sonar.host.url SonarQube服务端地址 http://localhost:9000 是 sonar.projectKey 项目的唯一标识。以字母,-,_,:,至少有一个非数字 对于Maven插件的话,默认值是 :其他形式插件不提供默认值 是 sonar.projectName 在SonarQube Web UI上面显示的项目名 对于Maven插件形式,默认值是其他形式插件不提供默认值 否 sonar.projectVersion 项目的扫描版本 对于Maven插件形式,默认值是其他形式插件不提供默认值 否 sonar.login 发送扫描结果到SonarQube时的认证方式之一。值类型可为用户生成的认证Token，用户名 是 sonar.password 当sonar.login值类型为认证Token时，则不填 是 sonar.ws.timeout 等待服务端响应的最大秒数 60 否 sonar.projectDescription 项目描述。用于在项目Web UI中显示项目的描述 对于Maven插件形式,默认值是 否 sonar.links.homepage 项目地址。用于在项目Web UI中显示项目访问链接 对于Maven插件形式,默认值是 否 sonar.links.issue 项目代码Issue管理地址。用于在项目Web UI中显示Issue管理链接 对于Maven插件形式,默认值是 否 sonar.links.scm 项目源代码仓库地址。用于在项目Web UI中显示源代码仓库链接 for Maven projects 否 sonar.sources 以逗号分割的main源代码文件夹路径 否 sonar.tests 以逗号分割的测试源代码文件夹路径 否 sonar.sourceEncoding 源代码文件的编码格式，例如：UTF-8, MacRoman, Shift_JIS 系统的编码格式 否 sonar.externalIssuesReportPaths 否 sonar.projectDate 格式： yyyy-MM-dd, 例如: 2010-12-01 Current date 否 sonar.projectBaseDir 针对多模块项目时，指定要扫描源代码的目录路径 否 sonar.working.directory 指定Sonarscanner的工作空间。必须是不存在的，相对路径。针对MSBuild的插件，此参数不兼容 ~/.scannerwork 否 sonar.scm.provider 否 sonar.scm.forceReloadAll 否 sonar.scm.exclusions.disabled 否 sonar.scm.revision 否 sonar.buildString 100 否 sonar.analysis.[yourKey] 10 否 sonar.log.level 控制Sonarscanner输出日志的级别 INFO 否 sonar.verbose 输出更多Sonarscanner客户端和Sonarqube服务的扫描信息 false 否 sonar.showProfiling 否 sonar.scanner.dumpToFile 输出扫描期间所有的配置参数到文件中 否 sonar.scanner.metadataFilePath 指定report-task.txt文件的生成路径 等于sonar.working.directory的值 否 1. SonarScanner 下载地址：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/ 配置文件 全局配置文件路径：$安装目录/conf/sonar-scanner.properties 项目配置文件路径：$项目根目录/sonar-project.properties CLI命令参数 $ sonar-scanner usage: sonar-scanner [options] 参数: -D,--define Define property -h,--help Display help information -v,--version Display version information -X,--debug Produce execution debug output If you need more debug information you can add one of the following to your command line: -X, --verbose, or -Dsonar.verbose=true. 2. SonarScanner for Maven 官方文档：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-maven/ 注意： 从maven-sonar-plugin 3.4.0.905开始，不再支持SonarQube 从maven-sonar-plugin 3.1开始，不再支持Maven 全局参数 详见SonarQube服务端配置 配置Maven的setting.xml org.sonarsource.scanner.maven sonar true http://myserver:9000 f6eedc3d8bfa850a15f2ffcd 在项目pom.xml中配置项目扫描参数 ....上文省略.... com.curiosuer springboot2 0.0.1 用于演示Spring Boot2的一些功能 Curiouser-Demo-SpringBoot2-${gitlabBranch} http://springboot2-demo.apps.okd311.curiouser.com/swagger-ui.html ${gitlabSourceRepoHomepage}/commit/${gitlabMergeRequestLastCommit} ${gitlabSourceRepoHomepage}/issues ${BUILD_URL} UTF-8 UTF-8 1.8 指定的值 --> ${gitlabMergeRequestLastCommit} src/main/ 1 ${gitlabMergeRequestLastCommit} ${gitlabMergeRequestLastCommit} reuseReports jacoco ....下文省略.... 注意：pom.xml中有些参数的值是可以在Jenkins CI流水线中通过环境变量获取的。 执行扫描命令 mvn test sonar:sonar -Dspring.profiles.active=local 默认参数 sonar.projectKey ==> POM中的: sonar.projectName ==> POM中的 sonar.projectVersion ==> POM中的 sonar.projectDescription ==> POM中的 sonar.links.homepage ==> POM中的 sonar.links.ci ==> POM中的 sonar.links.issue ==> POM中的 sonar.links.scm ==> POM中的 七、扫描结果解析 附录 1、Sonar检查代码质量的七个维度 复杂度分布（complexity）：代码复杂度过高将难以理解、难以维护 重复代码（duplications）：程序中包含大量复制粘度的代码是质量低下的表现 单元测试（unit tests）：统计并展示单元测试覆盖率 编码规范（coding rules）：通过Findbugs、PMD、CheckStyle等规范代码编写 注释（commments）：少了可读性差，多了看起来费劲 潜在的Bug（potential bugs）：通过Findbugs、PMD、CheckStyle等检测潜在bug 结构与设计（architecture & design）：依赖i、耦合等 2、常见检查分析工具的内置规范 Checkstyle：分析源代码文件 Javadoc 注释：检查类及方法的 Javadoc 注释 命名约定：检查命名是否符合命名规范 标题：检查文件是否以某些行开头 Import 语句：检查 Import 语句是否符合定义规范 代码块大小，即检查类、方法等代码块的行数 空白：检查空白符，如 tab，回车符等 修饰符：修饰符号的检查，如修饰符的定义顺序 块：检查是否有空块或无效块 代码问题：检查重复代码，条件判断，魔数等问题 类设计：检查类的定义是否符合规范，如构造函数的定义等问题 FindBugs：分析字节码文件 Bad practice 坏的实践：常见代码错误，用于静态代码检查时进行缺陷模式匹配 Correctness 可能导致错误的代码，如空指针引用等 国际化相关问题：如错误的字符串转换 可能受到的恶意攻击，如访问权限修饰符的定义等 多线程的正确性：如多线程编程时常见的同步，线程调度问题。 运行时性能问题：如由变量定义，方法调用导致的代码低效问题。 PMD：：分析源代码文件 可能的 Bugs：检查潜在代码错误，如空 try/catch/finally/switch 语句 未使用代码（Dead code）：检查未使用的变量，参数，方法 复杂的表达式：检查不必要的 if 语句，可被 while 替代的 for 循环 重复的代码：检查重复的代码 循环体创建新对象：检查在循环体内实例化新对象 资源关闭：检查 Connect，Result，Statement 等资源使用之后是否被关闭掉 Jtest 可能的错误：如内存破坏、内存泄露、指针错误、库错误、逻辑错误和算法错误等 未使用代码：检查未使用的变量，参数，方法 初始化错误：内存分配错误、变量初始化错误、变量定义冲突 命名约定：检查命名是否符合命名规范 Javadoc 注释：检查类及方法的 Javadoc 注释 线程和同步：检验多线程编程时常见的同步，线程调度问题 国际化问题： 垃圾回收：检查变量及 JDBC 资源是否存在内存泄露隐患 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-29 14:26:47 "},"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html":{"url":"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html","title":"SonarScanner-将扫描结果以comment的形式回写到gitlab","keywords":"","body":"SonarScanner使用Sonarqube的Gitlab插件将扫描结果以gitlab comment的形式回写到Gitlab 一、Context 在Jenkins中做CI过程中,有一个步骤是代码编译完,使用sonar scanner扫描代码,检查静态代码中的语法错误等,然后将扫描结果发送到sonarqube,供项目经理查看代码质量. sonarqube可以安装插件gitlab,让sonarscanner扫描完代码,将结果以gitlab注释的方式回写到提交的commit中.方便开发人员排查代码. 以下操作过程各组件的版本 sonarqube: 7.3 (build 15553) sonarscanner: 3.3.0.1492 sonarqube gitlab插件: 4.0.0 gitlab: 10.8.4 ce jenkins: 2.150.2 Jenkins CI流水线是在使用Jenkins Slave(Kubernetes插件动态生成Slave POD)节点中来运行的,所以Sonarscanner,Maven等工具都是在Kubernetes Jenkins Slave镜像中已经安装好的. 二、操作 1、安装sonar-gitlab-plugin插件 插件Github:https://github.com/gabrie-allaigre/sonar-gitlab-plugin/ 2、生成用户访问Token 3、gitlab创建sonarscanner的用户,并生成AccessKey 4、在gitlab中将sonarqube加入到对应项目仓库的Members中 5、Sonarqube中编辑gitlab插件的全局配置 扫描项目时，扫描参数生效优先级如下： UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数（例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数） 项目分析客户端命令行中配置的参数 所以可以在UI界面全局配置中配置一些通用、不经常变动的、由管理员控制的参数。例如：gitlab插件的通用配置、gitlab地址等参数 6、Jenkins Pipeline中使用sonarscanner扫描代码 stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=6a6fa6f1702ae42f8d0a0fe14166d9a2 \\ -Dsonar.projectName=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectKey=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectVersion=$GIT_COMMIT \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.sources=src/main \\ -Dsonar.test=src/test \\ -Dsonar.java.binaries=target/classes \\ -Dsonar.java.test.binaries='target/test-classes/*/*.class' \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.java.coveragePlugin=jacoco \\ -Dsonar.dynamicAnalysis=reuseReports \" } } 三、效果 四、Soanarscanner Gitlab插件参数详解 Variable Comment Type Version sonar.gitlab.url GitLab url Administration, Variable >= 1.6.6 sonar.gitlab.max_global_issues Maximum number of anomalies to be displayed in the global comment Administration, Variable >= 1.6.6 sonar.gitlab.user_token Token of the user who can make reports on the project, either global or per project Administration, Project, Variable >= 1.6.6 sonar.gitlab.project_id Project ID in GitLab or internal id or namespace + name or namespace + path or url http or ssh url or url or web Project, Variable >= 1.6.6 sonar.gitlab.commit_sha SHA of the commit comment Variable >= 1.6.6 sonar.gitlab.ref Branch name or reference of the commit Variable sonar.gitlab.ref_name Branch name or reference of the commit Variable >= 1.6.6 sonar.gitlab.max_blocker_issues_gate Max blocker issue for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_critical_issues_gate Max critical issues for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_major_issues_gate Max major issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_minor_issues_gate Max minor issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_info_issues_gate Max info issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.ignore_certificate Ignore Certificate for access GitLab, use for auto-signing cert (default false) Administration, Variable >= 2.0.0 sonar.gitlab.comment_no_issue Add a comment even when there is no new issue (default false) Administration, Variable >= 2.0.0 sonar.gitlab.disable_inline_comments Disable issue reporting as inline comments (default false) Administration, Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_file Show issue for commit file only (default false) Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_line Show issue for commit line only (default false) Variable >= 2.1.0 sonar.gitlab.build_init_state State that should be the first when build commit status update is called (default pending) Administration, Variable >= 2.0.0 sonar.gitlab.disable_global_comment Disable global comment, report only inline (default false) Administration, Variable >= 2.0.0 sonar.gitlab.failure_notification_mode Notification is in current build (exit-code) or in commit status (commit-status) (default commit-status) Administration, Variable >= 2.0.0 sonar.gitlab.global_template Template for global comment in commit Administration, Variable >= 2.0.0 sonar.gitlab.ping_user Ping the user who made an issue by @ mentioning. Only for default comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.unique_issue_per_inline Unique issue per inline comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.prefix_directory Add prefix when create link for GitLab Variable >= 2.1.0 sonar.gitlab.api_version GitLab API version (default v4 or v3) Administration, Variable >= 2.1.0 sonar.gitlab.all_issues All issues new and old (default false, only new) Administration, Variable >= 2.1.0 sonar.gitlab.json_mode Create a json report in root for GitLab EE (codeclimate.json or gl-sast-report.json) Project, Variable >= 3.0.0 sonar.gitlab.query_max_retry Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.query_wait Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.quality_gate_fail_mode Quality gate fail mode: error, warn or none (default error) Administration, Variable >= 3.0.0 sonar.gitlab.issue_filter Filter on issue, if MAJOR then show only MAJOR, CRITICAL and BLOCKER (default INFO) Administration, Variable >= 3.0.0 sonar.gitlab.load_rules Load rules for all issues (default false) Administration, Variable >= 3.0.0 sonar.gitlab.disable_proxy Disable proxy if system contains proxy config (default false) Administration, Variable >= 4.0.0 sonar.gitlab.merge_request_discussion Allows to post the comments as discussions (default false) Project, Variable >= 4.0.0 sonar.gitlab.ci_merge_request_iid The IID of the merge request if it’s pipelines for merge requests Project, Variable >= 4.0.0 五、问题 1、当项目是私有仓库时 2、获取项目仓库的ProjectID 3、gitlab插件4.0.0无法兼容Sonarqube 7.6-community至7.9-community的版本 报错如下！插件GIthub的原始Issue：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/issues/213 [ERROR] Failed to execute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed toexecute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 原因 解决方案 已经修改编译好的插件Jar包：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/releases/download/4.1.0-SNAPSHOT/sonar-gitlab-plugin-4.1.0-SNAPSHOT.jar 参考链接 https://gitlab.com/gitlab-org/gitlab-ce/issues/28342 https://www.cnblogs.com/amyzhu/p/8988519.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-18 13:44:11 "},"origin/ldap-Jenkins对接LDAP.html":{"url":"origin/ldap-Jenkins对接LDAP.html","title":"Jenkins","keywords":"","body":"一. Context OpenLDAP的条目组织形式 二. Jenkins配置 1. Jenkins安装LDAP插件 安装插件有两种方法： 方法一：后台插件管理里直接安装 优点：简单方便，不需要考虑插件依赖问题 缺点：因为网络等各种问题安装不成功 安装方法：登录Jenkins --> 系统管理 --> 插件管理 --> 可选插件 --> 搜索LDAP --> 选中 --> 直接安装 --> 安装完成重启 方法二：官网下载安装文件后台上传 优点：一定可以安装成功的 缺点：麻烦，要去官网找插件并解决依赖 安装方法：官网下载插件 --> 登录Jenkins --> 系统管理 --> 插件管理 --> 高级 --> 上传插件 --> 选择文件 --> 上传 --> 安装完成后重启 LDAP插件下载地址：https://updates.jenkins.io/download/plugins/ldap/ 2. 登录Jenkins --> 系统管理 --> 全局安全配置 root DN：这里的root DN只是指搜索的根，并非LDAP服务器的root dn。由于LDAP数据库的数据组织结构类似一颗大树，而搜索是递归执行的，理论上，我们如果从子节点（而不是根节点）开始搜索，因为缩小了搜索范围那么就可以获得更高的性能。这里的root DN指的就是这个子节点的DN，当然也可以不填，表示从LDAP的根节点开始搜索 User search base：这个配置也是为了缩小LDAP搜索的范围，例如Jenkins系统只允许ou为Admin下的用户才能登陆，那么你这里可以填写ou=Admin，这是一个相对的值，相对于上边的root DN，例如你上边的root DN填写的是dc=domain,dc=com，那么user search base这里填写了ou=Admin，那么登陆用户去LDAP搜索时就只会搜索ou=Admin,dc=domain,dc=com下的用户了 User search filter：这个配置定义登陆的“用户名”对应LDAP中的哪个字段，如果你想用LDAP中的uid作为用户名来登录，那么这里可以配置为uid={0}（{0}会自动的替换为用户提交的用户名），如果你想用LDAP中的mail作为用户名来登录，那么这里就需要改为mail={0}。在测试的时候如果提示你user xxx does not exist，而你确定密码输入正确时，就要考虑下输入的用户名是不是这里定义的这个值了 Group search base：参考上边User search base解释 Group search filter：这个配置允许你将过滤器限制为所需的objectClass来提高搜索性能，也就是说可以只搜索用户属性中包含某个objectClass的用户，这就要求你对你的LDAP足够了解，一般我们也不配置 Group membership：没配置，没有详细研究 Manager DN：这个配置在你的LDAP服务器不允许匿名访问的情况下用来做认证（详细的认证过程参考文章LDAP落地实战（二）：SVN集成OpenLDAP认证中关于LDAP服务器认证过程的讲解），通常DN为cn=admin,dc=domain,dc=com这样 Manager Password：上边配置dn的密码 Display Name LDAP attribute：配置用户的显示名称，一般为显示名称就配置为uid，如果你想显示其他字段属性也可以这里配置，例如mail Email Address LDAP attribute：配置用户Email对应的字段属性，一般没有修改过的话都是mail，除非你用其他的字段属性来标识用户邮箱，这里可以配置 3. 登录验证 参考链接 https://mp.weixin.qq.com/s/S5ozDJSh4yTSfP_glNoiOQ https://plugins.jenkins.io/ldap https://wiki.jenkins.io/display/JENKINS/LDAP+Plugin#LDAPPlugin-Groupmembership https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_liunx_52_ldap_for_jenkins.html https://blog.csdn.net/wanglei_storage/article/details/52935312 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/ldap-SonarQube对接LDAP.html":{"url":"origin/ldap-SonarQube对接LDAP.html","title":"SonarQube","keywords":"","body":"一、Context OpenLDAP的条目组织形式 Sonaeqube官方文档的操作步骤 二、操作 1、Sonarqube安装LDAP插件 配置--> 应用市场 2、修改配置文件/opt/sonarqube/conf/sonar.properties 如果sonarqube的部署实例是使用Dockers的话，则可通过环境变量的方式注入以下配置 sonar.security.realm=LDAP sonar.forceAuthentication=true ldap.authentication=simple ldap.url=ldap://openldap-service.openldap.svc:389 ldap.bindDn=cn=admin,dc=curiouser,dc=com ldap.bindPassword=****** # User Configuration ldap.user.baseDn=ou=employee,dc=curiouser,dc=com ldap.user.request=(&(memberOf=cn=sonarqube,ou=applications,dc=curiouser,dc=com)(cn={0})) ldap.user.realNameAttribute=sn ldap.user.emailAttribute=mail 相关配置 Property Description Default value Required Example sonar.security.realm Set this to LDAP authenticate first against the external sytem. If the external system is not reachable or if the user is not defined in the external system, authentication will be performed against SonarQube's internal database. none Yes LDAP (only possible value) sonar.authenticator.downcase Set to true when connecting to a LDAP server using a case-insensitive setup. false No ldap.url URL of the LDAP server. If you are using ldaps, you should install the server certificate into the Java truststore. none Yes ldap://localhost:10389 ldap.bindDn The username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory. none No cn=sonar,ou=users,o=mycompany ldap.bindPassword The password of the user to connect with. Leave this blank for anonymous access to the LDAP directory. none No secret ldap.authentication Possible values: simple, CRAM-MD5, DIGEST-MD5, GSSAPI. See the tutorial on authentication mechanisms simple No ldap.realm See Digest-MD5 Authentication, CRAM-MD5 Authentication none No example.org ldap.contextFactoryClass Context factory class. com.sun.jndi.ldap.LdapCtxFactory No ldap.StartTLS Enable use of StartTLS false No ldap.followReferrals Follow referrals or not. See Referrals in the JNDI true 用户配置 Property Description Default value Required Example ldap.user.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for users. None Yes cn=users,dc=example,dc=org ldap.user.request LDAP user request. (&(objectClass=inetOrgPerson)(uid={login})) No (&(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute Attribute in LDAP defining the user’s real name. cn No ldap.user.emailAttribute Attribute in LDAP defining the user’s email. mail No Group Mapping Only groups are supported (not roles). Only static groups are supported (not dynamic groups). For the delegation of authorization, groups must be first defined in SonarQube. Then, the following properties must be defined to allow SonarQube to automatically synchronize the relationships between users and groups. Property Description Default value Required Example for Active Directory ldap.group.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for groups. none No cn=groups,dc=example,dc=org ldap.group.request LDAP group request. (&(objectClass=groupOfUniqueNames)(uniqueMember={dn})) No (&(objectClass=group)(member={dn})) ldap.group.idAttribute Property used to specifiy the attribute to be used for returning the list of user groups in the compatibility mode. cn No sAMAccountName 重启Sonarqube，启动过程中如果出现以下日志，则证明LDAP连接成功 INFO org.sonar.INFO Security realm: LDAP ... INFO o.s.p.l.LdapContextFactory Test LDAP connection: OK 3、登录验证 4、权限控制 将admin用户的管理员权限删除，赋予另一个用户 参考链接 https://hub.docker.com/_/sonarqube?tab=description https://docs.sonarqube.org/latest/instance-administration/delegated-auth/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/ldap-Gitlab对接LDAP.html":{"url":"origin/ldap-Gitlab对接LDAP.html","title":"Gitlab","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、配置 1. 修改/etc/gitlab/gitlab.rb ..................省略............................. gitlab_rails['ldap_enabled'] = true ###! **remember to close this block with 'EOS' below** gitlab_rails['ldap_servers'] = YAML.load 三、测试登录 四、注意 当用户第一次使用LDAP登录GitLab时，如果其LDAP电子邮件地址是现有GitLab用户的电子邮件地址时，那么LDAP DN用户将与现有gitlab用户相关联。如果在GitLab的数据库中没有找到LDAP电子邮件属性，就会创建一个新用户。 换句话说，如果现有的GitLab用户希望自己启用LDAP登录，那么他们应该检查他们的GitLab电子邮件地址是否匹配LDAP电子邮件地址，然后通过他们的LDAP凭证登录GitLab。 https://docs.gitlab.com/ee/administration/auth/ldap.html#enabling-ldap-sign-in-for-existing-gitlab-users 参考链接 https://blog.csdn.net/tongdao/article/details/52538365 https://docs.gitlab.com/ee/administration/auth/ldap.html#configuration https://docs.gitlab.com/ee/administration/auth/how_to_configure_ldap_gitlab_ce/index.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/ldap-Nexus对接LDAP.html":{"url":"origin/ldap-Nexus对接LDAP.html","title":"Nexus","keywords":"","body":"Preflight Nexus 3 OpenLDAP 3.15.2-01 1.2.4 一、Context OpenLDAP的条目组织形式 二、Nexus设置 1. Nexus开启认证Realm 2. 配置LDAP Name：Enter a unique name for the new configuration. LDAP server address：Enter Protocol, Hostname, and Port of your LDAP server. Protocol：Valid values in this drop-down are ldap and ldaps that correspond to the Lightweight Directory Access Protocol and the Lightweight Directory Access Protocol over SSL. Hostname：The hostname or IP address of the LDAP server. Port：The port on which the LDAP server is listening. Port 389 is the default port for the ldap protocol, and port 636 is the default port for the ldaps. Search base：The search base further qualifies the connection to the LDAP server. The search base usually corresponds to the domain name of an organization. For example, the search base could be dc=example,dc=com. Note: If the values in your search base contain spaces, escape them with \"%20\", as in \"dc=example%20corp,dc=com\" You can configure one of four authentication methods to be used when connecting to the LDAP Server with the Authentication method drop-down. Simple Authentication：Simple authentication consists of a Username and Password. Simple authentication is not recommended for production deployments not using the secure ldaps protocol as it sends a clear-text password over the network. Anonymous Authentication：The anonymous authentication uses the server address and search base without further authentication. Digest-MD5：This is an improvement on the CRAM-MD5 authentication method. For more information, see RFC-2831. CRAM-MD5：The Challenge-Response Authentication Method (CRAM) is based on the HMAC-MD5 MAC algorithm. In this authentication method, the server sends a challenge string to the client. The client responds with a username followed by a Hex digest that the server compares to an expected value. For more information, see RFC-2195.For a full discussion of LDAP authentication approaches, see RFC-2829 and RFC-2251. SASL Realm：The Simple Authentication and Security Layer (SASL) realm used to connect to the LDAP server. It is only available if the authentication method is Digest-MD5 or CRAM-MD5. Username or DN：Username or DN (Distinguished Name) of an LDAP user with read access to all necessary users and groups. It is used to connect to the LDAP server. Password：Password for the Username or DN configured above. Base DN：Corresponds to the collection of distinguished names used as the base for user entries. This DN is relative to the Search Base. For example, if your users are all contained in ou=users,dc=sonatype,dc=com and you specified a Search Base of dc=sonatype,dc=com, you use a value of ou=users. User subtree：Check the box if True. Uncheck if False. Values are true if there is a tree below the Base DN that can contain user entries and false if all users are contain within the specified Base DN. For example, if all users are in ou=users,dc=sonatype,dc=com this field should be False. If users can appear in organizational units within organizational units such as ou=development,ou=users,dc=sonatype,dc=com, this field should be True . Object class：This value is a standard object class defined in RFC-2798. It specifies the object class for users. Common values are inetOrgPerson, person, user, or posixAccount. User filter：This allows you to configure a filter to limit the search for user records. It can be used as a performance improvement. User ID attribute：This is the attribute of the object class specified above, that supplies the identifier for the user from the LDAP server. The repository manager uses this attribute as the User ID value. Real name attribute：This is the attribute of the Object class that supplies the real name of the user. The repository manager uses this attribute when it needs to display the real name of a user similar to usage of the internal First name and Last name attributes. Email attribute：This is the attribute of the Object class that supplies the email address of the user. The repository manager uses this attribute for the Email attribute of the user. It is used for email notifications of the user. Password attribute：It can be used to configure the Object class, which supplies the password (\"userPassword\"). If this field is blank the user will be authenticated against a bind with the LDAP server. The password attribute is optional. When not configured authentication will occur as a bind to the LDAP server. Otherwise this is the attribute of the Object class that supplies the password of the user. The repository manager uses this attribute when it is authenticating a user against an LDAP server. Group Base DN：This field is similar to the Base DN field described for User Element Mapping, but applies to groups instead of users. For example, if your groups were defined under ou=groups,dc=sonatype,dc=com, this field would have a value of ou=groups. Group subtree：This field is similar to the User subtree field described for User Element Mapping, but configures groups instead of users. If all groups are defined under the entry defined in Base DN, set the field to false. If a group can be defined in a tree of organizational units under the Base DN, set the field to true. Group object class：This value in this field is a standard object class defined in RFC-2307. The class is simply a collection of references to unique entries in an LDAP directory and can be used to associate user entries with a group. Examples are groupOfUniqueNames, posixGroup or custom values. Group ID attribute：Specifies the attribute of the object class that specifies the group identifier. If the value of this field corresponds to the ID of a role, members of this group will have the corresponding privileges. Group member attribute：Specifies the attribute of the object class which specifies a member of a group. An example value is uniqueMember. Group member format：This field captures the format of the Group Member Attribute, and is used by the repository manager to extract a username from this attribute. An example values is ${dn} . 3. 分配Nexus管理员的角色\"nx-admin\"给LDAP上的一个用户，作为nexus新的管理员。然后将admin用户禁用。 参考链接 https://help.sonatype.com/repomanager3/security/ldap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/ldap-Grafana对接LDAP.html":{"url":"origin/ldap-Grafana对接LDAP.html","title":"Grafana","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、操作 1、修改/etc/grafana/grafana.ini .............省略............. [auth.ldap] enabled = true config_file = /etc/grafana/ldap.toml allow_sign_up = true .............省略............. 2、修改/etc/grafana/ldap.toml .............省略............. # To troubleshoot and get more log info enable ldap debug logging in grafana.ini # [log] # filters = ldap:debug [[servers]] # Ldap server host (specify multiple hosts space separated) host = \"openldap-service.openldap.svc\" # Default port is 389 or 636 if use_ssl = true port = 389 # Set to true if ldap server supports TLS use_ssl = false # Set to true if connect ldap server with STARTTLS pattern (create connection in insecure, then upgrade to secure connection with TLS) start_tls = false # set to true if you want to skip ssl cert validation ssl_skip_verify = false # set to the path to your root CA certificate or leave unset to use system defaults # root_ca_cert = \"/path/to/certificate.crt\" # Authentication against LDAP servers requiring client certificates # client_cert = \"/path/to/client.crt\" # client_key = \"/path/to/client.key\" # Search user bind dn bind_dn = \"cn=admin,dc=curiouser,dc=com\" # Search user bind password，If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" bind_password = '*********' # User search filter, for example \"(cn=%s)\" or \"(sAMAccountName=%s)\" or \"(uid=%s)\" search_filter = \"(&(memberOf=cn=grafana,ou=applications,dc=curiouser,dc=com))\" # An array of base dns to search through search_base_dns = [\"ou=employee,dc=curiouser,dc=com\"] ## For Posix or LDAP setups that does not support member_of attribute you can define the below settings。Please check grafana LDAP docs for examples # group_search_filter = \"(&(objectClass=posixGroup)(memberUid=%s))\" # group_search_base_dns = [\"ou=groups,dc=grafana,dc=org\"] # group_search_filter_user_attribute = \"uid\" # Specify names of the ldap attributes your ldap uses [servers.attributes] name = \"sn\" username = \"cn\" member_of = \"memberOf\" email = \"mail\" # Map ldap groups to grafana org roles [[servers.group_mappings]] #group_dn = \"cn=admins,dc=grafana,dc=org\" #org_role = \"Admin\" # To make user an instance admin (Grafana Admin) uncomment line below # grafana_admin = true # The Grafana organization database id, optional, if left out the default org (id 1) will be used # org_id = 1 [[servers.group_mappings]] group_dn = \"cn=users,dc=grafana,dc=org\" org_role = \"Editor\" [[servers.group_mappings]] # If you want to match all (or no ldap groups) then you can use wildcard group_dn = \"*\" #org_role = \"Viewer\" .............省略............. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/logging-日志系统技术概览简介.html":{"url":"origin/logging-日志系统技术概览简介.html","title":"日志系统技术概览简介","keywords":"","body":"日志系统概览简介 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-20 09:25:01 "},"origin/logging-日志系统数据在个组件中的流转格式.html":{"url":"origin/logging-日志系统数据在个组件中的流转格式.html","title":"日志系统数据在个组件中的流转格式","keywords":"","body":"原始日志 2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\"business\":\"curiouser\",\"currentTime\":\"2019-09-24 09:12:39.052\",\"data\":\"{\"args\": {\"AuthQueryDTO\": {\"clientId\":\"ppush-platform\",\"clientSecret\":\"Jygv8V4TerC5rDxO\"},},\"result\": {\"expireTime\":-1,\"token\":\"77ff1cd2d1985b6d2d99bd54453bbc5f\",\"type\":\"1\"}}\",\"datatype\":0,\"interface1\":\"com.curiouser.auth.center.controller.ClientApiController\",\"level\":\"INFO\",\"method\":\"serverAuth\",\"module\":\"curiouser-auth-center\",\"reqTime\":8,\"requestId\":\"req-bf1bcc406dfa4d35b9062e06fbad78cd\",\"thread\":\"XNIO-1 task-14\",\"urlPath\":\"/client/server/token\"} This is a test log ! hahaha {\"datatype\":0,\"business\":\"alert\",\"module\":\"alert-rule\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"level\":\"WARN \",\"method\":\"isConnectionAlive\",\"thread\":\"XNIO-1 task-20\",\"requestId\":\"req-498fe711243b444e9b73ed6d5dc20a20\",\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\"} 经过\"原始日志+Filebeat\"处理过的日志 {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"fields\":{\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\",\"CLUSTER\":\"cluster_dev\"}} {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"This is a test log ! hahaha\",\"fields\":{\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"req2fe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer\"流程处理过的日志（内容没变，字段顺序变了，消息顺序变了） {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"This is a test log ! hahaha\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.336Z\",\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cenoller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"r8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.332Z\",\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka \"流程处理过的日志(内容没变，字段顺序变了，消息顺序变了) {\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cencuriouseroller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-cencuriousereqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":0},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"message\":\"This is a test log ! hahaha\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":645},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer\"流程处理过的日志(内容没变，字段顺序变了，消息顺序变了) {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"This is a test log ! hahaha\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer--->elasticsearch\"流程处理过的日志（将logstash-consumer发送的日志数据放在”_source“字段下，同时） # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"s_ZaY20BY8hT6jLicmK4\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 675 }, \"@version\": \"1\", \"data\": { \"business\": \"alert\", \"interface\": \"com.zaxxer.hikari.pool.PoolBase\", \"method\": \"isConnectionAlive\", \"requestId\": \"reqfe711243b444e9b73ed6d5dc20a20\", \"data\": \"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\", \"datatype\": 0, \"thread\": \"XNIO-1 task-20\", \"level\": \"WARN \", \"module\": \"alert-rule\", \"currentTime\": \"2019-09-24 20:50:00,056\" }, \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"1fUgY20BY8hT6jLiJAfF\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"This is a test log ! hahaha\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"@timestamp\": \"2019-09-24T11:34:29.741Z\", \"fields\": { \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"CLUSTER\": \"cluster_dev\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"PROJECTNAME\": \"test\" }, \"@version\": \"1\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 645 } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T11:34:29.741Z\" ] }, \"sort\": [ 1569324869741 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"DOlaY20B_ehr23pid9GM\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf106dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 0 }, \"@version\": \"1\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"ENV\": \"dev\", \"TEMPLATE\": 2019082110, \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } 附录 1、filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /root/logs/test.log exclude_files: [\"/root/logs/_filebeat\", \".gz$\"] recursive_glob.enabled: true setup.template.settings: index.number_of_shards: 3 processors: - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"data\" overwrite_keys: false - drop_fields: fields: [\"agent\", \"tags\", \"input\", \"ecs\"] fields: NAMESPACE: \"test\" PROJECTNAME: \"test\" CLUSTER: cluster_dev ENV: dev CANARY: sit0 TEMPLATE: 2019082110 output.logstash: hosts: [\"localhost:5044\"] #output.file: # path: \"/root/logs/output\" # filename: filebeat.log 2、logstash_producer配置 input { beats { id => \"logstash_producer_input_beats\" port => 5044 } } output { #file{ # path => \"/root/logs/output/logstah-producer.log\" #} kafka { id => \"logstash_producer_output_kafka\" codec => json topic_id => \"logs\" bootstrap_servers => \"localhost:9092\" compression_type => \"snappy\" } } 3、logstash_consumer配置 input { kafka { id => \"logstash_consumer_input_kafka\" bootstrap_servers => \"localhost:9092\" topics => \"logs\" group_id => \"applications_logs_group\" codec => \"json\" auto_offset_reset => \"earliest\" } } output { #file{ # path => \"/root/logs/output/logstah-consumer.log\" #} elasticsearch { id => \"logstash_consumer_output_elasticsearch\" hosts => [\"localhost:9092\"] index=>\"%{[fields][NAMESPACE]}-%{[fields][PROJECTNAME]}-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"logstash-pipeline\" password => \"logstash-pipeline\" } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-08 18:06:33 "},"origin/logging-kafka基础知识.html":{"url":"origin/logging-kafka基础知识.html","title":"基础知识","keywords":"","body":"一、Kafka 与 Zookeeper 1、Zookeeper在Kafka集群分布式消息中的作用 1.1、选举Controller Kafka是高可用的分布式消息系统，首先要解决的就是资源协调分配和多副本状态维护的问题。解决这些问题通常就是两种思路，一是依靠Zookeeper来协调，二是设定一个中心节点，让这个中心节点来协调。如果依靠Zookeeper来协调，会存在大量的竞争条件，对Zookeeper的访问压力增大，而且如果Zookeeper出现了问题（比如网络抖动），系统很容易出现紊乱。Kafka采用的是第二种思路，即选举一个中心节点来进行资源协调与多副本状态维护，这个中心节点被称作Controller（一个特殊的Broker），这个选举过程依靠Zookeeper来完成。 Broker启动时，会竞争创建临时\"/controller\"。如果创建成功，则成为Controller，并把Broker的id等信息写入这个节点。同时会全程监控\"/controller\"的数据变化，如果旧的Controller挂掉，则开启新一轮的竞争过程。 1.2、注册Broker Kafka要进行资源协调，第一件需要知道的事情就是各个Broker的存活状态，这个问题利用Zookeeper可以很容易做到。 假设某个Broker，id为0，它启动时，会创建\"/brokers/ids/0\"临时节点，并把端口等信息写进去。Controller会监控\"/brokers/ids\"的节点变化，以实时感知各broker的状态，进行资源协调。 1.3、协调topic的创建、调整与销毁 在Kafka这个多副本分区的消息系统里，创建一个topic，至少需要以下3个步骤： 持久化topic的多副本分区信息 为每个分区挑选一个副本leader 将上述信息发送给对应的Broker，以完成实际的日志文件创建过程 Controller的存在，可以很容易完成上面的b和c步骤，但a步骤不行，如果Controller挂掉，则这些信息会不可用。Kafka把这些信息保存在Zookeeper中，依靠其高可用特性来保证这些信息的高可用。假设某个topic名字为mytopic，创建时，其分区信息保存在\"/brokers/topics/mytopic\"中。Controller全程监控\"/brokers/topics\"的孩子节点变动，实时感知这些信息，以完成后续步骤。 创建完成之后，后续往往会有分区调整和topic删除等需求。普通青年可能会觉得这两个问题很简单，给Controller发个相关请求就可以了。事实远非如此！ 拿分区调整来说，假设某分区有三个副本，分别位于Broker-1、Broker-2和Broker-3，leader为1，现在扩容增加了Broker-4、Broker-5、Broker-6，为了平衡机器间压力，需要将副本1 2 3移到4 5 6，至少经历以下步骤： 修改该分区的副本信息为1 2 3 4 5 6，leader为1 等待4 5 6副本追赶1 2 3的进度直至大家都同步(in sync) 从4 5 6中挑选一个新的副本leader，假设为4 修改该分区的副本信息为4 5 6，leader为4 以上每个步骤都有可能失败，如何才能保证这次调整顺利进行呢？ 首先，我们不能直接修改该分区的副本信息为 4 5 6，原因很简单，需要等待4 5 6的追赶过程以便产生新leader。其次，操作未完全成功的命令需要保存下来，如果操作过程中，Controller挂掉，则新的Controller可以从头开始直至成功。Kafka怎么做的呢？ 通常是Admin控制台）把调整命令写入\"/admin/reassign_partitions\"节点 Controller监控\"/admin/reassign_partitions\"，拿到调整命令，执行上述步骤 如果操作成功则删除该节点；如果Controller挂掉，新的Controller还会拿到这个命令并从头开始执行 当然，这里一次只能有一个调整命令，但一个调整命令可以同时调整多个topic的多个分区。 在这个过程中，Zookeeper的作用是：持久化操作命令并实时通知操作者，是不是只有Zookeeper可以做这个事情呢，不是，但Zookeeper可以做得很好，保证命令高可用。 类似的操作还有topic删除，副本的leader变更等，都是沿用上面的套路。 1.4. 保存topic级别和client级别的配置信息 Broker的集群中有全局配置信息，但如果想针对某个topic或者某个client进行配置呢，Kafka把这些信息保存在Zookeeper中，各个Broker实时监控以更新。 1.5、脑裂问题 脑裂问题是指，在一个设有中心节点的系统中，出现了两个中心节点。两个中心同时传达命令，自然会造成系统的紊乱。 Kafka利用Zookeeper所做的第一件也是至关重要的一件事情是选举Controller，那么自然就有疑问，有没有可能产生两个Controller呢？ 首先，Zookeeper也是有leader的，它有没有可能产生两个leader呢？答案是不会。 quorum机制可以保证，不可能同时存在两个leader获得大多数支持。假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。 Kafka的Controller也采用了epoch，具体机制如下: 所有Broker监控\"/controller\"，节点被删除则开启新一轮选举，节点变化则获取新的epoch Controller会注册SessionExpiredListener，一旦因为网络问题导致Session失效，则自动丧失Controller身份，重新参与选举 收到Controller的请求，如果其epoch小于现在已知的controller_epoch，则直接拒绝 理论上来说，如果Controller的SessionExpired处理成功，则可以避免双leader，但假设SessionExpire处理意外失效的情况：旧Controller假死，新的Controller创建。旧Controller复活，SessionExpired处理意外失效，仍然认为自己是leader。 这时虽然有两个leader，但没有关系，leader只会发信息给存活的broker（仍然与Zookeeper在Session内的），而这些存活的broker则肯定能感知到新leader的存在，旧leader的请求会被拒绝。 1.6、如果Zookeeper挂了会怎样 每个Broker有一个metaDataCache，缓存有topic和partition的基本信息，可以正常的生产和消费信息，但不能进行topic的创建、调整和删除等操作。 此外，Broker会不断重试连接。 1.7、Zookeeper用量估计 假设Broker数目为B，topic数目为T，所有topic总partition数目为P，Client数目为C，以下数值均为峰值： qps: 100以内 连接数: B watcher数目：3 * B + 2 * T + 6 Zookeeper节点数（叶子节点）: B + P + T + C + 8 2、kafka注册到zookeeper中的数据存储结构 Zookeeper路径的创建者与监听者 路径 创建者 监听者 类型 /controller 各个broker竞争创建 所有broker全程监控data change 临时节点 /controller_epoch controller 无 永久节点 /brokers/ids broker启动时检查并确保存在 controller全程监控child change 永久节点 /brokers/ids/{id} id对应的broker 无 临时节点 /brokers/topics broker启动时检查确保存在 controller全程监控child change 永久节点 /brokers/topics/{topic} controller收到创建请求，或者broker启用自动创建topic时，或admin工具 controller全程监控data change 永久节点 /brokers/topics/{topic}/{partition}/state partiton的leader partition reassign时，controller临时监控data change 永久节点 /config/changes broker启动时检查并确保存在 所有broker全程监控child change 永久节点 /config/topics broker启动时检查并确保存在 无 永久节点 /config/clients broker启动时检查并确保存在 无 永久节点 /brokers/seqid broker启动时检查并确保存在 待确认 永久节点 /admin/delete_topics broker启动时检查并确保存在 controller全程监控child change 永久节点 /isr_change_notification broker启动时检查并确保存在 controller全程监控child change 永久节点 /admin/reassign_partitions admin 工具 controller全程监控data change 永久节点，reassign结束后会删除 /admin/preferred_replica_election admin 工具 controller全程监控data change 永久节点，replica election结束后会删除 2.1、Topic注册信息 /brokers/topics/[topic] 存储某个topic的partitions所有分配信息 { \"version\": \"版本编号目前固定为数字1\", \"partitions\": { \"partitionId编号\": [ 同步副本组brokerId列表 ], \"partitionId编号\": [ 同步副本组brokerId列表 ], ....... } } 2.2、Partition状态信息 /brokers/topics/[topic]/partitions/[partition-Id]/state Schema: { \"controller_epoch\": 表示kafka集群中的中央控制器选举次数, \"leader\": 表示该partition选举leader的brokerId, \"version\": 版本编号默认为1, \"leader_epoch\": 该partition leader选举次数, \"isr\": [同步副本组brokerId列表] } 2.3、Broker注册信息 /brokers/ids/[0...N] 每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL) Schema: { \"jmx_port\": jmx端口号, \"timestamp\": kafka broker初始启动时的时间戳, \"host\": 主机名或ip地址, \"version\": 版本编号默认为1, \"port\": kafka broker的服务端端口号,由server.properties中参数port确定 } 2.4、Controller epoch /controller_epoch -> int (epoch) 此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1 2.5、Controller注册信息 /controller -> int (broker id of the controller) 存储center controller中央控制器所在kafka broker的信息 { \"version\": 版本编号默认为1, \"brokerid\": kafka集群中broker唯一编号, \"timestamp\": kafka broker中央控制器变更时的时间戳 } 2.6、Consumer注册信息 /consumers/[groupId]/ids/[consumerIdString] 每个consumer都有一个唯一的ID(consumerId可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.这是一个临时的znode,此节点的值为请看consumerIdString产生规则,即表示此consumer目前所消费的topic + partitions列表. Schema: { \"version\": 版本编号默认为1, \"subscription\": { //订阅topic列表 \"topic名称\": consumer中topic消费者线程数 }, \"pattern\": \"static\", \"timestamp\": \"consumer启动时的时间戳\" } 2.7、Consumer offset /consumers/[groupId]/offsets/[topic]/[partitionId] -> long (offset) 用来跟踪每个consumer目前所消费的partition中最大的offset.此znode为持久节点,可以看出offset跟group_id有关,以表明当消费者组(consumer group)中一个消费者失效,重新触发balance,其他consumer可以继续消费 2.8、admin管理信息 二、Kafka中的消费者与消费者组 1、消费者组里面的消费者消费Topic Partition的消息时流程 每个consumer客户端被创建时,会向zookeeper注册自己的信息.主要是为了\"负载均衡\" 同一个Consumer Group中的Consumers，Kafka将相应Topic中的每个消息只发送给其中一个Consumer。 Consumer Group中的每个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer； 一个Consumer group的多个consumer的所有线程依次有序地消费一个topic的所有partitions,如果Consumer group中所有consumer总线程大于partitions数量，则会出现空闲情况 举例说明： kafka集群中创建一个topic为report-log，4个partitions 索引编号为0,1,2,3。假如有目前有三个消费者node（注意：一个consumer中一个消费线程可以消费一个或多个partition） 如果每个consumer创建一个consumer thread线程,各个node消费情况如下，node1消费索引编号为0,1分区，node2费索引编号为2,node3费索引编号为3 如果每个consumer创建2个consumer thread线程，各个node消费情况如下(是从consumer node先后启动状态来确定的)，node1消费索引编号为0,1分区；node2费索引编号为2,3；node3为空闲状态 总结：从以上可知，Consumer Group中各个consumer是根据先后启动的顺序有序消费一个topic的所有partitions的。如果Consumer Group中所有consumer的总线程数大于partitions数量，则可能consumer thread或consumer会出现空闲状态。 2、Consumer均衡算法 当一个group中,有consumer加入或者离开时,会触发partitions均衡(均衡的最终目的,是提升topic的并发消费能力) 假如topic1,具有如下partitions: P0,P1,P2,P3 加入group中,有如下consumer: C0,C1 首先根据partition索引号对partitions进行排序，假设排序: P0,P1,P2,P3 根据(consumer.id + '-'+ thread序号)对消费者进行排序,假设排序: C0,C1 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i M),P((i + 1) M -1)] 3、Consumer启动流程 首先进行\"Consumer Id注册\"; 然后在\"Consumer id 注册\"节点下注册一个watch用来监听当前group中其他consumer的\"退出\"和\"加入\";只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions). 在\"Broker id 注册\"节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance. 参考链接 http://blog.csdn.net/lizhitao/article/details/23744675 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/logging-kafka常用操作.html":{"url":"origin/logging-kafka常用操作.html","title":"kafka常用操作","keywords":"","body":"Apache Kafka常用操作 一、Topic管理 1. 列出所有Topic kafka-topics.sh --zookeeper 127.0.0.1:2181 --list kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list 2. 创建一个topic kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 2 --partitions 3 --topic Test #--replication-factor参数指定Topic的数据副本个数 #--partitions参数指定Topic的分区个数 3. 删除Topic kafka-topics.sh --delete --zookeeper 127.0.0.1:2181 --topic Test 或者 #只会删除zookeeper中的元数据，消息文件须手动删除 kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper 172.16.3.12:2181/kafka/q-35aw0fye --topic Test 4. 查看Topic的详细信息 kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic Test Topic:Test PartitionCount:2 ReplicationFactor:1 Configs: Topic: Test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: Test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 #第一行，列出了topic的名称，分区数(PartitionCount),副本数(ReplicationFactor)以及其他的配置(Config.s) #Leader:1 表示为做为读写的broker的节点编号 #Replicas:表示该topic的每个分区在那些borker中保存 #Isr:表示当前有效的broker, Isr是Replicas的子集 5. 增加Topic分区个数（只能增加扩容） kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --partitions 2 6. 给Topic增加配置 kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --config flush.messages=1 7. 删除Topic的配置 kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --delete-config flush.messages=1 8. 查看消费者组 kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --list 9. 查看Topic各个分区的消息偏移量最大（小）值 kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 127.0.0.1:9092 --time -1 --topic Test # time为-1时表示最大值，time为-2时表示最小值 10. 查看Topic中指定consumer组内消息消费的offset kafka的offset保存位置分为两种情况 0.9.0.0版本之前默认保存在zookeeper当中 ，0.9.0.0版本之后保存在broker对应的topic当中 kafka-consumer-offset-checker.sh --zookeeper 127.0.0.1:2181 --group logstash-group --topic Test GROUP TOPIC PID OFFSET LOGSIZE LAG Ower 消费者组 话题id 分区id 当前已消费的条数 总条数 未消费的条数 所有者 console-consumer-98995 Test 0 112 318084 317972 none console-consumer-98995 Test 1 -1 318088 unknown none 方式二： $ kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --offsets --group Group-Name TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID basic_log_k8s 1 127509 333334 205825 logstash-0-490058d7-154f-4111-b514-57de254ecae8 /192.168.3.72 logstash-0 basic_log_k8s 0 127317 333333 206016 logstash-0-2bd85bcc-282e-41a0-a9c3-6d0dbefd547f /192.168.0.40 logstash-0 11. 修改指定消费者分组对应topic的offset 第一种情况offset信息保存在topic中 $ bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 #参数解析： #--bootstrap-server 代表你的kafka集群 你的offset保存在topic中 #--group 代表你的消费者分组 #--topic 代表你消费的主题 #--execute 代表支持复位偏移 #--reset-offsets 代表要进行偏移操作 #--to-offset 代表你要偏移到哪个位置 是long类型数值，只能比前面查询出来的小 #还有其他的--to- ** 方式可以自己验证 本人验证过--to-datetime 没有成功 第二种方式offset信息保存在zookeeper当中 $ bin/kafka-consumer-groups.sh --zookeeper kafka_zk1:2181 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 12. 修改topic副本因子数 官方文档：https://kafka.apache.org/21/documentation.html#replication ① 先查看Topic的信息 $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 ② 准备JSON文件 { \"version\": 1, \"partitions\": [ { \"topic\": \"test\", \"partition\": 0, \"replicas\": [2, 1, 3] }, { \"topic\": \"test\", \"partition\": 1, \"replicas\": [3, 2, 1] }] } ③ kafka-reassign-partitions命令增加topic分区副本数 $ kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file replication.json --execute Current partition replica assignment {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3]}]} Save this to use as the --reassignment-json-file option during rollback Successfully started reassignment of partitions {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2,1,3]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3,2,1]}]} ④ 使用verify参数来检查副本数据是否复制分配完成 $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] is still in progress Reassignment of partition [test,1] is still in progress $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] completed successfully Reassignment of partition [test,1] completed successfully $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:iteblog PartitionCount:2 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Topic: test Partition: 1 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 13. 均衡Topic分区到新增Broker节点 重新分配官方文档地址：http://kafka.apache.org/documentation/#basic_ops_cluster_expansion 翻译官方文档中文地址：http://orchome.com/36 参考文章：https://blog.csdn.net/forrest_ou/article/details/79141391 ① 确定要重启分配分区的主题，新建topics-to-move.json json文件 { \"topics\": [ {\"topic\": \"foo1\"}, {\"topic\": \"foo2\"} ], \"version\":1 } // foo1 foo2 为要重新分配的主题 ② 使用 bin/kafka-reassign-partitions.sh重新分配工具生成分配规则的json语句分配到 5，6机器 kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list \"5,6\" –generate ③ 有分配规则的json语句输出到控制台，复制到新建的json文件expand-cluster-reassignment.json中，例如： {\"version\":1, \"partitions\":[{\"topic\":\"foo1\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":2,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":2,\"replicas\":[5,6]}] } //描述分配之后分区的分布情况 ④ 执行命令，开始分区重新分配 kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –execute ⑤ 验证是否完成 kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –verify //当输出全部都是completed successfully表明移动已经完成. 注意 kafka新建主题时的分区分配策略：随机选取第一个分区节点，然后往后依次增加。例如第一个分区选取为1，第二个分区就 是2，第三个分区就是3. 1，2，3是brokerid。不会负载均衡，所以要手动重新分配分区操作，尽量均衡。 在生产的同时进行数据迁移会出现重复数据。所以迁移的时候避免重复生产数据，应该停止迁移主题的生产。同时消费不会，同时消费之后出现短暂的leader报错，会自动恢复。 新增了broker节点，如果有主题的分区在新增加的节点上，生产和消费的客户端都应该在hosts配置文件中增加新增的broker节点，否则无法生产消费，但是也不报错。 可以不需要第一步和第二步，自己手动新建分配的json文件 14. 查询Topic不可用的分区 kafka-topics.sh --describe --unavailable-partitions --zookeeper localhost:2181 二、其他 1. 自带测试生产者 kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic Test 2. 自带测试消费者 kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --from-beginning --topic Test 3. 自带性能测试 位于bin/kafka-producer-perf-test.sh.主要参数有以下: messages 生产者发送总的消息数量 message-size 每条消息大小（单位为b） batch-size 每次批量发送消息的数量 topics 生产者发送的topic threads 生产者使用几个线程同时发送 例如 kafka-producer-perf-test.sh --messages 100000 --message-size 1000 --batch-size 10000 --topics test --threads 4 --broker-list 127.0.0.1:9092 start.time, end.time, compression, message.size, batch.size, total.data.sent.in.MB, MB.sec, total.data.sent.in.nMsg, nMsg.sec 2015-10-15 18:56:27:542, 2015-10-15 18:56:30:880, 0, 1000, 10000, 95.37, 28.5702, 100000, 29958.0587 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-17 16:02:17 "},"origin/filebeat-简介安装配置.html":{"url":"origin/filebeat-简介安装配置.html","title":"简介安装配置","keywords":"","body":"Filebeat的简介、安装、配置、Pipeline 一. 简介 Filebeat由两个主要组件组成： Inputs： 负责管理harvester并找到所有要读取的文件来源。如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。每个Inputs都在自己的Go协程中运行 每个prospector类型可以定义多次 Harvesters： 一个harvester负责读取一个单个文件的内容，每个文件启动一个harvester。harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会续读这个文件。这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。默认情况下，Filebeat保存文件打开的状态直到close_inactive到达。 关闭harvester会产生以下结果： 如果在harvester仍在读取文件时文件被删除，则关闭文件句柄，释放底层资源。 文件的采集只会在scan_frequency过后重新开始 如果在harvester关闭的情况下移动或移除文件，则不会继续处理文件 二. 安装 默认的安装文件路径 Type Description Default Location Config Option home Home of the Filebeat installation. path.home bin The location for the binary files. {path.home}/bin config The location for configuration files. {path.home} path.config data The location for persistent data files. {path.home}/data path.data logs The location for the logs created by Filebeat. {path.home}/logs path.logs YUM/RPM [elastic-7.x] name=Elastic repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install filebeat-7.4.0 RPM下载地址：https://www.elastic.co/cn/downloads/beats/filebeat yum localinstall -y filebeat-7*.rpm 安装文件路径 Type Description Location home Home of the Filebeat installation. /usr/share/filebeat bin The location for the binary files. /usr/share/filebeat/bin config The location for configuration files. /etc/filebeat data The location for persistent data files. /var/lib/filebeat logs The location for the logs created by Filebeat. /var/log/filebeat 二进制文件 zip, tar.gz, tgz 压缩格式的二进制安装包，下载地址：https://www.elastic.co/cn/downloads/beats/filebeat 安装文件路径 Type Description Location home Home of the Filebeat installation. {extract.path} bin The location for the binary files. {extract.path} config The location for configuration files. {extract.path} data The location for persistent data files. {extract.path}/data logs The location for the logs created by Filebeat. {extract.path}/logs Filebeat命令行启动 /usr/share/filebeat/bin/filebeat Commands SUBCOMMAND [FLAGS] Commands 描述 export 导出配置到控制台，包括index template, ILM policy, dashboard help 显示帮助文档 keystore 管理secrets keystore. modules 管理配置Modules run Runs Filebeat. This command is used by default if you start Filebeat without specifying a command. setup 设置初始环境。包括index template, ILM policy, write alias, Kibana dashboards (when available), machine learning jobs (when available). test 测试配置文件 version 显示版本信息 Global Flags 描述 -E \"SETTING_NAME=VALUE\" 覆盖配置文件中的配置项 --M \"VAR_NAME=VALUE\" 覆盖Module配置文件的中配置项 -c FILE 指定filebeat的配置文件路径。路径要相对于`path.config -d SELECTORS -e --path.config --path.data --path.home --path.logs --strict.perms 示例： /usr/share/filebeat/bin/filebeat --modules mysql -M \"mysql.slowlog.var.paths=[/root/slow.log]\" -e /usr/share/filebeat/bin/filebeat -e -E output.console.pretty=true --modules mysql -M \"mysql.slowlog.var.paths=[\"/root/mysql-slow-sql-log/mysql-slowsql.log\"]\" -M \"mysql.error.enabled=false\" -E output.elasticsearch.enabled=false SystemD启动 systemctl enable filebeat systemctl start filebeat systemctl stop filebeat systemctl status filebeat journalctl -u filebeat.service systemctl daemon-reload systemctl restart filebeat Filebeat的SystemD配置文件 $ /usr/lib/systemd/system/filebeat.service [Unit] Description=Filebeat sends log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat.yml\" Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target Variable Description Default value BEAT_LOG_OPTS Log options -e BEAT_CONFIG_OPTS Flags for configuration file path -c /etc/filebeat/filebeat.yml BEAT_PATH_OPTS Other paths -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat 三. Docker镜像 docker pull docker.elastic.co/beats/filebeat:7.4.0 docker pull filebeat:7.4.0 镜像中的安装文件路径 Type Description Location home Home of the Filebeat installation. /usr/share/filebeat bin The location for the binary files. /usr/share/filebeat config The location for configuration files. /usr/share/filebeat data The location for persistent data files. /usr/share/filebeat/data logs The location for the logs created by Filebeat. /usr/share/filebeat/logs Kubernetes部署 默认部署到kube-system命名空间 部署类型是Daemonset，会部署到每一个Node上 每个Node上的/var/lib/docker/containers目录会挂载到filebeat容器中 默认Filebeat会将日志吐到kube-system命名空间下的elasticsearch中，如果需要指定吐到其他elasticsearch中，修改环境变量 - name: ELASTICSEARCH_HOST value: elasticsearch - name: ELASTICSEARCH_PORT value: \"9200\" - name: ELASTICSEARCH_USERNAME value: elastic - name: ELASTICSEARCH_PASSWORD value: changeme curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml kubectl create -f filebeat-kubernetes.yaml kubectl --namespace=kube-system get ds/filebeat OKD部署 curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml 修改部署文件 securityContext: runAsUser: 0 privileged: true oc adm policy add-scc-to-user privileged system:serviceaccount:kube-system:filebeat 四. 配置 Filebeat的配置文件路径：/etc/filebeat/filebeat.yml 配置语法为YAML 配置项 描述 示例 processors.* Processors配置 processors:- include_fields: fields: [\"cpu\"]- drop_fields: fields: [\"cpu.user\", \"cpu.system\"] filebeat.modules: Module配置 filebeat.modules:- module: mysql error: enabled: true filebeat.inputs: Input配置 filebeat.inputs:- type: log enabled: false paths: - /var/log/*.log output.*: Output配置 output.console: enabled: true path.* 组件产生文件的位置配置 path.home: /usr/share/filebeatpath.data: ${path.home}/datapath.logs: ${path.home}/logs setup.template.* Template配置 logging.* 日志配置 logging.level: infologging.to_stderr: falselogging.to_files: true monitoring.* X-Pack监控配置 monitoring.enabled: falsemonitoring.elasticsearch.hosts: [\"localhost:9200\"] http.* HTTP Endpoint配置 http.enabled: falsehttp.port: 5066http.host: localhost filebeat.autodiscover.* Filebeat自动发现配置 通用配置 全局配置项 queue.* 缓存队列设置 全局配置项 配置项 默认值 描述 示例 registry.path ${path.data}/registry 注册表文件的根路径 filebeat.registry.path: registry registry.file_permissions 0600 注册表文件的权限。Window下该配置项无效 filebeat.registry.file_permissions: 0600 registry.flush 0s filebeat.registry.flush: 5s registry.migrate_file filebeat.registry.migrate_file: /path/to/old/registry_file config_dir filebeat.config_dir: path/to/configs shutdown_timeout 5s filebeat.shutdown_timeout: 5s 通用配置项 配置项 默认值 描述 示例 name name: \"my-shipper\" tags tags: [\"service-X\", \"web-tier\"] fields fields: {project: \"myproject\", instance-id: \"57452459\"} fields_under_root 如果该选项设置为true，则新增fields会放在根路径下，而不是放在fields路径下。自定义的field会覆盖filebeat默认的field。 fields_under_root: true processors 该配置项可配置以下Processors，详见 max_procs 配置示例 # Modules配置项 filebeat.modules: - module: system # 通用配置项 fields: level: debug review: 1 fields_under_root: false # Processors配置项 processors: - decode_json_fields: # Input配置项 filebeat.inputs: - type: log # Output配置项 output.elasticsearch: output.logstash: 五. Input插件类型 Input类型 类型 描述 配置示例 Log 从日志文件中读取每一行 filebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log Stdin filebeat.inputs: - type: stdin Container filebeat.inputs: - type: container paths: - '/var/lib/docker/containers//.log' Kafka filebeat.inputs: - type: kafka hosts: - kafka-broker-1:9092 - kafka-broker-2:9092 topics: [\"my-topic\"] group_id: \"filebeat\" Redis filebeat.inputs: - type: redis hosts: [\"localhost:6379\"] password: \"${redis_pwd}\" UDP filebeat.inputs: - type: udp max_message_size: 10KiB host: \"localhost:8080\" Docker filebeat.inputs: - type: docker containers.ids: - 'e067b58476dc57d6986dd347' TCP filebeat.inputs: - type: tcp max_message_size: 10MiB host: \"localhost:9000\" Syslog filebeat.inputs: - type: syslog protocol.udp: host: \"localhost:9000\" s3 filebeat.inputs: - type: s3 queue_url: https://test.amazonaws.com/12/test access_key_id: my-access-key secret_access_key: my-secret-access-key NetFlow Google Pub/Sub 六. Output插件类型 类型 描述 配置样例 Elasticsearch output.elasticsearch: hosts: [\"https://localhost:9200\"] protocol: \"https\" index: \"filebeat-%{[agent.version]}-%{+yyyy.MM.dd}\" ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] ssl.certificate: \"/etc/pki/client/cert.pem\" ssl.key: \"/etc/pki/client/cert.key\" username: \"filebeat_internal\" password: \"YOUR_PASSWORD\" Logstash output.logstash: hosts: [\"127.0.0.1:5044\"] Kafka output.kafka: hosts: [\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"] topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 Redis output.redis: hosts: [\"localhost\"] password: \"my_password\" key: \"filebeat\" db: 0 timeout: 5 File output.file: path: \"/tmp/filebeat\" filename: filebeat #rotate_every_kb: 10000 #number_of_files: 7 #permissions: 0600 Console output.console: pretty: true Cloud 七. Processors插件 配置语法 processors: - if: then: - : - : ... else: - : - : 可以再Input中添加Processor - type: processors: - : when: 条件语法 equals equals: http.response.code: 200 contains contains: status: \"Specific error\" regexp regexp: system.process.name: \"foo.*\" range：The condition supports lt, lte, gt and gte. The condition accepts only integer or float values. range: http.response.code: gte: 400 network network: source.ip: private destination.ip: '192.168.1.0/24' destination.ip: ['192.168.1.0/24', '10.0.0.0/8', loopback] has_fields has_fields: ['http.response.code'] or or: - - - ... ----------------------------- or: - equals: http.response.code: 304 - equals: http.response.code: 404 and and: - - - ... ----------------------------- and: - equals: http.response.code: 200 - equals: status: OK ----------------------------- or: - - and: - - not not: -------------- not: equals: status: OK 支持的Processors 类型 作用 配置样例 add_cloud_metadata add_docker_metadata processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" add_fields processors:- add_fields: target: project fields: name: myproject id: '574734885120952459' add_host_metadata processors: - add_host_metadata: netinfo.enabled: false cache.ttl: 5m geo: name: nyc-dc1-rack1 location: 40.7128, -74.0060 continent_name: North America country_iso_code: US region_name: New York region_iso_code: NY city_name: New York add_kubernetes_metadata processors: - add_kubernetes_metadata: host: kube_config: ~/.kube/config default_indexers.enabled: false default_matchers.enabled: false indexers: - ip_port: matchers: - fields: lookup_fields: [\"metricset.host\"] add_labels processors:- add_labels: labels: number: 1 with.dots: test nested: with.dots: nested array: - do - re - with.field: mi add_locale processors:- add_locale: ~processors:- add_locale: format: abbreviation add_observer_metadata add_process_metadata add_tags processors:- add_tags: tags: [web, production] target: \"environment\" community_id convert processors: - convert: fields: - {from: \"src_ip\", to: \"source.ip\", type: \"ip\"} - {from: \"src_port\", to: \"source.port\", type: \"integer\"} ignore_missing: true fail_on_error: false decode_base64_field decode_cef decode_csv_fields decode_json_fields decompress_gzip_field dissect processors:- dissect: tokenizer: \"%{key1} %{key2}\" field: \"message\" target_prefix: \"dissect\" dns drop_event processors:- drop_event: when: condition drop_fields processors:- drop_fields: when: condition fields: [\"field1\", \"field2\", ...] ignore_missing: false extract_array processors: - extract_array: field: my_array mappings: source.ip: 0 destination.ip: 1 network.transport: 2 include_fields processors: - include_fields: when: condition fields: [\"field1\", \"field2\", ...] registered_domain rename processors: - rename: fields: - from: \"a.g\" to: \"e.d\" ignore_missing: false fail_on_error: true script timestamp 八. Modules Filebeat modules simplify the collection, parsing, and visualization of common log formats. A typical module (say, for the Nginx logs) is composed of one or more filesets (in the case of Nginx, access and error). A fileset contains the following: Filebeat input configurations, which contain the default paths where to look for the log files. These default paths depend on the operating system. The Filebeat configuration is also responsible with stitching together multiline events when needed. Elasticsearch Ingest Node pipeline definition, which is used to parse the log lines. Fields definitions, which are used to configure Elasticsearch with the correct types for each field. They also contain short descriptions for each of the fields. Sample Kibana dashboards, when available, that can be used to visualize the log files. Filebeat automatically adjusts these configurations based on your environment and loads them to the respective Elastic stack components. Filebeat modules require Elasticsearch 5.2 or later. Modules管理 1. 查看所有Modules filebeat modules list 2. 开启Modules filebeat modules enable module名 支持的Modules 类型 作用 配置样例 Modules overview Apache module Auditd module AWS module CEF module Cisco module Coredns Module Elasticsearch module Envoyproxy Module Google Cloud module haproxy module IBM MQ module Icinga module IIS module Iptables module Kafka module Kibana module Logstash module MongoDB module MSSQL module MySQL module nats module NetFlow module Nginx module Osquery module Palo Alto Networks module PostgreSQL module RabbitMQ module Redis module Santa module Suricata module System module Traefik module Zeek (Bro) Module 九. 采集注册文件解析 采集注册文件路径：/var/lib/filebeat/registry/filebeat/data.json [{\"source\":\"/root/mysql-slow-sql-log/mysql-slowsql.log\",\"offset\":1365442,\"timestamp\":\"2019-10-11T09:29:35.185399057+08:00\",\"ttl\":-1,\"type\":\"log\",\"meta\":null,\"FileStateOS\":{\"inode\":2360926,\"device\":2051}}] source # 记录采集日志的完整路径 offset # 已经采集的日志的字节数;已经采集到日志的哪个字节位置 timestamp # 日志最后一次发生变化的时间戳 ttl # 采集失效时间，-1表示只要日志存在，就一直采集该日志 type: meta filestateos # 操作系统相关 　　inode # 日志文件的inode号 　　device # 日志所在磁盘的磁盘编号 硬盘格式化的时候，操作系统自动将硬盘分成了两个区域。 一个是数据区，用来存放文件的数据信息 一个是inode区，用来存放文件的元信息，比如文件的创建者、创建时间、文件大小等等 每一个文件都有对应的inode，里边包含了与该文件有关的一些信息，可以用stat命令查看文件的inode信息 > stat /var/log/messages File: ‘/var/log/messages’ Size: 56216339 Blocks: 109808 IO Block: 4096 regular file Device: 803h/2051d Inode: 1053379 Links: 1 Access: (0600/-rw-------) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-10-06 03:20:01.528781081 +0800 Modify: 2019-10-12 13:59:13.059112545 +0800 Change: 2019-10-12 13:59:13.059112545 +0800 Birth: - 2051为十进制数，对应十六进制数803 参考链接 https://www.cnblogs.com/micmouse521/p/8085229.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-17 18:22:55 "},"origin/logstash-简介安装配置Pipeline.html":{"url":"origin/logstash-简介安装配置Pipeline.html","title":"简介安装配置Pipeline","keywords":"","body":"Logastash的简介、安装、配置、Pipeline、插件 一. 简介 官方文档：https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html Logstash是一个开源数据收集引擎，具有实时管道功能。 Logstash可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地 Logstash 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，还有强大的插件功能，常用于日志处理。 Logstash耗资源较大，运行占用CPU和内存高。另外没有消息队列缓存，存在数据丢失隐患 Logstash使用Ruby语言编写的运行在Java虚拟机上的具有收集、分析和转发数据流功能的工具 Logstash使用Pipeline方式进行日志的搜集，处理和输出 Event：logstash将数据流中的每一条数据在input处被转换为event，在output处event再被转换为目标格式的数据 Inputs：用于从数据源获取Event。每个Input启动一个线程，从对应数据源获取数据，将数据写入一个队列 Filters：用于过滤、修改Event Outputs：负责输出Event到其他系统中 Logstash使用Pipeline流水线的形式来处理数据Event事件，大致流程如下 其中inputs和outputs支持codecs（coder&decoder）在1.3.0 版之前，logstash 只支持纯文本形式输入，然后用filter处理它。但现在，我们可以在输入期间处理不同类型的数据。所以现在的数据处理流程 箭头代表数据流向。可以有多个input。中间的queue负责将数据分发到不通的pipline中，每个pipline由batcher，filter和output构成。batcher的作用是批量从queue中取数据（可配置）。 logstash数据流历程 首先有一个输入数据，例如是一个web.log文件，其中每一行都是一条数据。file imput会从文件中取出数据，然后通过json codec将数据转换成logstash event。 这条event会通过queue流入某一条pipline处理线程中，首先会存放在batcher中。当batcher达到处理数据的条件（如一定时间或event一定规模）后，batcher会把数据发送到filter中，filter对event数据进行处理后转到output，output就把数据输出到指定的输出位置。 输出后还会返回ACK给queue，包含已经处理的event，queue会将已处理的event进行标记。 queue分类 In Memory： 在内存中，固定大小，无法处理进程crash. 机器宕机等情况，会导致数据丢失。 Persistent Queue：可处理进程crash情况，保证数据不丢失。保证数据至少消费一次；充当缓冲区，可代替kafka等消息队列作用。 Dead Letter Queues：存放logstash因数据类型错误等原因无法处理的Event Persistent Queue（PQ）处理流程 一条数据经由input进入PQ，PQ将数据备份在disk，然后PQ响应input表示已收到数据； 数据从PQ到达filter/output，其处理到事件后返回ACK到PQ； PQ收到ACK后删除磁盘的备份数据； 二. 安装 1. 安装Java环境 在一些Linux环境下，必须设置JAVA_HOME环境变量，否则Logstash在安装期间没有检测到JAVA_HOME环境变量，会报错并且启动不起来服务。如果JDK目录在/opt下，则 在/usr/bin/下建立软连接指向JAVA_HOME/bin路径下的java 2. 安装Logstash YUM/RPM [elasticsearch-7.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install -y logstash-7.2.0 手动下载RPM安装，官方下载链接：https://www.elastic.co/downloads/logstash yum localinstall -y logstash-7*.rpm RPM包安装后各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. /usr/share/logstash bin Binary scripts including logstash to start Logstash and logstash-plugin to install plugins /usr/share/logstash/bin settings Configuration files, including logstash.yml, jvm.options, and startup.options /etc/logstash path.settings conf Logstash pipeline configuration files /etc/logstash/conf.d/*.conf See /etc/logstash/pipelines.yml logs Log files /var/log/logstash path.logs plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. /usr/share/logstash/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. /var/lib/logstash path.data 二进制包 二进制包中各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. {extract.path}- Directory created by unpacking the archive bin Binary scripts, including logstash to start Logstash and logstash-plugin to install plugins {extract.path}/bin settings Configuration files, including logstash.yml and jvm.options {extract.path}/config path.settings logs Log files {extract.path}/logs path.logs plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. {extract.path}/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. {extract.path}/data path.data 3. 启动 以服务形式或命令启动Logstash systemctl start logstash #后台会起一个名叫org.jruby.Main的Java后台进程，用jps -l查看 jps -l 使用二进制执行文件启动 /user/share/logstash/bin/logstash -f logstash.conf --config.reload.automatic #-f 指定配置文件路径 #--config.reload.automatic 自动检测加载配置文件，该参数在有-e参数是不生效 #--config.reload.interval 设置多少秒检测一次配置文件 如果Logstash启动时没有配置自动加载配置文件，重启进程时加上。 4. 验证 /usr/share/logstash/bin/logstash -e 'input { stdin { } } output { stdout {} }' #参数-e：直接从命令行定义配置信息 #配置从标准输入读取输入，然后输出到标准输出 stdin > hello world stdout> 2013-11-21T01:22:14.405+0000 0.0.0.0 hello world #Logstash会在消息上添加时间戳和IP地址 #Ctrl+D 退出Logstash 5. 命令行参数 参数 描述 默认值 -r, --config.reload.automatic Monitor configuration changes and reload whenever it is changed. NOTE: use SIGHUP to manually reload the config false -n, --node.name NAME Specify the name of this logstash instance, if no value is given it will default to the current hostname. 当前主机名 -f, --path.config CONFIG_PATH Load the logstash config from a specific file or directory. If a directory is given, all files in that directory will be concatenated in lexicographical order and then parsed as a single config file. You can also specify wildcards (globs) and any matched files will be loaded in the order described above. -e, --config.string CONFIG_STRING Use the given string as the configuration data. Same syntax as the config file. If no input is pecified, then the following is used as the default input: \"input { stdin { type => stdin } }\" and if no output is specified, then the following is used as the default output: \"output { stdout { codec => rubydebug } }\" If you wish to use both defaults, please use the empty string for the '-e' flag. nil --log.level LEVEL Set the log level for logstash. Possible values are: fatal error warn info debug trace (default: \"info\") -l, --path.logs PATH Write logstash internal logs to the given file. Without this flag, logstash will emit logs to standard output. /usr/share/logstash/logs -t, --config.test_and_exit Check configuration for valid syntax and then exit. false --config.reload.interval RELOAD_INTERVAL How frequently to poll the configuration location for changes, in seconds 3000000000 --http.host HTTP_HOST Web API binding host 127.0.0.1 --http.port HTTP_PORT Web API http port 9600..9700 --log.format FORMAT Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text (using Ruby's Object#inspect) plain --path.settings SETTINGS_DIR Directory containing logstash.yml file. This can also be set through the LS_SETTINGS_DIR environment variable /usr/share/logstash/config -p, --path.plugins PATH A path of where to find plugins. This flag can be given multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: 'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin. [] --path.data PATH This should point to a writable directory. Logstash will use this directory whenever it needs to store data. Plugins will also have access to this path. /usr/share/logstash/data -u, --pipeline.batch.delay DELAY_IN_MS When creating pipeline batches, how long to wait while polling for the next event. 50 --pipeline.id ID Sets the ID of the pipeline. main -b, --pipeline.batch.size SIZE Size of batches the pipeline is to work in. 125 -V, --version Emit the version of logstash and its friends, then exit. -M, --modules.variable MODULES_VARIABLE Load variables for module template. Multiple instances of '-M' or '--modules.variable' are supported. Ignored if '--modules' flag is not used. Should be in the format of '-M \"MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE\"' as in '-M \"example.var.filter.mutate.fieldname=fieldvalue\"' --modules MODULES Load Logstash modules. Modules can be defined using multiple instances '--modules module1 --modules module2', or comma-separated syntax '--modules=module1,module2' Cannot be used in conjunction with '-e' or '-f' Use of '--modules' will override modules declared in the 'logstash.yml' file. --setup Load index template into Elasticsearch, and saved searches, index-pattern, visualizations, and dashboards into Kibana when running modules. false -w, --pipeline.workers COUNT Sets the number of pipeline workers to run. 20 --config.debug Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled). WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false --pipeline.unsafe_shutdown Force logstash to exit during shutdown even if there are still inflight events in memory. By default, logstash will refuse to quit until all received events have been pushed to the outputs. false --java-execution Use Java execution engine. true -i, --interactive SHELL Drop to shell instead of running as normal. Valid shells are \"irb\" and \"pry\" --verbose Set the log level to info. 三. Docker镜像 docker pull docker.elastic.co/logstash/logstash:7.4.0 docker pull logstash:7.4.0 镜像中各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. /usr/share/logstash bin Binary scripts, including logstash to start Logstash and logstash-plugin to install plugins /usr/share/logstash/bin settings Configuration files, including logstash.yml and jvm.options /usr/share/logstash/config path.settings conf Logstash pipeline configuration files /usr/share/logstash/pipeline path.config plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. /usr/share/logstash/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. /usr/share/logstash/data path.data Note：基于该镜像启动的容器，日志是直接输出到控制台的，无法直接输出到日志文件中 docker镜像是基于.tar.gz格式的二进制包创建的 将pipeline文件挂载到/usr/share/logstash/pipeline/下启动 docker run --rm -it \\ -v ./test.conf:/usr/share/logstash/pipeline/test.conf \\ docker.elastic.co/logstash/logstash:7.4.0 默认pipeline文件：/usr/share/logstash/pipeline/logstash.conf input { beats { port => 5044 } } output { stdout { codec => rubydebug } } 也就是说如果不配置挂载pipeline文件就直接启动容器，logstash将启动一个最小化的pipeline：Beat Input ---> Stdout Output 可通过设置环境变量的形式配置logstash。 docker run --rm -it -e PIPELINE_WORKERS:2 docker.elastic.co/logstash/logstash:7.4.0。例如以下环境变量对应的logstash配置 Environment Variable Logstash Setting PIPELINE_WORKERS pipeline.workers LOG_LEVEL log.level XPACK_MONITORING_ENABLED xpack.monitoring.enabled logstash docker 镜像中的默认配置 http.host 0.0.0.0 xpack.monitoring.elasticsearch.hosts http://elasticsearch:9200 四. 配置 Logstash配置文件中配置项的格式是基于YAML语法，例如： pipeline: batch: size: 125 delay: 50 也可以使用平级格式 pipeline.batch.size: 125 pipeline.batch.delay: 50 配置项的值可以引用系统级别的环境变量 pipeline.batch.size: ${BATCH_SIZE} pipeline.batch.delay: ${BATCH_DELAY:50} node.name: \"node_${LS_NODE_NAME}\" path.queue: \"/tmp/${QUEUE_DIR:queue}\" 如果设置多个自定义的配置项时，推荐使用以下格式 modules: - name: MODULE_NAME1 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE var.PLUGIN_TYPE2.PLUGIN_NAME2.KEY1: VALUE var.PLUGIN_TYPE3.PLUGIN_NAME3.KEY1: VALUE - name: MODULE_NAME2 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE 常见的logstash配置 Setting Description Default value node.name A descriptive name for the node. Machine’s hostname path.data The directory that Logstash and its plugins use for any persistent needs. LOGSTASH_HOME/data pipeline.id The ID of the pipeline. main pipeline.java_execution Use the Java execution engine. true pipeline.workers The number of workers that will, in parallel, execute the filter and output stages of the pipeline. If you find that events are backing up, or that the CPU is not saturated, consider increasing this number to better utilize machine processing power. Number of the host’s CPU cores pipeline.batch.size The maximum number of events an individual worker thread will collect from inputs before attempting to execute its filters and outputs. Larger batch sizes are generally more efficient, but come at the cost of increased memory overhead. You may need to increase JVM heap space in the jvm.options config file. See Logstash Configuration Files for more info. 125 pipeline.batch.delay When creating pipeline event batches, how long in milliseconds to wait for each event before dispatching an undersized batch to pipeline workers. 50 pipeline.unsafe_shutdown When set to true, forces Logstash to exit during shutdown even if there are still inflight events in memory. By default, Logstash will refuse to quit until all received events have been pushed to the outputs. Enabling this option can lead to data loss during shutdown. false pipeline.plugin_classloaders (Beta) Load Java plugins in independent classloaders to isolate their dependencies. false path.config The path to the Logstash config for the main pipeline. If you specify a directory or wildcard, config files are read from the directory in alphabetical order. Platform-specific. See Logstash Directory Layout. config.string A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as the config file. None config.test_and_exit When set to true, checks that the configuration is valid and then exits. Note that grok patterns are not checked for correctness with this setting. Logstash can read multiple config files from a directory. If you combine this setting with log.level: debug, Logstash will log the combined config file, annotating each config block with the source file it came from. false config.reload.automatic When set to true, periodically checks if the configuration has changed and reloads the configuration whenever it is changed. This can also be triggered manually through the SIGHUP signal. false config.reload.interval How often in seconds Logstash checks the config files for changes. 3s config.debug When set to true, shows the fully compiled configuration as a debug log message. You must also set log.level: debug. WARNING: The log message will include any password options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false config.support_escapes When set to true, quoted strings will process the following escape sequences: \\n becomes a literal newline (ASCII 10). \\r becomes a literal carriage return (ASCII 13). \\t becomes a literal tab (ASCII 9). \\\\ becomes a literal backslash \\. \\\" becomes a literal double quotation mark. \\' becomes a literal quotation mark. false modules When configured, modules must be in the nested YAML structure described above this table. None queue.type The internal queuing model to use for event buffering. Specify memory for legacy in-memory based queuing, or persisted for disk-based ACKed queueing (persistent queues). memory path.queue The directory path where the data files will be stored when persistent queues are enabled (queue.type: persisted). path.data/queue queue.page_capacity The size of the page data files used when persistent queues are enabled (queue.type: persisted). The queue data consists of append-only data files separated into pages. 64mb queue.max_events The maximum number of unread events in the queue when persistent queues are enabled (queue.type: persisted). 0 (unlimited) queue.max_bytes The total capacity of the queue in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both queue.max_events and queue.max_bytes are specified, Logstash uses whichever criteria is reached first. 1024mb (1g) queue.checkpoint.acks The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (queue.type: persisted). Specify queue.checkpoint.acks: 0 to set this value to unlimited. 1024 queue.checkpoint.writes The maximum number of written events before forcing a checkpoint when persistent queues are enabled (queue.type: persisted). Specify queue.checkpoint.writes: 0 to set this value to unlimited. 1024 queue.checkpoint.retry When enabled, Logstash will retry once per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. false queue.drain When enabled, Logstash waits until the persistent queue is drained before shutting down. false dead_letter_queue.enable Flag to instruct Logstash to enable the DLQ feature supported by plugins. false dead_letter_queue.max_bytes The maximum size of each dead letter queue. Entries will be dropped if they would increase the size of the dead letter queue beyond this setting. 1024mb path.dead_letter_queue The directory path where the data files will be stored for the dead-letter queue. path.data/dead_letter_queue http.host The bind address for the metrics REST endpoint. \"127.0.0.1\" http.port The bind port for the metrics REST endpoint. 9600 log.level 设置Logstash日志输出级别 可用值：fatal error warn info debug trace info log.format The log format. Set to json to log in JSON format, or plain to use Object#.inspect. plain path.logs The directory where Logstash will write its log to. LOGSTASH_HOME/logs path.plugins Where to find custom plugins. You can specify this setting multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: PATH/logstash/TYPE/NAME.rb where TYPE is inputs, filters, outputs, or codecs, and NAME is the name of the plugin. Platform-specific. See Logstash Directory Layout. 五. Pipeline 1. 配置项结构 Logstash Pipeline文件的配置项分为三个部分： input{ input插件{ 插件配置项 } } filter{ filter插件{ 插件配置项 } } output{ output插件{ 插件配置项 } } Note: 如果在filter中添加了多种处理规则，则按照它的顺序一一处理，但是有一些插件并不是线程安全的。 如果在filter中指定了两个一样的的插件，这两个任务并不能保证准确的按顺序执行，因此官方也推荐避免在filter中重复使用插件。 2. 插件的条件控制 官方文档：https://www.elastic.co/guide/en/logstash/6.7/event-dependent-configuration.html#conditionals 有时需要在特定条件下过滤或输出事件。为此，您可以使用条件（conditional）来决定filter和output处理特定的事件。比如在elk系统中想要添加一个type类型的关键字来根据不同的条件赋值，最后好做统计。条件语支持if，else if和else语句并且可以嵌套。 条件语法 if EXPRESSION { ... } else if EXPRESSION { ... } else { ... } 操作符 比较操作： 相等: ==, !=, , >, , >= 正则: `=~(匹配正则), !~(不匹配正则) 包含:in(包含), not in(不包含) 布尔操作： and(与), or(或), nand(非与), xor(非或) 一元运算符： !(取反) ()(复合表达式), !()(对复合表达式结果取反) 示例 filter { if [foo] in [foobar] { mutate { add_tag => \"field in field\" } } if [foo] in \"foo\" { mutate { add_tag => \"field in string\" } } if \"hello\" in [greeting] { mutate { add_tag => \"string in field\" } } if [foo] in [\"hello\", \"world\", \"foo\"] { mutate { add_tag => \"field in list\" } } if [missing] in [alsomissing] { mutate { add_tag => \"shouldnotexist\" } } if !(\"foo\" in [\"hello\", \"world\"]) { mutate { add_tag => \"shouldexist\" } } if [message] =~ /\\w+\\s+\\/\\w+(\\/learner\\/course\\/)/ { mutate { add_field => { \"learner_type\" => \"course\" } } } mutate { add_field => { \"show\" => \"This data will be in the output\" } } mutate { add_field => { \"[@metadata][test]\" => \"Hello\" } } mutate { add_field => { \"[@metadata][no_show]\" => \"This data will not be in the output\" } } } output { if \"_grokparsefailure\" not in [tags] { elasticsearch { ... } } if [@metadata][test] == \"Hello\" { stdout { codec => rubydebug } } if [loglevel] == \"ERROR\" and [deployment] == \"production\" { pagerduty { ... } } } 注意： 如果if[foo] in \"String\"在执行这样的语句时无法把该字段值转化成String类型。所以最好要加field if exist判断 if [\"foo\"] { mutate { add_field => \"bar\" => \"%{foo}\" } } 3. 引用event中的字段 直接引用字段，使用[],嵌套字段使用多层[][]即可 { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- if [b] =~ \"2\" { .......... } if [c][c1] == \"3\" { ........... } 在字符串中以sprintf方式引用,使用%{} { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- add_field => { \"test\" => \"test: %{b}\" } add_field => { \"test\" => \"test: %{[c][c1]}\" } 六. Input插件 插件一览表 Plugin Description Github repository azure_event_hubs Receives events from Azure Event Hubs azure_event_hubs beats Receives events from the Elastic Beats framework logstash-input-beats cloudwatch Pulls events from the Amazon Web Services CloudWatch API logstash-input-cloudwatch couchdb_changes Streams events from CouchDB’s _changes URI logstash-input-couchdb_changes dead_letter_queue read events from Logstash’s dead letter queue logstash-input-dead_letter_queue elasticsearch Reads query results from an Elasticsearch cluster logstash-input-elasticsearch exec Captures the output of a shell command as an event logstash-input-exec file Streams events from files logstash-input-file ganglia Reads Ganglia packets over UDP logstash-input-ganglia gelf Reads GELF-format messages from Graylog2 as events logstash-input-gelf generator Generates random log events for test purposes logstash-input-generator github Reads events from a GitHub webhook logstash-input-github google_cloud_storage Extract events from files in a Google Cloud Storage bucket logstash-input-google_cloud_storage google_pubsub Consume events from a Google Cloud PubSub service logstash-input-google_pubsub graphite Reads metrics from the graphite tool logstash-input-graphite heartbeat Generates heartbeat events for testing logstash-input-heartbeat http Receives events over HTTP or HTTPS logstash-input-http http_poller Decodes the output of an HTTP API into events logstash-input-http_poller imap Reads mail from an IMAP server logstash-input-imap irc Reads events from an IRC server logstash-input-irc java_generator Generates synthetic log events core plugin java_stdin Reads events from standard input core plugin jdbc Creates events from JDBC data logstash-input-jdbc jms Reads events from a Jms Broker logstash-input-jms jmx Retrieves metrics from remote Java applications over JMX logstash-input-jmx kafka Reads events from a Kafka topic logstash-input-kafka kinesis Receives events through an AWS Kinesis stream logstash-input-kinesis log4j Reads events over a TCP socket from a Log4j SocketAppender object logstash-input-log4j lumberjack Receives events using the Lumberjack protocl logstash-input-lumberjack meetup Captures the output of command line tools as an event logstash-input-meetup pipe Streams events from a long-running command pipe logstash-input-pipe puppet_facter Receives facts from a Puppet server logstash-input-puppet_facter rabbitmq Pulls events from a RabbitMQ exchange logstash-input-rabbitmq redis Reads events from a Redis instance logstash-input-redis relp Receives RELP events over a TCP socket logstash-input-relp rss Captures the output of command line tools as an event logstash-input-rss s3 Streams events from files in a S3 bucket logstash-input-s3 salesforce Creates events based on a Salesforce SOQL query logstash-input-salesforce snmp Polls network devices using Simple Network Management Protocol (SNMP) logstash-input-snmp snmptrap Creates events based on SNMP trap messages logstash-input-snmptrap sqlite Creates events based on rows in an SQLite database logstash-input-sqlite sqs Pulls events from an Amazon Web Services Simple Queue Service queue logstash-input-sqs stdin Reads events from standard input logstash-input-stdin stomp Creates events received with the STOMP protocol logstash-input-stomp syslog Reads syslog messages as events logstash-input-syslog tcp Reads events from a TCP socket logstash-input-tcp twitter Reads events from the Twitter Streaming API logstash-input-twitter udp Reads events over UDP logstash-input-udp unix Reads events over a UNIX socket logstash-input-unix varnishlog Reads from the varnish cache shared memory log logstash-input-varnishlog websocket Reads events from a websocket logstash-input-websocket wmi Creates events based on the results of a WMI query logstash-input-wmi xmpp Receives events over the XMPP/Jabber protocol logstash-input-xmpp 插件通用配置项 参数 参数值类型 必须 默认值 详解 add_field hash No {} 向事件添加字段。 codec codec No plain 用于输入数据的编解码器，在输入数据之前，输入编解码器是一种方便的解码方法，不需要在你的Logstash管道中使用单独的过滤器 enable_metric boolean No true 禁用或启用这个特定插件实例的指标日志，默认情况下，我们记录所有我们可以记录的指标，但是你可以禁用特定插件的指标集合。 id string No 向插件配置添加唯一的ID，如果没有指定ID，则Logstash将生成一个，强烈建议在配置中设置此ID，当你有两个或多个相同类型的插件时，这一点特别有用。例如，如果你有两个log4j输入，在本例中添加一个命名ID将有助于在使用监视API时监视Logstash。input { kafka { id => \"my_plugin_id\" }} tags array No 向事件添加任意数量的标记，这有助于以后的处理。 type string No 向该输入处理的所有事件添加type字段，类型主要用于过滤器激活，该type作为事件本身的一部分存储，因此你也可以使用该类型在Kibana中搜索它。如果你试图在已经拥有一个type的事件上设置一个type（例如，当你将事件从发送者发送到索引器时），那么新的输入将不会覆盖现有的type，发送方的type集在其生命周期中始终与该事件保持一致，甚至在发送到另一个Logstash服务器时也是如此。 七. Filter插件 插件一览表 Plugin Description Github repository aggregate Aggregates information from several events originating with a single task logstash-filter-aggregate alter Performs general alterations to fields that the mutate filter does not handle logstash-filter-alter bytes Parses string representations of computer storage sizes, such as \"123 MB\" or \"5.6gb\", into their numeric value in bytes logstash-filter-bytes cidr Checks IP addresses against a list of network blocks logstash-filter-cidr cipher Applies or removes a cipher to an event logstash-filter-cipher clone Duplicates events logstash-filter-clone csv Parses comma-separated value data into individual fields logstash-filter-csv date Parses dates from fields to use as the Logstash timestamp for an event logstash-filter-date de_dot Computationally expensive filter that removes dots from a field name logstash-filter-de_dot dissect Extracts unstructured event data into fields using delimiters logstash-filter-dissect dns Performs a standard or reverse DNS lookup logstash-filter-dns drop Drops all events logstash-filter-drop elapsed Calculates the elapsed time between a pair of events logstash-filter-elapsed elasticsearch Copies fields from previous log events in Elasticsearch to current events logstash-filter-elasticsearch environment Stores environment variables as metadata sub-fields logstash-filter-environment extractnumbers Extracts numbers from a string logstash-filter-extractnumbers fingerprint Fingerprints fields by replacing values with a consistent hash logstash-filter-fingerprint geoip Adds geographical information about an IP address logstash-filter-geoip grok Parses unstructured event data into fields logstash-filter-grok http Provides integration with external web services/REST APIs logstash-filter-http i18n Removes special characters from a field logstash-filter-i18n java_uuid Generates a UUID and adds it to each processed event core plugin jdbc_static Enriches events with data pre-loaded from a remote database logstash-filter-jdbc_static jdbc_streaming Enrich events with your database data logstash-filter-jdbc_streaming json Parses JSON events logstash-filter-json json_encode Serializes a field to JSON logstash-filter-json_encode kv Parses key-value pairs logstash-filter-kv memcached Provides integration with external data in Memcached logstash-filter-memcached metricize Takes complex events containing a number of metrics and splits these up into multiple events, each holding a single metric logstash-filter-metricize metrics Aggregates metrics logstash-filter-metrics mutate Performs mutations on fields logstash-filter-mutate prune Prunes event data based on a list of fields to blacklist or whitelist logstash-filter-prune range Checks that specified fields stay within given size or length limits logstash-filter-range ruby Executes arbitrary Ruby code logstash-filter-ruby sleep Sleeps for a specified time span logstash-filter-sleep split Splits multi-line messages into distinct events logstash-filter-split syslog_pri Parses the PRI (priority) field of a syslog message logstash-filter-syslog_pri threats_classifier Enriches security logs with information about the attacker’s intent logstash-filter-threats_classifier throttle Throttles the number of events logstash-filter-throttle tld Replaces the contents of the default message field with whatever you specify in the configuration logstash-filter-tld translate Replaces field contents based on a hash or YAML file logstash-filter-translate truncate Truncates fields longer than a given length logstash-filter-truncate urldecode Decodes URL-encoded fields logstash-filter-urldecode useragent Parses user agent strings into fields logstash-filter-useragent uuid Adds a UUID to events logstash-filter-uuid xml Parses XML into fields logstash-filter-xml 插件通用配置项 Setting Input type Required add_field hash No add_tag array No enable_metric boolean No id string No periodic_flush boolean No remove_field array No remove_tag array No 八. Output插件 插件一览表 Plugin Description Github repository boundary Sends annotations to Boundary based on Logstash events logstash-output-boundary circonus Sends annotations to Circonus based on Logstash events logstash-output-circonus cloudwatch Aggregates and sends metric data to AWS CloudWatch logstash-output-cloudwatch csv Writes events to disk in a delimited format logstash-output-csv datadog Sends events to DataDogHQ based on Logstash events logstash-output-datadog datadog_metrics Sends metrics to DataDogHQ based on Logstash events logstash-output-datadog_metrics elastic_app_search Sends events to the Elastic App Search solution logstash-output-elastic_app_search elasticsearch Stores logs in Elasticsearch logstash-output-elasticsearch email Sends email to a specified address when output is received logstash-output-email exec Runs a command for a matching event logstash-output-exec file Writes events to files on disk logstash-output-file ganglia Writes metrics to Ganglia’s gmond logstash-output-ganglia gelf Generates GELF formatted output for Graylog2 logstash-output-gelf google_bigquery Writes events to Google BigQuery logstash-output-google_bigquery google_cloud_storage Uploads log events to Google Cloud Storage logstash-output-google_cloud_storage google_pubsub Uploads log events to Google Cloud Pubsub logstash-output-google_pubsub graphite Writes metrics to Graphite logstash-output-graphite graphtastic Sends metric data on Windows logstash-output-graphtastic http Sends events to a generic HTTP or HTTPS endpoint logstash-output-http influxdb Writes metrics to InfluxDB logstash-output-influxdb irc Writes events to IRC logstash-output-irc java_sink Discards any events received core plugin java_stdout Prints events to the STDOUT of the shell core plugin juggernaut Pushes messages to the Juggernaut websockets server logstash-output-juggernaut kafka Writes events to a Kafka topic logstash-output-kafka librato Sends metrics, annotations, and alerts to Librato based on Logstash events logstash-output-librato loggly Ships logs to Loggly logstash-output-loggly lumberjack Sends events using the lumberjack protocol logstash-output-lumberjack metriccatcher Writes metrics to MetricCatcher logstash-output-metriccatcher mongodb Writes events to MongoDB logstash-output-mongodb nagios Sends passive check results to Nagios logstash-output-nagios nagios_nsca Sends passive check results to Nagios using the NSCA protocol logstash-output-nagios_nsca opentsdb Writes metrics to OpenTSDB logstash-output-opentsdb pagerduty Sends notifications based on preconfigured services and escalation policies logstash-output-pagerduty pipe Pipes events to another program’s standard input logstash-output-pipe rabbitmq Pushes events to a RabbitMQ exchange logstash-output-rabbitmq redis Sends events to a Redis queue using the RPUSH command logstash-output-redis redmine Creates tickets using the Redmine API logstash-output-redmine riak Writes events to the Riak distributed key/value store logstash-output-riak riemann Sends metrics to Riemann logstash-output-riemann s3 Sends Logstash events to the Amazon Simple Storage Service logstash-output-s3 sns Sends events to Amazon’s Simple Notification Service logstash-output-sns solr_http Stores and indexes logs in Solr logstash-output-solr_http sqs Pushes events to an Amazon Web Services Simple Queue Service queue logstash-output-sqs statsd Sends metrics using the statsd network daemon logstash-output-statsd stdout Prints events to the standard output logstash-output-stdout stomp Writes events using the STOMP protocol logstash-output-stomp syslog Sends events to a syslog server logstash-output-syslog tcp Writes events over a TCP socket logstash-output-tcp timber Sends events to the Timber.io logging service logstash-output-timber udp Sends events over UDP logstash-output-udp webhdfs Sends Logstash events to HDFS using the webhdfs REST API logstash-output-webhdfs websocket Publishes messages to a websocket logstash-output-websocket xmpp Posts events over XMPP logstash-output-xmpp zabbix Sends events to a Zabbix server logstash-output-zabbix 插件通用配置项 Setting Input type Required codec codec No enable_metric boolean No id string No 九. Codec插件 插件一览表 Plugin Description Github repository avro Reads serialized Avro records as Logstash events logstash-codec-avro cef Reads the ArcSight Common Event Format (CEF). logstash-codec-cef cloudfront Reads AWS CloudFront reports logstash-codec-cloudfront cloudtrail Reads AWS CloudTrail log files logstash-codec-cloudtrail collectd Reads events from the collectd binary protocol using UDP. logstash-codec-collectd dots Sends 1 dot per event to stdout for performance tracking logstash-codec-dots edn Reads EDN format data logstash-codec-edn edn_lines Reads newline-delimited EDN format data logstash-codec-edn_lines es_bulk Reads the Elasticsearch bulk format into separate events, along with metadata logstash-codec-es_bulk fluent Reads the fluentd msgpack schema logstash-codec-fluent graphite Reads graphite formatted lines logstash-codec-graphite gzip_lines Reads gzip encoded content logstash-codec-gzip_lines jdots Renders each processed event as a dot core plugin java_line Encodes and decodes line-oriented text data core plugin java_plain Processes text data with no delimiters between events core plugin json Reads JSON formatted content, creating one event per element in a JSON array logstash-codec-json json_lines Reads newline-delimited JSON logstash-codec-json_lines line Reads line-oriented text data logstash-codec-line msgpack Reads MessagePack encoded content logstash-codec-msgpack multiline Merges multiline messages into a single event logstash-codec-multiline netflow Reads Netflow v5 and Netflow v9 data logstash-codec-netflow nmap Reads Nmap data in XML format logstash-codec-nmap plain Reads plaintext with no delimiting between events logstash-codec-plain protobuf Reads protobuf messages and converts to Logstash Events logstash-codec-protobuf rubydebug Applies the Ruby Awesome Print library to Logstash events logstash-codec-rubydebug 十. 插件管理 Logstash 插件是使用 Ruby开发的，Logstash 从很早的1.5.0+版开始，其插件模块和核心模块便分开维护，其插件使用的是 RubyGems包管理器来管理维护。所以 Logstash插件本质上就是自包含的RubyGems。 RubyGems（简称 gems）是一个用于对 Ruby组件进行打包的 Ruby 打包系统。 它提供一个分发 Ruby 程序和库的标准格式，还提供一个管理程序包安装的工具。 插件的名字格式：logstash-{input/output/filter}-插件名 示例：filter中的date插件：logstash-filter-date 1. 安装插件 #以安装dissect插件为例 /usr/share/logstash/bin/logstash-plugin install 插件名 #参数详解： --path.plugins 指定安装路径 2. 查看已安装的插件 /usr/share/logstash/bin/logstash-plugin list #参数详解： --verbose 查看插件的版本 --verbose 查看组（input, filter, codec, output）下面的所有插件。例如查看filter下的所有插件 3. 更新插件 #更新某个插件 /usr/share/logstash/bin/logstash-plugin update 插件名 #更新全部插件 /usr/share/logstash/bin/logstash-plugin update 4. 卸载插件 /usr/share/logstash/bin/logstash-plugin remove 插件名 5. 给插件管理器设置代理 export HTTP_PROXY=http://127.0.0.1:3128 6. 修改插件仓库地址 Logstash插件默认仓库地址是：http://rubygems.org 有一些开源的插件仓库： Geminabox：https://github.com/geminabox/geminabox Gemirro：https://github.com/PierreRambaud/gemirro Gemfury：https://gemfury.com/ Artifactory：http://www.jfrog.com/open-source/ 编辑/usr/share/logstash/Gemfile，将source \"https://rubygems.org\"改为source \"https://my.private.repository\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-12 18:06:09 "},"origin/logstash-采集MySQL慢查询日志到Elasticsearch.html":{"url":"origin/logstash-采集MySQL慢查询日志到Elasticsearch.html","title":"Pipeline示例--采集MySQL慢查询日志到Elasticsearch","keywords":"","body":"Logstash采集MySQL慢查询日志到Elasticsearch 一、原始日志数据 ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- /opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with: Tcp port: 0 Unix socket: /opt/logs/mysql/mysql.sock Time Id Command Argument /opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with: Tcp port: 3306 Unix socket: /opt/logs/mysql/mysql.sock Time Id Command Argument ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-08-20T05:08:37.928071Z # User@Host: root[root] @ [172.17.89.18] Id: 1038 # Query_time: 2.100371 Lock_time: 1.263743 Rows_sent: 0 Rows_examined: 0 SET timestamp=1566277715; -- Dumping database structure for curiouser_alert_rule DROP DATABASE IF EXISTS `curiouser_alert_rule`; ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-09-02T02:56:05.166482Z # User@Host: root[root] @ [172.17.88.142] Id: 38433 # Query_time: 0.526962 Lock_time: 0.000101 Rows_sent: 1000 Rows_examined: 1000 SET timestamp=1567392964; /* ApplicationName=DataGrip 2019.2.1 */ select * from curiouser_notification.notif_send_records order by id desc limit 1000; ---------------------------------------标识分隔符（源文件中不存在）---------------------------------------------------- # Time: 2019-09-24T04:52:37.816164Z # User@Host: root[root] @ [172.17.0.115] Id: 88405 # Query_time: 0.622215 Lock_time: 0.000149 Rows_sent: 0 Rows_examined: 1 SET timestamp=1569300757; UPDATE xxl_job_registry SET `update_time` = NOW() WHERE `registry_group` = 'EXECUTOR' AND `registry_key` = 'metadata' AND `registry_value` = '192.168.215.94:9999'; ---------------------------------------标识分隔符（源文件中不存在）-------------------------------------------------- # Time: 2019-09-02T04:45:00.840024Z # User@Host: root[root] @ [172.17.0.113] Id: 38663 # Query_time: 0.557195 Lock_time: 0.000000 Rows_sent: 0 Rows_examined: 0 use curiouser_alert_rule; SET timestamp=1567399500; commit; Note：只摘录了几种典型格式的日志 二、Pipeline Note：Input插件将指定分割的多行数据变成一行放到一个Event的message中供filter插件处理 input { file{ path => \"/root/logs/mysql-log/test.log\" start_position => \"beginning\" codec => multiline { # 以\"# Time:\"为分隔符，中间的所有多行内容归为一行并填充到Event时间中 pattern => \"^# Time:\" negate => true what => \"previous\" # 指定最多读取多少行，默认500行（以防执行初始数据库数据sql语句超过默认行） max_lines => 20000 } } } filter { grok { # 在使用codec/multiline搭配使用的时候，需要注意，grok和普通正则一样默认是不支持匹配回车换行的。就像你需要=～//m一样也需要单独指定，具体写法是在表达式开始位置加(?m)标记 match => { \"message\" => \"(?m)^# Time:.*\\s+#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n\\s*(?(?\\w+)\\b.*)$\" } # 对于能匹配上面Grok正则的message就删除掉，不能匹配会原始保留 remove_field => [ \"message\" ] } #mutate { # gsub => [ \"sql\", \"\\n# Time: \\d+\\s+\\d+:\\d+:\\d+\", \"\" ] #} date { match => [ \"timestamp\", \"UNIX\" ] remove_field => [ \"timestamp\" ] } } output { #stdout { } elasticsearch { id => \"logstash_mysqlslowsql\" hosts => [\"localhost:9200\"] index=>\"mysql-test-log-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"elastic\" password => \"elastic\" } #file{ # path => \"/root/logs/mysql-log/test-out.log\" #} } # ---------------------------------如果以\"^# User@Host:“为分隔符时的Grok正则表达式------------------------------ match => { \"message\" => \"(?m)^# User@Host: %{USER:User}\\[[^\\]]+\\] @ (?:(?\\S*) )?\\[(?:%{IP:Client_IP})?\\]\\s.*# Query_time: %{NUMBER:Query_Time:float}\\s+Lock_time: %{NUMBER:Lock_Time:float}\\s+Rows_sent: %{NUMBER:Rows_Sent:int}\\s+Rows_examined: %{NUMBER:Rows_Examined:int}\\s*(?:use %{DATA:Database};\\s*)?SET timestamp=%{NUMBER:timestamp};\\s*(?(?\\w+)\\s+.*)\\n# Time:.*$\" } match => { \"(?m)^#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n-{0,2}\\s*(?(?\\w+)\\b.*;)\\s*(?:\\n#\\s+Time)?.*$\" } 三、日志数据经logstash处理后的数据格式 {\"message\":\"/opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with:\\nTcp port: 0 Unix socket: /opt/logs/mysql/mysql.sock\\nTime Id Command Argument\\n/opt/app/mysql/bin/mysqld, Version: 8.0.15 (MySQL Community Server - GPL). started with:\\nTcp port: 3306 Unix socket: /opt/logs/mysql/mysql.sock\\nTime Id Command Argument\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.402Z\",\"@version\":\"1\"} {\"message\":\"# Time: 2019-08-20T05:08:37.928071Z\\n# User@Host: root[root] @ [172.17.89.18] Id: 1038\\n# Query_time: 2.100371 Lock_time: 1.263743 Rows_sent: 0 Rows_examined: 0\\nSET timestamp=1566277715;\\n-- Dumping database structure for curiouser_alert_rule\\nDROP DATABASE IF EXISTS `curiouser_alert_rule`;\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.444Z\",\"@version\":\"1\"} {\"message\":\"# Time: 2019-09-02T02:56:05.166482Z\\n# User@Host: root[root] @ [172.17.88.142] Id: 38433\\n# Query_time: 0.526962 Lock_time: 0.000101 Rows_sent: 1000 Rows_examined: 1000\\nSET timestamp=1567392964;\\n/* ApplicationName=DataGrip 2019.2.1 */ select * from curiouser_notification.notif_send_records order by id desc limit 1000;\",\"host\":\"allinone.tools.curiouser.com\",\"path\":\"/root/logs/mysql-log/test.log\",\"tags\":[\"multiline\",\"_grokparsefailure\"],\"@timestamp\":\"2019-10-11T03:48:55.447Z\",\"@version\":\"1\"} {\"host\":\"allinone.tools.curiouser.com\",\"row_id\":88405,\"tags\":[\"multiline\"],\"clientip\":\"172.17.0.115\",\"@timestamp\":\"2019-09-24T04:52:37.000Z\",\"@version\":\"1\",\"user\":\"root\",\"sql\":\"UPDATE xxl_job_registry\\n SET `update_time` = NOW()\\n WHERE `registry_group` = 'EXECUTOR'\\n AND `registry_key` = 'metadata'\\n AND `registry_value` = '192.168.215.94:9999';\",\"Query_time\":0.622215,\"path\":\"/root/logs/mysql-log/test.log\",\"action\":\"UPDATE\",\"Row_sent\":0,\"lock_time\":1.49E-4,\"Rows_examined\":1} {\"host\":\"allinone.tools.curiouser.com\",\"row_id\":38663,\"tags\":[\"multiline\"],\"clientip\":\"172.17.0.113\",\"@timestamp\":\"2019-09-02T04:45:00.000Z\",\"@version\":\"1\",\"user\":\"root\",\"sql\":\"commit;\",\"database\":\"curiouser_alert_rule\",\"Query_time\":0.557195,\"path\":\"/root/logs/mysql-log/test.log\",\"action\":\"commit\",\"Row_sent\":0,\"lock_time\":0.0,\"Rows_examined\":0} 四、日志数据在Elasticsearch中存储的结构 { \"_index\": \"mysql-test-log-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"klPvuG0BV8kuVZccHbmj\", \"_version\": 1, \"_score\": null, \"_source\": { \"host\": \"allinone.tools.curiouser.com\", \"row_id\": 88405, \"tags\": [ \"multiline\" ], \"clientip\": \"172.17.0.115\", \"@timestamp\": \"2019-09-24T04:52:37.000Z\", \"@version\": \"1\", \"user\": \"root\", \"sql\": \"UPDATE xxl_job_registry\\n SET `update_time` = NOW()\\n WHERE `registry_group` = 'EXECUTOR'\\n AND `registry_key` = 'metadata'\\n AND `registry_value` = '192.168.215.94:9999';\", \"Query_time\": 0.622215, \"path\": \"/root/logs/mysql-log/test.log\", \"action\": \"UPDATE\", \"Row_sent\": 0, \"lock_time\": 0.000149, \"Rows_examined\": 1 }, \"fields\": { \"@timestamp\": [ \"2019-09-24T04:52:37.000Z\" ] }, \"sort\": [ 1569300757000 ] } 五、问题： 有几种特殊格式的日志能采集到，但无法格式解析(时间戳会是以采集时间为准) 日志文件中最后一条最新的日志在logstash退出时才进行了收集 附录： 1、filter中grok插件的正则 ^ : 匹配输入字符串的开始位置 \\s : 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \\f\\n\\r\\t\\v] \\S : 匹配任何可见字符。等价于 \\f\\n\\r\\t\\v \\n : 匹配一个换行符。等价于\\x0a和\\cJ \\b : 匹配一个单词边界，也就是指单词和空格间的位置（即正则表达式的“匹配”有两种概念，一种是匹配字符，一种是匹配位置，这里的\\b就是匹配位置的）。例如，“er\\b”可以匹配“never”中的“er”，但不能匹配“verb”中的“er” \\w : 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的\"单词\"字符使用Unicode字符集 默认的正则匹配模式：%{NUMBER:row_id:int} 匹配模式:字段名:数值类型 自定义的正则匹配模式： (?the pattern here) 2、MySQL开启慢查询日志 # slow_query_log 慢查询开启状态 # slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） # long_query_time 查询超过多少秒才记录 # 查看慢查询相关参数 mysql> show variables like 'slow_query%'; mysql> show variables like 'long_query_time'; # 全局变量设置 mysql> set global slow_query_log='ON'; mysql> set global slow_query_log_file='/usr/local/mysql/data/slow.log'; mysql> set global long_query_time=1; # 修改配置文件my.cnf [mysqld] slow_query_log = ON slow_query_log_file = /usr/local/mysql/data/slow.log long_query_time = 1 # 使用mysqldumpslow和mysqlsla分析mysql慢查询日志 # mysqldumpslow 慢日志分析工具 -s 按照那种方式排序 c：访问计数 l：锁定时间 r:返回记录 al：平均锁定时间 ar：平均访问记录数 at：平均查询时间 -t 是top n的意思，返回多少条数据。-g 可以跟上正则匹配模式，大小写不敏感。 # 得到返回记录最多的20个sql mysqldumpslow -s r -t 20 sqlslow.log # 得到平均访问次数最多的20条sql mysqldumpslow -s ar -t 20 sqlslow.log # 得到平均访问次数最多,并且里面含有ttt字符的20条sql mysqldumpslow -s ar -t 20 -g \"ttt\" sqldlow.log # 如果出现 -bash: mysqldumpslow: command not found 错误，请执行\"ln -s /usr/local/mysql/bin/mysqldumpslow /usr/bin\" # 如果出现如下错误，Died at /usr/bin/mysqldumpslow line 161, <> chunk 405659.说明你要分析的sql日志太大了，请拆分后再分析 5.5版本慢查询日志 # Time: 180810 8:45:12 # User@Host: select[select] @ [10.63.253.59] # Query_time: 1.064555 Lock_time: 0.000054 Rows_sent: 1 Rows_examined: 319707 SET timestamp=1533861912; SELECT COUNT(*) FROM hs_forum_thread t WHERE t.`fid`='50' AND t.`displayorder`>='0'; 5.6版本慢查询日志 # Time: 160928 18:36:08 # User@Host: root[root] @ localhost [] Id: 4922 # Query_time: 5.207662 Lock_time: 0.000085 Rows_sent: 1 Rows_examined: 526068 use db_name; SET timestamp=1475058968; select count(*) from redeem_item_consume where id 5.7版本慢查询日志 # Time: 2018-07-09T10:04:14.666231Z # User@Host: bbs_code[bbs_code] @ [10.82.9.220] Id: 9304381 # Query_time: 5.274805 Lock_time: 0.000052 Rows_sent: 0 Rows_examined: 2 SET timestamp=1531130654; SELECT * FROM pre_common_session WHERE sid='Ba1cSC' OR lastactivity 慢查询日志异同点： 每个版本的Time字段格式都不一样 相较于5.6、5.7版本，5.5版本少了Id字段 use db语句不是每条慢日志都有的 可能会出现像下边这样的情况，慢查询块# Time：下可能跟了多个慢查询语句 # Time: 160918 2:00:03 # User@Host: dba_monitor[dba_monitor] @ [10.63.144.82] Id: 968 # Query_time: 0.007479 Lock_time: 0.000181 Rows_sent: 172 Rows_examined: 344 SET timestamp=1474135203; SELECT table_schema as 'DB',table_name as 'TABLE',CONCAT(ROUND(( data_length + index_length ) / ( 1024 * 1024 *1024 ), 2), '') as 'TOTAL',TABLE_COMMENT FROM information_schema.TABLES ORDER BY data_length + index_length DESC; # User@Host: dba_monitor[dba_monitor] @ [10.63.144.82] Id: 969 # Query_time: 0.003303 Lock_time: 0.000395 Rows_sent: 233 Rows_examined: 233 SET timestamp=1474135203; select TABLE_SCHEMA,TABLE_NAME,COLUMN_NAME,ORDINAL_POSITION,COLUMN_TYPE,ifnull(COLUMN_COMMENT,0) from COLUMNS where table_schema not in ('mysql','information_schema','performance_schema','test'); 参考链接 https://my.oschina.net/lics/blog/916618 https://blog.51cto.com/fengwan/1758920 https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-16 09:21:54 "},"origin/elasticsearch--_cat-API.html":{"url":"origin/elasticsearch--_cat-API.html","title":"_cat","keywords":"","body":"Elasticsearch _cat APIs 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html 查看_cat API支持的所有Endpoint GET /_cat curl -XGET http://127.0.0.1:9200/_cat /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates 查询Endpoint参数 GET /_cat/health?help curl -XGET \"http://127.0.0.1:9200/_cat/health?help\" 　 　 # 参数全称 | 参数缩写 | 参数详解 ---------------------------------------------------------------------------------------------------- epoch | t,time | seconds since 1970-01-01 00:00:00 timestamp | ts,hms,hhmmss | time in HH:MM:SS cluster | cl | cluster name status | st | health status node.total | nt,nodeTotal | total number of nodes node.data | nd,nodeData | number of nodes that can store data shards | t,sh,shards.total,shardsTotal | total number of shards pri | p,shards.primary,shardsPrimary | number of primary shards relo | r,shards.relocating,shardsRelocating | number of relocating nodes init | i,shards.initializing,shardsInitializing | number of initializing nodes unassign | u,shards.unassigned,shardsUnassigned | number of unassigned shards pending_tasks | pt,pendingTasks | number of pending tasks max_task_wait_time | mtwt,maxTaskWaitTime | wait time of longest task pending active_shards_percent | asp,activeShardsPercent | active number of shards in percent 使用参数控制查询条件 GET /_cat/health?h=st,t #带表头 GET /_cat/health?v&h=st,t 控制查询的输出排序 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date #查询出来的Index将会以store.size的大小降序输出。只输出Index名，store.size大小，创建时间戳 curl -XGET \"http://elasticsearch-service.logger.svc:9200/_cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date\" 控制查询的输出格式 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date&format=yaml yaml - index: \"test-test-2019.05.21\" store.size: \"4.1gb\" creation.date: \"1558432572904\" - index: \".monitoring-es-7-2019.06.17\" store.size: \"1.2gb\" creation.date: \"1560729605158\" json [ { \"index\" : \"test-test-2019.05.21\", \"store.size\" : \"4.1gb\", \"creation.date\" : \"1558432572904\" }, { \"index\" : \".monitoring-es-7-2019.06.17\", \"store.size\" : \"1.2gb\", \"creation.date\" : \"1560729605158\" } ] text (default) index store.size creation.date test-test-2019.05.21 4.1gb 1558432572904 .monitoring-es-7-2019.06.17 1.2gb 1560729605158 cbor smile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-08 18:24:05 "},"origin/elasticsearch-7.1的xpack权限控制.html":{"url":"origin/elasticsearch-7.1的xpack权限控制.html","title":"Xpack","keywords":"","body":"一、Context 之前ELK套装安装X-Pack的安全功能时，只有安装30天的试用许可证时间，以允许访问所有功能。 当许可证到期时，X-Pack将以降级模式运行。可以购买订阅以继续使用X-Pack组件的全部功能（https://www.elastic.co/subscriptions）。但是,最近官方从6.8.0和7.1.0开始ELK开始免费提供安全功能. 本次实验,所有ELK组件版本均为7.1.0,以容器单节点运行 二. Elasticsearch开启Xpack elasticsearch的容器化部署参考笔记: ElasticSearch的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数 xpack.monitoring.collection.enabled(开启自我监控): true path.repo(设置snapshot存储仓库的路径): /usr/share/elasticsearch/snapshots-repository discovery.type(设置当前节点为单节点模式): single-node cluster.name(设置elasticsearch的集群名): curiouser bootstrap.memory_lock: 'true' TZ(设置时区): Asia/Shanghai ES_JAVA_OPTS(设置elasticsearch的JVM堆栈大小): '-Xms1g -Xmx2g' ELASTIC_USERNAME: \"kibana\" ELASTIC_PASSWORD: \"kibana\" xpack.security.enabled: 'true' xpack.security.transport.ssl.enabled: \"true\" xpack.security.transport.ssl.verification_mode: \"certificate\" xpack.security.transport.ssl.keystore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.transport.ssl.truststore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.http.ssl.enabled: \"false\" 查看elasticsearch是否开启xpack的安全验证 curl -XGET 'localhost:9200/_cat/health?v&pretty' # curl -XGET \"http://127.0.0.1:9200/_cat/health?v&pretty\" # 使用上述命令会返回401,提示未授权验证,使用以下命令进行安全验证地访问 curl --user kibana:****kibana用户的密码**** -XGET 'localhost:9200/_cat/health?v&pretty' 三、Kibana开启Xpack kibana的容器化部署详见笔记: Kibana的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数: ELASTICSEARCH_USERNAME: kibana用户 ELASTICSEARCH_PASSWORD: kibana用户的随机密码 TZ(设置时区): Asia/Shanghai 镜像中默认指定的elasticsearch地址为:http://elasticsearch:9200,刚好在open shift中部署的elasticsearch的svc名为\"elasticsearch\",它的访问方式为:http://elasticsearch:9200或者http://elasticsearch.命名空间.svc:9200 登录Kibana进行验证 使用elastic 超级用户进行登录，密码来自 setup-passwords 命令输出的结果 四、Logstash开启Xpack 配置logstash发送监控数据到elasticsearch xpack.monitoring.elasticsearch.hosts: \"http://elasticsearch:9200\" xpack.monitoring.enabled: \"true\" xpack.monitoring.elasticsearch.username: \"logstash_system\" xpack.monitoring.elasticsearch.password: \"***logstash_system用户的密码****\" 在kibana中查看logstash的监控数据 在kibana中创建logstash-pipeline角色,授予\"manage_index_template\",\"monitor\"的集群权限和\"write\",\"delete\",\"create_index\",\"manage_ilm\",\"manage\"的Index权限,然后绑定到logstash-pipeline用户上,用以创建Index并向其中写入数据 在pipeline的elasticsearch output插件中设置用户和密码 output{ elasticsearch{ hosts => \"elasticsearch:9200\" index => \"%{AppID}-%{+YYYY.MM.dd}\" user => \"logstash-pipeline\" password => \"****logstash-pipeline用户密码****\" } } 查看logstash的pipeline是否将数据写入的elasticsearch 附录：Kibana上的角色权限 Cluster相关的角色权限 角色权限 权限描述 all Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. create_snapshot Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. manage Builds on monitor and adds cluster operations that change values in the cluster. This includes snapshotting, updating settings, and rerouting. It also includes obtaining snapshot and restore status. This privilege does not include the ability to manage security. manage_ccr All cross-cluster replication operations related to managing follower indices and auto-follow patterns. It also includes the authority to grant the privileges necessary to manage follower indices and auto-follow patterns. This privilege is necessary only on clusters that contain follower indices. manage_data_frame_transforms All operations on index templates. manage_ilm All operations on index templates. manage_index_templates All operations on index templates. manage_ingest_pipelines All operations on ingest node pipelines. manage_ml All machine learning operations, such as creating and deleting datafeeds, jobs, and model snapshots.Note：Datafeeds that were created prior to version 6.2 or created when security features were disabled run as a system user with elevated privileges, including permission to read all indices. Newer datafeeds run with the security roles of the user who created or updated them. manage_pipeline All operations on ingest pipelines. manage_rollup All rollup operations, including creating, starting, stopping and deleting rollup jobs. manage_saml Enables the use of internal Elasticsearch APIs to initiate and manage SAML authentication on behalf of other users. manage_security All security-related operations such as CRUD operations on users and roles and cache clearing. manage_token All security-related operations on tokens that are generated by the Elasticsearch Token Service. manage_watcher All watcher operations, such as putting watches, executing, activate or acknowledging.Note：Watches that were created prior to version 6.1 or created when the security features were disabled run as a system user with elevated privileges, including permission to read and write all indices. Newer watches run with the security roles of the user who created or updated them. monitor All cluster read-only operations, like cluster health and state, hot threads, node info, node and cluster stats, and pending cluster tasks. monitor_data_frame_transforms All read-only operations related to data frames. monitor_ml All read-only machine learning operations, such as getting information about datafeeds, jobs, model snapshots, or results. monitor_rollup All read-only rollup operations, such as viewing the list of historical and currently running rollup jobs and their capabilities. monitor_watcher All read-only watcher operations, such as getting a watch and watcher stats. read_ccr All read-only cross-cluster replication operations, such as getting information about indices and metadata for leader indices in the cluster. It also includes the authority to check whether users have the appropriate privileges to follow leader indices. This privilege is necessary only on clusters that contain leader indices. read_ilm All read-only index lifecycle management operations, such as getting policies and checking the status of index lifecycle management transport_client All privileges necessary for a transport client to connect. Required by the remote cluster to enable Cross Cluster Search. Index相关的角色权限 角色权限 权限描述 all Any action on an index create Privilege to index documents. Also grants access to the update mapping action.NoteThis privilege does not restrict the index operation to the creation of documents but instead restricts API use to the index API. The index API allows a user to overwrite a previously indexed document. create_index Privilege to create an index. A create index request may contain aliases to be added to the index once created. In that case the request requires the manage privilege as well, on both the index and the aliases names. delete Privilege to delete documents. delete_index Privilege to delete an index. index Privilege to index and update documents. Also grants access to the update mapping action. manage All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate). manage_follow_index All actions that are required to manage the lifecycle of a follower index, which includes creating a follower index, closing it, and converting it to a regular index. This privilege is necessary only on clusters that contain follower indices. manage_ilm All index lifecycle management operations relating to managing the execution of policies of an index This includes operations like retrying policies, and removing a policy from an index. manage_leader_index All actions that are required to manage the lifecycle of a leader index, which includes forgetting a follower. This privilege is necessary only on clusters that contain leader indices. monitor All actions that are required for monitoring (recovery, segments info, index stats and status). read Read-only access to actions (count, explain, get, mget, get indexed scripts, more like this, multi percolate/search/termvector, percolate, scroll, clear_scroll, search, suggest, tv). read_cross_cluster Read-only access to the search action from a remote cluster. view_index_metadata Read-only access to index metadata (aliases, aliases exists, get index, exists, field mappings, mappings, search shards, type exists, validate, warmers, settings, ilm). This privilege is primarily available for use by Kibana users. write Privilege to perform all write operations to documents, which includes the permission to index, update, and delete documents as well as performing bulk operations. Also grants access to the update mapping action. 参考链接 https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html https://www.elastic.co/guide/en/elastic-stack-overview/7.1/get-started-logstash-user.html https://www.elastic.co/guide/en/logstash/current/ls-security.html https://www.elastic.co/guide/en/logstash/current/docker-config.html#docker-env-config Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/elasticSearch-索引的快照备份与恢复.html":{"url":"origin/elasticSearch-索引的快照备份与恢复.html","title":"Snapshots","keywords":"","body":"一、Context shared file system：NFS S3 HDFS 二、使用NFS作为快照仓库后端存储 1. 在es集群中的某一个节点创建NFS文件系统，ES集群节点进行挂载 yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"/data/es/Elastic-SnapShots 172.16.3.0/24(rw,sync,no_root_squash,no_subtree_check) \" >> /etc/exports ;\\ export -r ;\\ showmount -e 127.0.0.1 2. 集群其他节点挂载NFS共享目录 yum install nfs-utils -y ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"172.16.3.5:/data/es/Elastic-SnapShots /data/es/Elastic-SnapShots nfs defaults 0 0\" >> /etc/fstab ;\\ mount -a ;\\ df -mh 3. 给elasticsearch授予共享目录/data/es/Elastic-SnapShots权限 chown -R elasticsearch:elasticsearch /data/es/Elastic-SnapShots 4. ES集群所有节点配置文件设置 echo 'path.repo: [\"/data/es/Elastic-SnapShots\"]' >> /etc/elasticsearch/elasticsearch.yml ;\\ systemctl restart elasticsearch;\\ systemctl status elasticsearch 三、使用HDFS作为快照仓库后端存储 ES版本：5.6.8 HDFS版本：2.6.0 1、所有ES节点安装repository-hdfs插件 在线安装插件 /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-hdfs 离线安装插件，插件下载地址：https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip wget https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip ;\\ /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-hdfs-5.6.8.zip 2、重启ES集群所有节点 systemctl restart elasticsearch ;\\ systemctl status elasticsearch 3、后续创建HDFS类型仓库时遇到的问题 ES会以elasticsearch用户(即启动elasticsearch后台进程的用户)在HDFS的/user下创建文件时提示权限不足。所以修改HDFS上/user的权限 hdfs dfs -chmod -R 777 /user 如果HDFS集群在ES集群外面，ES中的Hadoop客户端向通过Hadoop NameNode节点返回的DataNode节点写数据时会找不到DataNode节点。因为创建仓库时只是指定NameNode节点的外网地址，而返回的DataNode节点IP地址是DataNode向NameNode节点注册的内网IP地址，ES集群根本无法访问到。所以打通两者之间的网络。 四、在 kibana 的 Dev Tools 上管理快照仓库 1、注册NFS类型的快照仓库 PUT /_snapshot/快照仓库名 { \"type\": \"fs\", \"settings\": { \"compress\": true, \"location\": \"/data/es/Elastic-SnapShots\" } } ## settings的其他参数： ​ # chunk_size Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified in bytes or by using size value notation, i.e. 1g, 10m, 5k. Defaults to null (unlimited chunk size). #max_restore_bytes_per_sec Throttles per node restore rate. Defaults to 40mb per second. #max_snapshot_bytes_per_sec Throttles per node snapshot rate. Defaults to 40mb per second. #readonly Makes repository read-only. Defaults to false. 2、注册HDFS类型的快照仓库 PUT _snapshot/快照仓库名 { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://172.16.3.10:9000\", \"compress\": true, \"path\": \"elasticsearch/respositories\" } } ##settings的其他参数： ​ #uri The uri address for hdfs. ex: \"hdfs://:/\". (Required) #path The file path within the filesystem where data is stored/loaded. ex: \"path/to/file\". (Required) #load_defaults Whether to load the default Hadoop configuration or not. (Enabled by default) #conf. Inlined configuration parameter to be added to Hadoop configuration. (Optional) Only client oriented properties from the hadoop core and hdfs configuration files will be recognized by the plugin. #compress Whether to compress the metadata or not. (Disabled by default) #chunk_size Override the chunk size. (Disabled by default) #security.principal Kerberos principal to use when connecting to a secured HDFS cluster. If you are using a service principal for your elasticsearch node, you may use the _HOST pattern in the principal name and the plugin will replace the pattern with the hostname of the node at runtime (see Creating the Secure Repository). 3、删除快照仓库 DELETE /_snapshot/快照仓库名 4、查看所有的快照仓库 GET _snapshot/_all 五、快照管理 1、创建包含所有Index的全量快照 PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true 2、创建中包含指定索引的快照 PUT /_snapshot/快照仓库名/快照名?wait_for_completion=true { \"indices\": \"index-A,index-B\", \"ignore_unavailable\": true, \"include_global_state\": false } 3、查看仓库中所有的快照 GET _snapshot/快照仓库名/_all GET _cat/snapshots/快照仓库名 4、删除快照 DELETE _snapshot/快照仓库名/快照名 5、查看多个快照的状态 GET /_snapshot/快照仓库名/快照1,快照2/_status 6、查看某一个快照状态 GET _snapshot/快照仓库名/快照名/_status 7、恢复一个快照 POST _snapshot/快照仓库名/快照名/_restore 六、使用 _cat API格式化查询快照仓库中的的快照 使用Snapshot API查出来的信息是JSON格式的，后续处理比较麻烦。可使用\"_cat\" API Endpoint格式化查询输出Snapshot仓库中的快照信息。关于\"_cat\" API的详细使用信息详见Elasticsearch的\"_cat\"API 1、查看_cat的snapshots API的所有参数 GET _cat/snapshots?help 或 curl -XGET \"http://localhost:9200/_cat/snapshots?help\" 名字 简称 描述 id id,snapshot unique snapshot status s,status snapshot name start_epoch ste,startEpoch start time in seconds since 1970-01-01 00:00:00 start_time sti,startTime start time in HH:MM:SS end_epoch ete,endEpoch end time in seconds since 1970-01-01 00:00:00 end_time eti,endTime end time in HH:MM:SS duration dur,duration duration indices i,indices number of indices successful_shards ss,successful_shards number of successful shards failed_shards fs,failed_shards number of failed shards total_shards ts,total_shards number of total shards reason r,reason reason for failures 2、示例 例如只查看快照仓库中的快照名并排序 GET _cat/snapshots/pvc-snap-repo?h=id&s=id 或 curl -XGET \"http://elasticsearch:9200/_cat/snapshots/pvc-snap-repo?h=id&s=id\" # 返回的结果格式是纯文本的 # apm-7.1.1-metric-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-onboarding-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-span-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-transaction-2019.07.16-snapshot-2019.07.22 # ansi-kpo-tek1269219-h136-2019.07.16-snapshot-2019.07.22 # curiouser-ocp-allinone-audit-2019.08.15-snapshot-2019.08.22 # kibana_sample_data_logs-snapshot-2019.07.22 # springboot2-demo-dev-2019.07.12-snapshot-2019.07.15 # springboot2-demo-dev-2019.07.13-snapshot-2019.07.17 七、更新 Elasticsearch 7.2.0新版本有了管理Snapshot Repository的新功能 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-08 18:18:22 "},"origin/elasticsearch-插件管理.html":{"url":"origin/elasticsearch-插件管理.html","title":"插件管理","keywords":"","body":"ES自带的有插件管理脚本命令 以RPM方式安装的ES，插件管理脚本在/usr/share/elasticsearch/bin/elasticsearch-plugin。该脚本能安装，列出，移除插件 $> cd /usr/share/elasticsearch/bin/ $> ./elasticsearch-plugin list #列出所有插件 $> ./elasticsearch-plugin install plugin_name #安装插件 $> ./elasticsearch-plugin remove plugin_name #卸载插件 #该脚本的参数 #-v 输出详细信息 #-s 输出最简信息 The script may return the following exit codes: 0 : everything was OK 64 : unknown command or incorrect option parameter 74 : IO error 70 : any other error 设置代理来安装插件 $ sudo ES_JAVA_OPTS=\"-Dhttp.proxyHost=代理服务器IP地址 -Dhttp.proxyPort=代理服务器端口 -Dhttps.proxyHost=代理服务器IP地址 -Dhttps.proxyPort=代理服务器端口\" bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/elasticsearch-问题总结.html":{"url":"origin/elasticsearch-问题总结.html","title":"问题总结","keywords":"","body":"一、elasticsearch集群开启“xpack的monitoring功能“导致”failed to flush export bulks和 there are no ingest nodes in this cluster”报错 原因： xpack的monitoring功能需要定义exporter用于导出监控数据， 默认的exporter是local exporter，也就是直接写入本地的集群，并且要求节点开启了ingest选项。 解决方案: 将集群的结点配置里的ingest角色打开 或者在集群设置elasticsearch.yml里，将local exporter的use ingest关掉 xpack.monitoring.exporters.my_local: type: local use_ingest: false 但一般的，使用local cluster监控自己存在很大的问题，故障发生时，监控也没法看到了。 生产上最好是设置一个单独的监控集群，然后可以配置一个HTTP exporter，将监控数据送往这个监控集群 参考： https://www.elastic.co/guide/en/x-pack/5.5/monitoring-cluster.html#http-exporter-reference https://elasticsearch.cn/question/1915 二、Elasticsearch的监控日志索引Index的保存期限为7天 Elasticsearch的监控日志索引Index为\".monitoring-*\"开头的，保存期限为7天，7天之后会自动删除。 参考 https://discuss.elastic.co/t/how-system-index-like-monitoring-es-6-2018-02-06-are-being-deleted-automatically/119578 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-22 16:41:00 "},"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html":{"url":"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html","title":"Prometheus","keywords":"","body":"Kubernetes或Openshift的Prometheus监控体系 一、Overview cluster-monitoring-operator ：负责在 OpenShfit 环境中部署基于 Prometheus 的监控系统 GIthub：https://github.com/openshift/cluster-monitoring-operator 部署基于 Prometheus 监控系统中的组件 Prometheus Operator Prometheus Alertmanager cluster for cluster and application level alerting kube-state-metrics node_exporter prometheus operator：负责配置、管理Prometheus和Alertmanager GIthub： https://github.com/coreos/prometheus-operator 相关博客：https://blog.csdn.net/ygqygq2/article/details/83655552 功能： Create/Destroy: 在Kubernetes namespace中更容易启动一个Prometheus实例，一个特定的应用程序或团队更容易使用Operator。 Simple Configuration: 配置Prometheus的基础东西，比如在Kubernetes的本地资源versions, persistence, retention policies, 和replicas。 Target Services via Labels: 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。 架构 kube-state-metrics：监听 Kubernetes API server 并自动生成相关对象的metrics信息(并不修改相关对象的配置)，在80端口(默认)暴露出HTTP的endpoint /metric GIthub：https://github.com/kubernetes/kube-state-metrics node-exporter：以Daemonset的形式部署在Openshift集群的各个节点上，采集OS级别的metrics信息 监控的Target Prometheus itself Prometheus-Operator cluster-monitoring-operator Alertmanager cluster instances Kubernetes apiserver kubelets (the kubelet embeds cAdvisor for per container metrics) kube-controllers kube-state-metrics node-exporter etcd (if etcd monitoring is enabled) Note: Prometheus Pod 中，除了 Prometheus 容器外，还有一个 prometheus-config-reloader 容器。它负责导入在需要的时候让Prometheus 重新加载配置文件。 配置文件被以 Secret 形式创建并挂载给 prometheus-config-reloader Pod。一旦配置有变化，它会调用 Prometheus 的接口，使其重新加载配置文件。 相关链接 https://docs.okd.io/3.11/install_config/prometheus_cluster_monitoring.html#prometheus-cluster-monitoring http://www.cnblogs.com/sammyliu/p/10155442.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html":{"url":"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html","title":"Ceph Exporter","keywords":"","body":"一、Overview 由于在Openshift集群外使用了Ceph RBD和Ceph Filesystem作为PV的后端动态存储文件系统，所以ceph的集群监控也可使用Prometheus体系中的Ceph Exporter，接入到Openshift集群中的Prometheus。 二、以DaemonSet的形式部署Ceph Exporter --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: ceph-exporter name: ceph-exporter spec: selector: matchLabels: app: ceph-exporter template: metadata: labels: app: ceph-exporter spec: containers: - image: digitalocean/ceph_exporter imagePullPolicy: IfNotPresent name: ceph-exporter ports: - containerPort: 9128 hostPort: 9128 name: http protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/ceph name: ceph-confdir resources: limits: cpu: 200m memory: 400Mi requests: cpu: 100m memory: 200Mi dnsPolicy: ClusterFirst hostNetwork: true hostPID: true nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: node-exporter　#使用Node-Exporter创建的ServiceAccount serviceAccountName: node-exporter terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - hostPath: path: /etc/ceph #将ceph节点的配置文件路径暴露给exporter type: \"\" name: ceph-confdir templateGeneration: 1 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate --- apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: ceph-exporter name: ceph-exporter subsets: - addresses: - ip: 192.168.1.96 nodeName: allinone.okd311.curiouser.com targetRef: kind: Pod ports: - name: http port: 9128 protocol: TCP --- apiVersion: v1 kind: Service metadata: annotations: prometheus.io/port: '9128' prometheus.io/scrape: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: clusterIP: None ports: - name: http port: 9128 protocol: TCP targetPort: http selector: app: ceph-exporter sessionAffinity: None type: ClusterIP --- apiVersion: route.openshift.io/v1 kind: Route metadata: annotations: openshift.io/host.generated: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: port: targetPort: http to: kind: Service name: ceph-exporter weight: 100 wildcardPolicy: None 三、Ceph Exporter对接Prometheus 备份Prometheus原配置文件secret Prometheus原始配置secret文件 创建新的Prometheus配置secret 在原Prometheus配置文件中添加consul 服务发现和ceph-exporter相关的配置 ...省略... - job_name: consul-prometheus metrics_path: /monitor/prometheus scrape_interval: 20s scheme: http scrape_timeout: 5s consul_sd_configs: - server: consul-server.consul.svc:8500 services: [] scheme: http allow_stale: true refresh_interval: 20s - job_name: openshift-monitoring/ceph-exporter/0 honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - openshift-monitoring scrape_interval: 30s scheme: http relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: ceph-exporter - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_pod_name target_label: pod - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_service_name target_label: job replacement: ${1} - source_labels: - __meta_kubernetes_service_label_k8s_app target_label: job regex: (.+) replacement: ${1} - target_label: endpoint replacement: http ...省略... 替换Prometheus的POD secret 四、验证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html":{"url":"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html","title":"Dockerfile中CMD与ENTRYPOINT命令的区别","keywords":"","body":"CMD与ENTRYPOINT区别 CMD命令设置容器启动后默认执行的命令及其参数，但CMD设置的命令能够被docker run命令后面的命令行参数替换 ENTRYPOINT配置容器启动时的执行命令（不会被忽略，一定会被执行，即使运行 docker run时指定了其他命令） ENTRYPOINT 的 Exec 格式用于设置容器启动时要执行的命令及其参数，同时可通过CMD命令或者命令行参数提供额外的参数 ENTRYPOINT 中的参数始终会被使用，这是与CMD命令不同的一点 1. Shell格式和Exec格式命令 Shell格式：指令 CMD java -jar test.jar Exec格式：指令 [\"executable\", \"param1\", \"param2\", ...] ENTRYPOINT [\"java\", \"-jar\", \"test.jar\"] 2. Shell格式和Exec格式命令的区别 Shell格式中的命令会直接被Shell解析 Exec格式不会直接解析，需要加参数 3. CMD和ENTRYPOINT指令支持的命令格式 CMD 指令的命令支持以下三种格式: Exec格式: CMD [\"executable\",\"param1\",\"param2\"] Exec参数: CMD [\"param1\",\"param2\"] 用来为ENTRYPOINT 提供参数 Shell格式: CMD command param1 param2 ENTRYPOINT 指令的命令支持以下了两种格式: Exec格式：可用使用CMD的参数和可使用docker run [image] 参数后面追加的参数 Shell格式 ：不会使用 CMD参数，可使用docker run [image] 参数后面追加的参数 4. 示例 ENTRYPOINT的Exec格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的Exec格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的shell格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello /bin/sh -c Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的shell格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 + CMD的Shell格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 +CMD的Exec格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello 参考链接 https://blog.csdn.net/weixin_42971363/article/details/91506844 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-22 13:11:57 "},"origin/docker-使用Makefile操作Dockerfile.html":{"url":"origin/docker-使用Makefile操作Dockerfile.html","title":"使用Makefile操作Dockerfile.md","keywords":"","body":" IMAGE_BASE = docker-registry-default.apps.okd311.curiouser.com/openshift IMAGE_NAME = demo-springboot2 IMAGE_VERSION = latest IMAGE_TAGVERSION = $(GIT_COMMIT) all: build tag push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . tag: docker tag ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} makefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误：`Makefile:10: * multiple target patterns. Stop.`** Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-23 09:18:56 "},"origin/shell-变量.html":{"url":"origin/shell-变量.html","title":"变量","keywords":"","body":"一. 变量的定义 1. 将命令的输出赋予变量 var=`shell命令` # `是反引号 2. 变量的参数替换和扩展 表达式 含义 ${var_DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 * ${var=DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 * ${var:-DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * ${var:=DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * ${var+OTHER} 如果var声明了, 那么其值就是$OTHER, 否则就为null字符串 ${var:+OTHER} 如 果var被设置了, 那么其值就是$OTHER, 否则就为null字符串 ${var?ERR_MSG} 如果var没 被声明, 那么就打印$ERR_MSG* ${var:?ERR_MSG} 如果var没 被设置, 那么就打印$ERR_MSG* ${!varprefix*} 匹配之前所有以varprefix开头进行声明的变量 ${!varprefix@} 匹配之前所有以varprefix开头进行声明的变量 3. 读取标准输入输出赋值给变量 read -p \"请输入一个字符： \" key echo $key 二. 变量的引用 $var ${var} ${var:defaultvalue} 三. 内置变量 内置变量 描述 $? 上一条命令执行状态 0 代表执行成功，1代表执行失败 $0~$9 位置参数1-9 ${10} 位置参数10 $# 位置参数个数 $$ 脚本进程的PID $- 传递到脚本中的标识 $! 运行在后台的最后一个作业的进程ID(PID) $_ 之前命令的最后一个参数 $@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 $* 稍有不同 $* 传递给脚本或函数的所有参数 $0 脚本的文件名 $* 和 $@ 的区别 $ 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。但是当它们被双引号(\" \")包含时，\"$\" 会将所有的参数作为一个整体，以\"$1 $2 … $n\"的形式输出所有参数；\"$@\" 会将各个参数分开，以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。 四. 数值型变量的运算 1. 数值型变量的加减乘除 #样本数据 a=120 b=110 ((c=$a+$b)) #结果：230 ((d=$a-$b)) #结果：10 ((e=$a*$b)) #结果：13200 ((f=$a/$b)) #结果：1 c=$((a+b)) #结果：220 d=$((a-b)) #结果：20 e=$((a*b)) #结果：12000 f=$((a/b)) #结果：1 c=`expr a+b` #结果：220 d=`expr $a - $b` #结果：20 e=`expr $a \\* $b` #结果：12000 f=`expr $a / $b` #结果：1 2. 数值型变量的自增 a=1 #第一种整型变量自增方式 a=$(($a+1)) echo $a #第二种整型变量自增方式 a=$[$a+1] echo $a #第三种整型变量自增方式 a=`expr $a + 1` echo $a #第四种整型变量自增方式 let a++ echo $a #第五种整型变量自增方式 let a+=1 echo $a #第六种整型变量自增方式 ((a++)) echo $a 3. 数值类型变量的位数截取 a=1560418197875 # 截去后三位,要求只取\"1560418197\" # 方式1: 数值运算 b=$((a/1000)) # 方式2：字符截取（将数值变量当成字符串来处理） c=${a:0:-3} Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:34:07 "},"origin/shell-文件目录的判断.html":{"url":"origin/shell-文件目录的判断.html","title":"文件目录的判断","keywords":"","body":"文件目录的判断 [ -a FILE ] 如果 FILE 存在则为真。 [ -b FILE ] 如果 FILE 存在且是一个块文件则返回为真。 [ -c FILE ] 如果 FILE 存在且是一个字符文件则返回为真。 [ -d FILE ] 如果 FILE 存在且是一个目录则返回为真。 [ -e FILE ] 如果 指定的文件或目录存在时返回为真。 [ -f FILE ] 如果 FILE 存在且是一个普通文件则返回为真。 [ -g FILE ] 如果 FILE 存在且设置了SGID则返回为真。 [ -h FILE ] 如果 FILE 存在且是一个符号符号链接文件则返回为真。（该选项在一些老系统上无效） [ -k FILE ] 如果 FILE 存在且已经设置了冒险位则返回为真。 [ -p FILE ] 如果 FILE 存并且是命令管道时返回为真。 [ -r FILE ] 如果 FILE 存在且是可读的则返回为真。 [ -s FILE ] 如果 FILE 存在且大小非0时为真则返回为真。 [ -u FILE ] 如果 FILE 存在且设置了SUID位时返回为真。 [ -w FILE ] 如果 FILE 存在且是可写的则返回为真。（一个目录为了它的内容被访问必然是可执行的） [ -x FILE ] 如果 FILE 存在且是可执行的则返回为真。 [ -O FILE ] 如果 FILE 存在且属有效用户ID则返回为真。 [ -G FILE ] 如果 FILE 存在且默认组为当前组则返回为真。（只检查系统默认组） [ -L FILE ] 如果 FILE 存在且是一个符号连接则返回为真。 [ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则返回为真。 [ -S FILE ] 如果 FILE 存在且是一个套接字则返回为真。 [ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 新, 或者 FILE1 存在但是 FILE2 不存在则返回为真。 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 老, 或者 FILE2 存在但是 FILE1 不存在则返回为真。 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则返回为真。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 15:53:54 "},"origin/shell-数值型的判断.html":{"url":"origin/shell-数值型的判断.html","title":"数值型的判断","keywords":"","body":"数值型的判断 # -gt 大于，如[ $a -gt $b ] # -lt 小于，如[ $a -lt $b ] # -eq 等于，如[ $a -eq $b ] # -ne 不等于，如[ $a -ne $b ] # -ge 大于等于，如[ $a -ge $b ] # le 小于等于 ，如 [ $a -le $b ] # 大于(需要双括号),如:(($a > $b)) # >= 大于等于(需要双括号),如:(($a >= $b)) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:53:49 "},"origin/shell-if判断.html":{"url":"origin/shell-if判断.html","title":"if判断","keywords":"","body":"if [ command ]; then 符合该条件执行的语句 fi if [ command ]; then command执行返回状态为0要执行的语句 else command执行返回状态为1要执行的语句 fi if [ command1 ]; then command1执行返回状态为0要执行的语句 elif [ command2 ]; then command2执行返回状态为0要执行的语句 else command1和command2执行返回状态都为1要执行的语句 fi PS: [ command ]，command前后要有空格 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:35:04 "},"origin/shell-for循环语句.html":{"url":"origin/shell-for循环语句.html","title":"for循环语句","keywords":"","body":"第一类：数字性循环 #!/bin/bash for((i=1;i #!/bin/bash for i in $(seq 1 10) do echo $(expr $i \\* 3 + 1); done #!/bin/bash for i in {1..10} do echo $(expr $i \\* 3 + 1); done #!/bin/bash awk 'BEGIN{for(i=1; i 第二类：字符性循环 #!/bin/bash for i in `ls`; do echo $i is file name\\! ; done #!/bin/bash for i in $* ; do echo $i is input chart\\! ; done #!/bin/bash for i in f1 f2 f3 ; do echo $i is appoint ; done #!/bin/bash list=\"rootfs usr data data2\" for i in $list; do echo $i is appoint ; done 第三类：路径查找 #!/bin/bash for file in /proc/*; do echo $file is file path \\! ; done #!/bin/bash for file in $(ls *.sh) do echo $file is file path \\! ; done Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:42:16 "},"origin/shell-while循环语句.html":{"url":"origin/shell-while循环语句.html","title":"while循环语句","keywords":"","body":"while condition ; do statements ... done Note: 和if一样，condition可以有一系列的statements组成，值是最后的statment的exit status Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:46:08 "},"origin/shell-until循环语句.html":{"url":"origin/shell-until循环语句.html","title":"until循环语句","keywords":"","body":"until [condition-is-true] ; do statements ... done Note: 执行statements，直至command正确运行。在循环的顶部判断条件,并且如果条件一直为false那就一直循环下去 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:45:54 "},"origin/shell-字符串的截取拼接.html":{"url":"origin/shell-字符串的截取拼接.html","title":"字符串的截取拼接","keywords":"","body":"一. 字符串的截取 表达式 含义 ${#string} $string的 长度 ${string:position} 在$string中, 从位置$position开始提取子串 ${string:position:length} 在$string中, 从位置$position开始提取长度为$length的子串 ${string#substring} 从 变量$string的开头, 删除最短匹配$substring的子串 ${string##substring} 从 变量$string的开头, 删除最长匹配$substring的子串 ${string%substring} 从 变量$string的结尾, 删除最短匹配$substring的子串 ${string%%substring} 从 变量$string的结尾, 删除最长匹配$substring的子串 ${string/substring/replacement} 使用$replacement, 来代替第一个匹配的$substring ${string//substring/replacement} 使用$replacement, 代替所有匹配的$substring ${string/#substring/replacement} 如果$string的前缀匹配$substring, 那么就用$replacement来代替匹配到的$substring ${string/%substring/replacement} 如果$string的后缀匹配$substring, 那么就用$replacement来代替匹配到的$substring expr match \"$string\" '$substring' 匹配$string开头的$substring* 的长度 expr \"$string\" : '$substring' 匹 配$string开头的$substring* 的长度 expr index \"$string\" $substring 在$string中匹配到的$substring的第一个字符出现的位置 expr substr $string $position $length 在$string中 从位置$position开始提取长度为$length的子串 expr match \"$string\" '\\($substring\\)' 从$string的 开头位置提取$substring* expr \"$string\" : '\\($substring\\)' 从$string的 开头位置提取$substring* expr match \"$string\" '.*\\($substring\\)' 从$string的 结尾提取$substring* expr \"$string\" : '.*\\($substring\\)' 从$string的 结尾提取$substring* 1. # 号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a#*/};echo $b # 结果：openshift/origin-metrics-cassandra:v3.9 2. ## 号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a##*/};echo $b # 结果：origin-metrics-cassandra:v3.9 3. %号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%/*};echo $b # 结果：docker.io/openshift 4. %% 号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%%/*};echo $b # 结果：docker.io 5. 从左边第几个字符开始，及字符的个数 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0:5};echo $b # 结果：docke 6. 从左边第几个字符开始，一直到结束 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:7};echo $b # 结果：io/openshift/origin-metrics-cassandra:v3.9 7. 从右边第几个字符开始，及字符的个数 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8:5};echo $b # 结果：dra:v 8. 从右边第几个字符开始，一直到结束 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8};echo $b # 结果：dra:v3.9 9. 截取字符串中的ip # 样本: a=\"当前 IP：123.456.789.172 来自于：中国 上海 上海 联通\" b=${a//[!0-9.]/};echo $b 或者 echo $a | grep -o -E \"[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]*\" # 结果：123.456.789.172 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 16:50:13 "},"origin/shell-字符串的包含判断关系.html":{"url":"origin/shell-字符串的包含判断关系.html","title":"字符串的包含判断关系","keywords":"","body":"样本数据 a=\"test\" b=\"curiouser\" c=\"test hahah devops\" 1. 通过grep来判断 if `echo $c |grep -q $a` ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 2. 字符串运算符 if [[ $c =~ $a ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 3. 用通配符*号代替str1中非str2的部分，如果结果相等说明包含，反之不包含 if [[ $c == *$a* ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi 4. 利用替换 if [[ ${c/$a//} == $c ]] ;then echo \"$c\" \" ----不包含--- \" \"$a\" else echo \"$c\" \" ----包含--- \" \"$a\" fi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 17:05:01 "},"origin/shell-自定义函数.html":{"url":"origin/shell-自定义函数.html","title":"自定义函数","keywords":"","body":"自定义函数的格式 [ function ] 函数名 [()] { action; [return int;] } # 1.函数在被调用前先声明好 # 2.function关键字可有无 自定义函数的调用 # 函数名() # 函数名(参数1,参数2) # 函数名 参数1 参数2 # $(函数名 参数1 参数2) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-20 15:58:01 "},"origin/maven-Settings配置文件详解.html":{"url":"origin/maven-Settings配置文件详解.html","title":"Mave Settings文件详解","keywords":"","body":"一、settings.xml文件作用 从settings.xml的文件名就可以看出，它是用来设置maven参数的配置文件。并且settings.xml是maven的全局配置文件。而pom.xml文件是所在项目的局部配置。Settings.xml中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 二、settings.xml文件位置 settings.xml文件一般存在于两个位置： 全局配置: ${M2_HOME}/conf/settings.xml 用户配置: user.home/.m2/settings.xml Note： 局部配置优先于全局配置。配置优先级从高到低：pom.xml> user settings > global settings 如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。 三、settings.xml元素详解 settings.xml中的顶级元素 LocalRepository 该值表示构建系统本地仓库的路径。其默认值：~/.m2/repository。 ${user.home}/.m2/repository InteractiveMode 表示maven是否需要和用户交互以获得输入。如果maven需要和用户交互以获得输入，则设置成true，反之则应为false。默认为true。 true UsePluginRegistry maven是否需要使用plugin-registry.xml文件来管理插件版本。如果需要让maven使用文件~/.m2/plugin-registry.xml来管理插件版本，则设为true。默认为false。 false Offline 表示maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。 当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 false PluginGroups 当插件的组织id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。 该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。 当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins和org.codehaus.mojo。 org.codehaus.mojo Servers 仓库的下载和部署是在pom.xml文件中的repositories和distributionManagement元素中定义的。然而，一般类似用户名、密码（有些仓库访问是需要安全认证的）等信息不应该在pom.xml文件中配置，这些信息可以配置在settings.xml中。 server001 my_login my_password ${usr.home}/.ssh/id_dsa some_passphrase 664 775 Mirrors 为仓库列表配置的下载镜像列表。 planetmirror.com PlanetMirror Australia http://downloads.planetmirror.com/pub/maven2 central Proxies 用来配置不同的代理 myproxy true http proxy.somewhere.com 8080 proxyuser somepassword *.google.com|ibiblio.org Profiles 根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 它包含了id、activation、repositories、pluginRepositories和 properties元素。这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。如果一个settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中带有相同id的profile。 test Activation 自动触发profile的条件逻辑。 如pom.xml中的profile一样，profile的作用在于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test） false 1.5 Windows XP Windows x86 5.1.2600 mavenVersion 2.0.3 ${basedir}/file2.properties ${basedir}/file1.properties 注：在maven工程的pom.xml所在目录下执行mvn help:active-profiles命令可以查看中央仓储的profile是否在工程中生效。 properties 对应profile的扩展属性列表。 maven属性和ant中的属性一样，可以用来存放一些值。这些值可以在pom.xml中的任何地方使用标记${X}来使用，这里X是指属性的名称。属性有五种不同的形式，并且都能在settings.xml文件中访问 1.0通过${project.version}获得version的值。 3. settings.x: 指代了settings.xml中对应元素的值。例如：false通过 ${settings.offline}获得offline的值。 4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问，例如 ${java.home}。 5. x: 在元素中，或者外部文件中设置，以$的形式使用。{someVar}的形式使用。 --> ${user.home}/our-project 注：如果该profile被激活，则可以在pom.xml中使用${user.install}。 Repositories 远程仓库列表，它是maven用来填充构建系统本地仓库所使用的一组远程仓库 codehausSnapshots Codehaus Snapshots false always warn http://snapshots.maven.codehaus.org/maven2 default pluginRepositories 发现插件的远程仓库列表。 和repository类似，只是repository是管理jar包依赖的仓库，pluginRepositories则是管理插件的仓库。 maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址 ActiveProfiles 手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。任何在activeProfile中定义的profile id，不论环境设置如何，其对应的 profile都会被激活。如果没有匹配的profile，则什么都不会发生。 例如，env-test是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。如果运行过程中找不到这样一个profile，Maven则会像往常一样运行。 env-test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-19 15:42:08 "},"origin/maven-生命周期阶段.html":{"url":"origin/maven-生命周期阶段.html","title":"Maven 生命周期阶段","keywords":"","body":"Maven的生命周期以及阶段插件 Maven拥有三个生命周期，每个生命周期包含一些阶段，这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段。 运行任何一个阶段的时候，它前面的所有阶段都会被运行 Maven三个生命周期只是定义了各个阶段要做的事情、但是不做任何实际工作、实际工作都是由插件的目标来完成的。插件以独立的形式存在、Maven会在需要的时候下载并使用插件 一个插件有可能有多个功能、每个功能就是一个目标。比如maven-dependency-plugin有十多个目标、每个目标对应了一个功能。插件的目标为dependency:analyze、dependency:tree和dependency:list。通用写法：插件前缀:插件目标。比如compiler:compile 一、Maven的生命周期阶段及插件绑定 上面三个生命周期中有很多原来的生命周期阶段没有默认绑定插件、也就意味着默认情况下他们没有任何意义。当然如果我们有自己特殊的处理、可以为他们绑定特殊的插件、比如下面会有提到的在打包的时候生成jar包的源码、可以在default生命周期的verify阶段绑定生成源码插件的生成源码的目标。 二、自定义插件绑定 自定义绑定允许我们自己掌控插件目标与生命周期的结合、下面以生成项目主代码的源码jar为例。使用到的插件和他的目标为：maven-source-plugin:jar-no-fork、将其绑定到default生命周期阶段verify上（可以任意指定三套生命周期的任意阶段）、在项目的POM配置中（也可以在父POM中配置、后面聚合与继承会有提到） org.apache.maven.plugins maven-source-plugin 2.1.1 attach-sources verify jar-no-fork # build元素下的plugins子元素中声明插件的使用。使用的maven-source-plugin插件，其groupId为org.apache.maven.plugins（官方插件的groupId），version版本为2.1.1.对于自定义绑定的插件，应应指定一个非快照的版本，避免插件版本变化造成构件不稳定。 execution元素用来配置执行的任务。上面配置了一个id为attach-source的任务，通过phrase将其绑定到了verify生命周期阶段上，再通过goals配置指定要执行的插件目标（及插件功能）。 需要注意的是： 即使不通过phrase来配置生命周期阶段，有的插件也定义了默认的生命周期阶段。可使用maven-help-plugin来查看插件的详细信息。例如： mvn help：describe-Dplugin=org.apache.maven.plugins:maven-source-plugin:2.1.1 上述配置有插件的坐标声明、还有excutions下面每个excution子元素配置的执行的一个个任务、通过phase指定与生命周期的那个阶段绑定、在通过goals指定执行绑定插件的哪些目标。 当插件的目标绑定到不同的生命周期阶段的时候、插件目标的执行顺序是有生命周期阶段的顺序决定的、当多个插件目标绑定到同一生命周期阶段的时候、顺序是按照插件声明的顺序来决定目标的执行顺序。 三、插件配置 有三种方式可配置插件功能目标执行任务时的参数 1、命令行 例如maven-surefire-Plugin提供了maven.test.skip参数，当值为true时就跳过单元测试。在命令行时，加上-D参数配置该插件的参数就能跳过单元测试 maven install -Dmaven.test.skip=true 参数-D是Java自带的，其功能就是通过命令行设置Java系统环境变量。 2、POM中设置插件的全局任务配置 例如在下面POM文件中配置了maven-source-plugin插件要编译什么java版本的源代码，生成什么java版本的字节码文件 org.apache.maven.plugins maven-source-plugin 2.1.1 1.5 1.5 这样，不管绑定到compile阶段的maven-source-plugin：compile任务，还是绑定到test-compile阶段的maven-source-plugin：testCompiler任务，就都能使用到该配置来基于Java 1.5版本来执行任务 3、POM中插件任务配置 用户还可以在POM文件中定义某插件的任务参数。例如以maven-antrun-plugin插件为例。他有一个run目标，可以用来在Maven中调用Ant任务。用户可以将maven-antrun-plugin的目标run绑定到多个生命周期阶段上，再加以不同的配置，就可以让Maven在不同的生命阶周期段执行不同的任务 org.apache.maven.plugins maven-antrun-plugin 1.3 ant-validate validate run HAHAHAHHAHAHAHHAHAHAHHAHAHA ant-verify verify run lalallalal 参考链接 https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference https://maven.apache.org/plugins/index.html https://blog.csdn.net/zhaojianting/article/details/80321488 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/regular-expression详解.html":{"url":"origin/regular-expression详解.html","title":"正则表达式","keywords":"","body":"正则表达式详解 一、什么是正则表达式？ 正则表达式（regular expression）就是用一个“字符串”来描述一个特征，然后去验证另一个“字符串”是否符合这个特征。比如 表达式“ab+” 描述的特征是“一个 'a' 和 任意个 'b' ”，那么 'ab', 'abb', 'abbbbbbbbbb' 都符合这个特征。 二、正则表达式能干什么？ 验证字符串是否符合指定特征，比如验证是否是合法的邮件地址 用来查找字符串，从一个长的文本中查找符合指定特征的字符串，比查找固定字符串更加灵活方便 用来替换，比普通的替换更强大 三、正则表达式规则 参考链接 http://www.regexlab.com/zh/regref.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-16 11:52:39 "},"origin/ceph-rbd单节点安装.html":{"url":"origin/ceph-rbd单节点安装.html","title":"Ceph RBD单节点安装","keywords":"","body":"Prerequisite Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 关闭防火墙和SeLinuxsystemctl disable firewalld ; systemctl stop firewalld ; sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config ; setenforce 0 ; sestatus -v SSH免密码登录打通ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ; ssh-copy-id root@allinone.curiouser.com ; ssh allinone.curiouser.com Hosts绑定IP地址域名解析echo \"192.168.1.21 allinone.curiouser.com\" >> /etc/hosts 创建ceph用户并设置用户密码和为其添加root权限useradd ceph && echo ceph:ceph | chpasswd ; echo \"ceph ALL=(root) NOPASSWD:ALL\" > /etc/sudoers.d/ceph ; chmod 0440 /etc/sudoers.d/ceph 配置Ceph和Epel的yum源仓库 vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=[http://download.ceph.com/rpm-jewel/el7/$basearch](http://download.ceph.com/rpm-jewel/el7/$basearch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=[http://download.ceph.com/rpm-jewel/el7/noarch](http://download.ceph.com/rpm-jewel/el7/noarch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 # =========================================================================================== vim /etc/yum.repos.d/epel.repo [epel] name=Extra Packages for Enterprise Linux 7 - $basearch # baseurl=[http://download.fedoraproject.org/pub/epel/7/$basearch](http://download.fedoraproject.org/pub/epel/7/$basearch) metalink=[https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch](https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch) failovermethod=priority enabled=1 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 # =========================================================================================== 可以修改ceph源（外国的源总是timeout） export CEPH_DEPLOY_REPO_URL=[http://mirrors.163.com/ceph/rpm-jewel/el7](http://mirrors.163.com/ceph/rpm-jewel/el7) ; export CEPH_DEPLOY_GPG_URL=[http://mirrors.163.com/ceph/keys/release.asc](http://mirrors.163.com/ceph/keys/release.asc) (可选)手动安装下载ceph的rpm包（使用ceph-deploy install 安装ceph包网速太慢。）官方下载: http://download.ceph.com/rpm-jewel/el7/x86_64/ 。需要下载的包如下： ceph-10.2.10-0.el7.x86_64.rpm ceph-base-10.2.10-0.el7.x86_64.rpm ceph-common-10.2.10-0.el7.x86_64.rpm ceph-mds-10.2.10-0.el7.x86_64.rpm ceph-mon-10.2.10-0.el7.x86_64.rpm ceph-osd-10.2.10-0.el7.x86_64.rpm ceph-radosgw-10.2.10-0.el7.x86_64.rpm ceph-selinux-10.2.10-0.el7.x86_64.rpm rbd-mirror-10.2.10-0.el7.x86_64.rpm yum localinstall -y ./*.rpm 一、安装Ceph-Deploy 安装Ceph-deploy yum install ceph-deploy -y 安装ceph相关的软件 ceph-deploy install $HOSTNAME 二、创建集群配置文件 创建ceph-deploy的集群配置文件夹，路径并切换过去mkdir my-cluster ;cd my-cluster 用 ceph-deploy 创建集群，用 new 命令、并指定主机作为初始监视器。 ceph-deploy new $HOSTNAME # 该操作会在~/my-cluster下会生成三个文件 $&gt; ls -rw-rw-r-- 1 ceph ceph 251 Jan 12 16:34 ceph.conf -rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log -rw------- 1 ceph ceph 73 Jan 12 16:30 ceph.mon.keyring # ceph.conf中默认的osd pool为3，对应了三个node节点。如果只有两个node节点，则需要修改ceph.conf中的默认值 [global] fsid = 25c13add-967e-4912-bb33-ebbc2cb9376d mon_initial_members = allinone.curiouser.com mon_host = 172.16.2.3 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx filestore_xattr_use_omap = true osd pool default size=1 三、创建Monitor ceph-deploy mon create $HOSTNAME ; ceph-deploy gatherkeys $HOSTNAME ; ceph mds stat #查看mds节点状态 四、创建OSD 方式一： (可选)手动节点上挂载lvm存储到某个目录下，作为node节点上OSD的数据存储目录 yum install -y lvm2 ; disk=/dev/vdc ; pvcreate ${disk} ; vgcreate ${disk} ; vgcreate -s 16m ceph-osd ${disk} ; PE_Number=`vgdisplay|grep \"Free PE\"|awk '{print $5}'` ; lvcreate -l ${PE_Number} -n ceph-osd ceph-osd ; mkfs.xfs /dev/ceph-osd/ceph-osd ; mkdir -p /data/ceph/osd ; chown -R ceph:ceph /data/ceph/osd ; echo \"/dev/ceph-osd/ceph-osd /data/ceph/osd xfs defaults 0 0\" &gt;&gt;/etc/fstab ; mount -a ; df -mh #LV的文件系统格式注意要xfs,CentOS推荐使用xfs的文件系统.如果是ext4，需要在/etc/ceph/ceph.conf 中添加参数用来限制文件名的长度 osd max object name len = 256 osd max object namespace len = 64 # 之后重启osd服务 systemctl restart ceph-osd.target 准备并激活node节点上的OSD #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/data/ceph/osd #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/data/ceph/osd #查看OSD状态 ceph osd tree 方式二：(不是以目录为OSD数据存储设备，而是直接以硬盘。其实就是省去手动在硬盘上创建分区的操作) #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/dev/vdc #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/dev/vdc1 #查看OSD状态 ceph osd tree 五、安装验证 #集群健康状态检查 $> ceph health HEALTH_OK $> ceph -s $> systemctl is-enabled ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 六、其他信息 Ceph相关的SystemD Units ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/ceph-filesystem单节点安装.html":{"url":"origin/ceph-filesystem单节点安装.html","title":"Ceph FileSystem单节点安装","keywords":"","body":"Context Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 一个cephfs至少要求两个librados存储池，一个为data，一个为metadata。当配置这两个存储池时，注意： 为metadata pool设置较高级别的副本级别，因为metadata的损坏可能导致整个文件系统不用 建议metadata pool使用低延时存储，比如SSD，因为metadata会直接影响客户端的响应速度 Preflight 一个 clean+active 的cluster（Ceph RBD单节点安装） cluster fb506b4e-43b8-4634-acb9-ea3ee5a97b91 health HEALTH_OK monmap e1: 1 mons at {allinone=192.168.1.96:6789/0} election epoch 29, quorum 0 allinone fsmap e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} osdmap e113: 1 osds: 1 up, 1 in flags sortbitwise,require_jewel_osds pgmap v61453: 192 pgs, 3 pools, 2639 MB data, 985 objects 2730 MB used, 94500 MB / 97231 MB avail 192 active+clean 一、操作 部署元数据服务器MDSceph-deploy mds create $HOSTNAME 创建cephfs需要的两个存储池：一个pool用来存储数据，一个pool用来存储元数据ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 创建CephFS ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls 二、验证 $ ceph mds stat e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} 三、客户端挂载 Kernel方式 #加载rbd内核模块 modprobe rbd lsmod | grep rbd # 获取client.admin用户的秘钥 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" mkdir /mnt/mycephfs mount -t ceph allinone.okd311.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== FUSE方式 yum -y install ceph-fuse ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m 192.168.197.154:6789 ~/mycephfs/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-16 17:38:11 "},"origin/pxe-kickstart无人值守部署OS.html":{"url":"origin/pxe-kickstart无人值守部署OS.html","title":"PXE-Kickstart无人值守部署OS","keywords":"","body":"一、PXE Kickstart网络引导无人值守部署主机OS 1. PXE简介 PXE(Pre-boot Execution Environment，预启动执行环境)是由Intel公司开发的最新技术，工作于Client/Server的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持通过网络启动作系统，在启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。 严格来说，PXE 并不是一种安装方式，而是一种引导方式。进行 PXE 安装的必要条件是在要安装的计算机中必须包含一个 PXE 支持的网卡（NIC），即网卡中必须要有 PXE Client。PXE 协议可以使算机通过网络启动。此协议分为 Client端和 Server 端，而PXE Client则在网卡的 ROM 中。当计算机引导时，BIOS 把 PXE Client 调入内存中执行，然后由 PXE Client 将放置在远端的文件通过网络下载到本地运行。运行 PXE 协议需要设置 DHCP 服务器和 TFTP 服务器。DHCP 服务器会给 PXE Client（将要安装系统的主机）分配一个 IP 地址，由于是给 PXE Client 分配 IP 地址，所以在配置 DHCP 服务器时需要增加相应的 PXE 设置。此外，在 PXE Client 的 ROM 中，已经存在了 TFTP Client，那么它就可以通过 TFTP 协议到 TFTP Server 上下载所需的文件了。 2. PXE工作流程 ① PXE Client 从自己的PXE网卡启动，向本网络中的DHCP服务器索取IP ② DHCP 服务器返回分配给客户机的IP 以及PXE文件的放置位置(该文件一般是放在一台TFTP服务器上) ③ PXE Client 向本网络中的TFTP服务器索取pxelinux.0 文件 ④ PXE Client 取得pxelinux.0 文件后之执行该文件 ⑤ 根据pxelinux.0 的执行结果，通过TFTP服务器加载内核和文件系统 ⑥ 进入安装画面, 此时可以通过选择HTTP、FTP、NFS 方式之一进行安装 详细工作流程，请参考下面这幅图： 3. Kickstart简介 Kickstart是一种无人值守的安装方式。它的工作原理是在安装过程中记录典型的需要人工干预填写的各种参数，并生成一个名为ks.cfg的文件。如果在安装过程中（不只局限于生成Kickstart安装文件的机器）出现要填写参数的情况，安装程序首先会去查找Kickstart生成的文件，如果找到合适的参数，就采用所找到的参数；如果没有找到合适的参数，便需要安装者手工干预了。所以，如果Kickstart文件涵盖了安装过程中可能出现的所有需要填写的参数，那么安装者完全可以只告诉安装程序从何处取ks.cfg文件，然后就去忙自己的事情。等安装完毕，安装程序会根据ks.cfg中的设置重启系统，并结束安装。 二、PXE+Kickstart无人值守安装OS的工作流程 三、PXE服务端配置 Prerequisite PXE主机： 主机名 IP地址 OS 路由器 pk.tools.curiouser.com 192.168.1.80 CentOS 7.5.1804 192.168.1.1 1、基础准备 上传ISO文件并挂载，关闭Firewall和SELinux yum install -y wget && \\ mkdir /mnt/{cdrom,iso} && \\ wget http://vault.centos.org/7.5.1804/isos/x86_64/CentOS-7-x86_64-Minimal-1804.iso /mnt/iso && \\ echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld 2、配置HTTD服务 安装服务 yum install -y httpd 配置服务 ln -s /mnt/cdrom/ /var/www/html/CentOS7 启动验证服务，服务端口tcp:80 systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 3、配置DHCP服务 安装服务 yum install -y dhcp 配置服务/etc/dhcp/dhcpd.conf default-lease-time 600; max-lease-time 7200; log-facility local7; subnet 192.168.1.0 netmask 255.255.255.0 { option routers 192.168.1.1; # 给 client 的默认网关 option subnet-mask 255.255.255.0; # 给 client 的子网掩码 option domain-name \"curiouser.com\"; # 给 client 的搜索域 option domain-name-servers 192.168.1.1; # 给 client 的域名服务器 range dynamic-bootp 192.168.1.100 192.168.1.120; # 可供分配的IP范围 default-lease-time 21600; max-lease-time 43200; next-server 192.168.1.80; # TFTP Server 的IP地址 filename \"pxelinux.0\"; # pxelinux启动文件位置; } 启动验证服务,服务端口为67 systemctl start dhcpd && \\ systemctl enable dhcpd && \\ systemctl status dhcpd && \\ ss -nulp | grep dhcpd 4、配置TFTP服务 安装服务 yum install -y tftp-server tftp xinetd net-tools 配置服务/etc/xinetd.d/tftp service tftp { socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /var/lib/tftpboot #默认disable是yes的，把它改为no即可 disable = no per_source = 11 cps = 100 2 flags = IPv4 } 启动验证服务,服务端口为UDP:69 systemctl start xinetd && \\ systemctl enable xinetd && \\ systemctl status xinetd && \\ ss -unlp | grep 69 && \\ netstat -a | grep tftp && \\ netstat -tunap | grep :69 5、准备相关文件 安装服务 yum install -y syslinux tree 拷贝文件 cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ && \\ cp /mnt/cdrom/images/pxeboot/{vmlinuz,initrd.img} /var/lib/tftpboot/ && \\ cp /mnt/cdrom/isolinux/{vesamenu.c32,boot.msg,splash.png} /var/lib/tftpboot/ && \\ cp /usr/share/syslinux/{chain.c32,mboot.c32,menu.c32,memdisk} /var/lib/tftpboot/ && \\ mkdir /var/lib/tftpboot/pxelinux.cfg /var/lib/tftpboot/目录结构 tree -phL 2 /var/lib/tftpboot/ ├── [-rw-r--r-- 84] boot.msg # 窗口提示信息文件,提示信息在菜单出现前出现，显示时间较短，可以添加些艺术字之类的信息。 ├── [-rw-r--r-- 20K] chain.c32 ├── [-rw-r--r-- 50M] initrd.img # 这是一个初始化文件，一个最小的系统镜像 ├── [-rw-r--r-- 33K] mboot.c32 ├── [-rw-r--r-- 26K] memdisk ├── [-rw-r--r-- 54K] menu.c32 ├── [-rw-r--r-- 26K] pxelinux.0 ├── [drwxr-xr-x 21] pxelinux.cfg # 启动菜单目录 ├── [-rw-r--r-- 186] splash.png # 窗口背景图片 ├── [-rw-r--r-- 149K] vesamenu.c32 # 系统自带的两种窗口模块之一 └── [-rwxr-xr-x 5.9M] vmlinuz # 内核文件 创建/var/lib/tftpboot/pxelinux.cfg/default （default文件参数详见：PXE引导配置文件参数详解） bash -c 'cat >/var/lib/tftpboot/pxelinux.cfg/default 6、创建KS文件 /var/www/html/CentOS7.cfg 方式一：手动编写(KS文件具体参数详情见笔记：Kickstart文件参数详解) install text lang en_US.UTF-8 keyboard us auth --useshadow --passalgo=sha512 url --url=\"http://192.168.1.80/CentOS7\" rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ selinux --disabled firewall --disabled network --bootproto=dhcp --device=ens192 --ipv6=auto --activate network --hostname=test reboot timezone Asia/Shanghai --isUtc --nontp bootloader --location=mbr --boot-drive=sda clearpart --all --drives=sda services --enabled=NetworkManager,sshd firstboot --enable ignoredisk --only-use=sda #(可选)autopart --type=lvm --fstype=xfs part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 %packages @^minimal @core %end %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') echo $ipaddr $HOSTNAME >> /etc/hosts echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 方式二、使用system-config-kickstart图形化界面配置 安装：system-config-kickstart yum install -y system-config-kickstart 7、验证KS文件的语法正确性 yum install -y pykickstart ksvalidator /var/www/html/CentOS7.cfg 8、(可选)自动安装配置脚本 前提： CentOS-7-x86_64-Everything-1804.iso已经放置在/mnt/iso文件夹下 pxe-kickstart-CentOS7.cfg mkdir /mnt/cdrom && \\ echo \"/mnt/iso/CentOS-7-x86_64-Everything-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld && \\ yum install -y httpd dhcp tftp-server tftp xinetd net-tools syslinux tree && \\ ln -s /mnt/cdrom/ /var/www/html/CentOS7 && \\ systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 && \\ bash -c 'cat >> /etc/dhcp/dhcpd.conf /etc/xinetd.d/tftp /var/lib/tftpboot/pxelinux.cfg/default 参考链接 ★★★★☆：https://blog.csdn.net/yanghua1012/article/details/80426659 ★★★★☆：http://www.178linux.com/99307 ★★★★☆：https://blog.51cto.com/lzhnb/2117618 ★★★☆☆：https://marclop.svbtle.com/creating-an-automated-centos-7-install-via-kickstart-file ★★★☆☆：https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-file-create ★★★☆☆：https://www.cnblogs.com/cloudos/p/8143929.html ★★★☆☆：http://bbs.51cto.com/thread-621450-1.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/pxe-kickstart文件参数详解.html":{"url":"origin/pxe-kickstart文件参数详解.html","title":"Kickstart文件参数详解","keywords":"","body":"一、简介 kickstart文件中各部分(section)要遵循一定的顺序。每个部分中的项(Item)并不需要按照一定的顺序排列，除非有其他要求。各部分的顺序如下： 命令部分 %packages部分 %pre, %post, 以及%traceback部分 -- 这些部分的顺序可以任意排列 %packages, %pre, %post以及%traceback部分需要以%end结束。 不要求的项(Item)可以被省略。 省略任何一个被要求的项将会导致安装程序向用户询问相关的问题，就像典型安装过程向用户询问那样。一旦用户给出了答案，安装过程将会继续自动进行，除非又遇到缺失的项。 以(#)开头的行作为注释行被忽略。 如果在kickstart安装中使用了不推荐的命令、选项或者语法，警告日志将会被记录到anaconda日志中。因为在一个或者两个发行版之间这些不推荐的项经常会被删掉，所以检查安装日志以确保没有使用这些项非常必要。当使用ksvalidator的时候，这些不推荐的项会导致错误。 如果选项后接等号（=），则必须指定一个值 引用磁盘的特殊说明： Kickstart一直通过设备节点名(例如 sda)来引用磁盘。Linux内核采用了更加动态的方法，设备名并不会在重启时保持不变。因此，这会使得在Kickstart脚本中引用磁盘变得复杂。为了满足稳定的设备命名，你可以在项(Item)中使用/dev/disk代替设备名。例如，你可以使用： part / --fstype=ext4 --onpart=/dev/disk/by-path/pci-0000:00:05.0-scsi-0:0:0:0-part1 part / --fstype=ext4 --onpart=/dev/disk/by-id/ata-ST3160815AS_6RA0C882-part1 来代替： part / --fstype=ext4 --onpart=sda1 这种方式提供了对磁盘的持久引用，因而比仅仅使用sda更加有意义。 这在大的存储环境中特别有意义。你也可以使用类似于shell的入口来应用磁盘。这种方式主要用来简化大的存储环境中clearpart以及ignoredisk命令的使用。例如，为了替代： ignoredisk --drives=sdaa,sdab,sdac 你可以使用如下的入口： ignoredisk --drives=/dev/disk/by-path/pci-0000:00:05.0-scsi-* 最后，如果想要在任何地方引用已经存在的分区或者文件系统（例如，在part --ondisk=中），你可以通过文件系统标签(label)或者UUID来进行。例如： part /data --ondisk=LABEL=data part /misc --ondisk=UUID=819ff6de-0bd6-4bf4-8b72-dbe41033a85b 二、必需选项 bootloader ：指明引导程序(bootloader)如何被安装。引申Bootloader相关概念：CentOS系统启动流程 # 建议给Bootloader设置密码以防止黑客修改系统启动项或者不授权登录系统 --append= 指定内核参数，要指定多个参数，使用空格分隔。引导程序默认的参数是\"rhgb quiet\"。举例: \"bootloader --location=mbr --append=\"hdd=ide-scsi ide=nodma\" --boot-drive= 指定安装bootloader到哪个磁盘上 --leavebootorder 防止安装程序更改 EFI 或者 ISeries/PSeries 系统中的现有可引导映像。 --driveorder= 指定在 BIOS 引导顺序中的首选驱动器。例如: bootloader --driveorder=sda,hda --location= 指定引导记录的写入位置（在大多数情况下不需要指定这个选项），有效值如下 1.mbr 默认选项。具体要看该驱动器是使用主引导记录（MBR）还是 GUID 分区表（GPT）方案： a.在使用 GPT 格式化的磁盘中，这个选项会在 BIOS 引导分区中安装 stage 1.5 引导装载程序。 b.在使用 MBR 格式化的磁盘中，会在 MBR 与第一个分区之间的空白空间中安装 stage 1.5。 2.partition 在包含内核的分区的第一个扇区中安装引导装载程序 3.none -不安装引导装载程序。 --password= 如果使用GRUB2,则会将使用这个选项指定的密码设定为引导装载程序密码.这应用来限制对GRUB2 shell的访问,并可以跳过任意内核选项.如果指定密码,GRUB2还会询问用户名。该 用户名总是root --iscrypted 通常当使用 --password= 选项指定引导装载程序密码时，会将其以明文方式保存在 Kickstart 文件中。如果要加密此密码，可使用这个选项和一个加密的密码。 请使用 grub2-mkpasswd-pbkdf2 命令生成加密的密码，输入要使用的密码，并将该命令的输出结果（以 grub.pbkdf2 开头的哈希符号）复制到 Kickstart 文件中 --timeout= 指定引导装载程序引导默认选项前等待的时间（以秒为单位）。 --default= 设定引导装载程序配置中的默认引导映像。 --extlinux 使用 extlinux 引导装载程序而不是 GRUB2。这个选项只能用于支持 extlinux 的系统。 --disabled 这个选项是 --location=none 的加强版。--location=none 只是简单地禁用 bootloader 安装，而 --disabled 则不仅禁用 bootloader 安装，也会禁用 bootloader 软件 包的安装，从而节省了空间。 keyboard：设置系统键盘类型 --vckeymap= # 指定VConsole应该使用的字符映射表。是字符映射表的文件名，和/usr/lib/kbd/keymaps目录下的文件名除去\".map.gz\"后相同。 --xlayouts=,,..., #指定 X 布局列表，该列表可使用逗号分开，无空格。接受与 setxkbmap(1) 相同格式的值，可以是 layout 格式（比如 cz），也可 #以是 layout (variant) 格式（比如 cz (qwerty)）。 --switch=,,..., #指定布局切换选项（在多个键盘布局间切换的快捷方式）列表。必须使用逗号分开多个选项，无空格。接受值与 setxkbmap(1) 格式相同。 ​ #示例使用 --xlayouts= 选项设置两个键盘布局（English (US) 和 Czech (qwerty)），并允许使用 Alt+Shift 在二者之间进行切换： # keyboard --xlayouts=us,'cz (qwerty)' --switch=grp:alt_shift_toggle lang：设置在安装过程中使用的语言以及系统的缺省语言 示例：lang en_US 文本模式的安装过程不支持某些语言(主要是中文,日语,韩文和印度的语言).如果用lang命令指定这些语言中的一种,安装过程仍然会使用英语,但是系统会缺省使用指定的语言. part 或者 partition：在系统上创建一个分区。（install模式必须） part |swap|pv.id|rdid.id options 1.mntpoint:挂载点，是在创建普通分区时指定新分区挂载位置的项；挂载点需要格式正. 例如 /, /usr, /home 2.swap 分区将被用作交换分区。为了自动决定交换分区的大小，可以使用--recommended选项。 3.raid. 表示创建的分区类型为raid型；必须用id号进行唯一区别； 4.pv. 表示所创建的分区类型为LVM型；必须用唯一id号进行区别； --size= 最小分区大小(MB)。这里可以指定一个整数值如500.不要在后面加MB。 --grow 告诉分区增长以填满可用空间(如果有的话)，或者填满设置的最大值。注意，--grow并不支持RAID卷在上的分区。 --maxsize= 分区被设置为grow时的最大分区大小(MB)。指定一个整数值，不要在后面加上MB。 --noformat 告诉安装程序不格式化分区，和--onpart一起使用。 --onpart= or --usepart= 把分区放在已经存在的设备上。使用\"--onpart=LABEL=name\"或者\"--onpart=UUID=name\" 来通过各自的标签(label)或uuid来指定一个分区。Stop (medium size).png Anaconda也许会以特殊的顺序创建分区，所以使用标签比只用分区名要安全些 --ondisk= or --ondrive= 强制在特定的磁盘上创建。 --asprimary 强制分区作为主分区，否则会导致分区失败。 --fsprofile= 为在该分区上创建文件系统的程序指定使用类型。使用类型定义了创建文件系统时各种各样的调整参数。为了让该选项能起作用，文件系统必须支持使用类型的概念，而且必须有 一个配置文件列出所有可用的类型。对于ext2/3/4，配置文件位于/etc/mke2fs.conf。 --fstype= 为分区设置文件系统类型。有效值包括ext4,ext3,ext2,btrfs,swap以及vfat。其它文件系统是否有效取决于传递给anaconda使能其它文件系统的命令行参数。 --fsoptions= 为挂载文件系统指定自由格式的字符串选项。该字符串将会被拷贝到安装系统的/etc/fstab文件中并且应该被引号括起来。 --label= 指定分区上创建的文件系统标签。如果所指定的标签已经被其它文件系统使用，新的标签将会被创建。 --recommended 自动决定分区大小。 --onbiosdisk= 强制在BIOS发现的特定磁盘上创建分区。 --encrypted 说明该分区应该被加密 --passphrase= 指定加密分区时指定的密码短语。如果没有上述--encrypted选项，该选项不起任何作用。如果没有密码短语被指定，将会使用系统范围内的默认密码短语，如果没有默认的， 安装器会停下来提醒。 --escrowcert= 从加载X.509认证。存储用证书加密过的数据加密密钥。只在--encrypted指定时有效。 --backuppassphrase 只在--escrowcert指定时相关。除了存储数据加密密钥之外，产生一个随机密码短语并将其添加到该分区。然后使用--escrowcert指定的证书加密并存储于/root。如果 不止一个LUKS卷使用--backuppassphrase，它们将共享该密码短语。 例： part /boot --fstype=“ext3” --size=100 part swap --fstype=“swap” –size=512 part / --bytes-pre-inode=4096 --fstype=“ext4”--size=10000 part /data --onpart=/dev/sdb1 --noformat part raid.100 --size=2000 part pv.100 --size=1000 auth/authconfig：设置系统的授权验证选项。它只是authconfig程序的封装，因而所有被authconfig程序识别的选项都可以应用于auth命令。想要获取完整的列表，请参考authconfig手册。默认情况下，密码一般会被加密但并不会放在shadow文件中。 --enablemd5,每个用户口令都使用md5加密. --enablenis,启用NIS支持.在缺省情况下,--enablenis使用在网络上找到的域.域应该总是用--nisdomain=选项手工设置. --nisdomain=,用在NIS服务的NIS域名. --nisserver=,用来提供NIS服务的服务器(默认通过广播). --useshadow或--enableshadow,使用屏蔽口令. --enableldap,在/etc/nsswitch.conf启用LDAP支持,允许系统从LDAP目录获取用户的信息(UIDs,主目录,shell 等等).要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN(distinguished name). --enableldapauth,把LDAP作为一个验证方法使用.这启用了用于验证和更改密码的使用LDAP目录的pam_ldap模块.要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN. --ldapserver=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定所使用的LDAP服务器的名字.这个选项在/etc/ldap.conf文件里设定. --ldapbasedn=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定用户信息存放的LDAP目录树里的DN.这个选项在/etc/ldap.conf文件里设置. --enableldaptls,使用TLS(传输层安全)查寻.该选项允许LDAP在验证前向LDAP服务器发送加密的用户名和口令. --enablekrb5,使用Kerberos 5验证用户.Kerberos自己不知道主目录,UID或shell.如果启用了Kerberos,必须启用LDAP,NIS,Hesiod或者使用/usr/sbin/useradd命令来使这个工作站获知用户的帐号.如果使用这个选项,必须安装pam_krb5软件包. --krb5realm=,工作站所属的Kerberos 5领域. --krb5kdc=,为领域请求提供服务的KDC.如果的领域内有多个KDC,使用逗号(,)来分隔它们. --krb5adminserver=,领域内还运行kadmind的KDC.该服务器处理改变口令以及其它管理请求.如果有不止一个KDC,该服务器必须是主KDC. --enablehesiod,启用Hesiod支持来查找用户主目录,UID 和 shell.在网络中设置和使用 Hesiod 的更多信息,可以在 glibc 软件包里包括的/usr/share/doc/glibc-2.x.x/README.hesiod里找到.Hesiod是使用DNS记录来存储用户,组和其他信息的 DNS 的扩展. --hesiodlhs,Hesiod LHS(\"left-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对 base DN的使用. --hesiodrhs,Hesiod RHS(\"right-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对base DN的使用. --enablesmbauth,启用对SMB服务器(典型的是Samba或Windows服务器)的用户验证.SMB验证支持不知道主目录,UID 或 shell.如果启用SMB,必须通过启用LDAP,NIS,Hesiod或者用/usr/sbin/useradd命令来使用户帐号为工作站所知.要使用这个选项,必须安装pam_smb软件包. --smbservers=,用来做SMB验证的服务器名称.要指定不止一个服务器,用逗号(,)来分隔它们. --smbworkgroup=,SMB服务器的工作组名称. --enablecache,启用nscd服务.nscd服务缓存用户,组和其他类型的信息.如果选择在网络上用NIS,LDAP或hesiod分发用户和组的信息,缓存就尤其有用. rootpw：设置系统root账号的密码 rootpw [--iscrypted|--plaintext] [--lock] password --iscrypted #如果该选项存在,口令就会假定已被加密.可使用以下命令生成用随机盐值进行sha512加密后的密码 # python -c 'import crypt,getpass;pw=getpass.getpass();print(crypt.crypt(pw) if (pw==getpass.getpass(\"Confirm: \")) else exit())' --plaintext # 使用不加密的密码 --lock #锁定root用户，root用户将无法登陆Console 三、可选选项 install/upgrade install：默认安装方法。必须从 cdrom、harddrive、nfs、liveimg 或者 url（用于 FTP、HTTP、或者 HTTPS 安装）中指定安装类型 upgrade: 升级现有系统. text/graphical：kickstart安装模式 text：在文本模式下执行kickstart安装. graphical： 在图形模式下执行kickstart安装.kickstart安装默认在图形模式下安装. cdrom/harddrive/url/nfs/liveimg：指定安装类型 cdrom: 从系统上的第一个光盘驱动器CD-ROM/DVD中安装. harddrive [--biospart= | --partition=] [--dir=]: # 从本地驱动器上包含ISO镜像的目录安装，该驱动器必须是vfat或者ext2文件系统。除了改目录之外，还需要以后面的方式提供install.img。 # 一种方式是由boot.iso启动，另一种是在ISO镜像相同的目录中创建一个images/目录，然后将install.img放在那里。 --biospart= 安装用到的BIOS分区（例如82p2）。 --partition= 安装用到的硬盘分区 --dir= 包含ISO镜像和images/install.img的目录 # 例如:harddrive --partition=hdb2 --dir=/tmp/install-tree nfs --server= --dir= [--opts=] # 从指定的NFS服务器安装. --server=,指定NFS服务器（主机名或者IP）。 --dir=,包含安装树的variant目录的目录. --opts=,用于挂载NFS输出的Mount选项(可选). # 例如:nfs --server=nfsserver.example.com --dir=/tmp/install-tree url --url=|--mirrorlist= [--proxy=] [--noverifyssl] # 通过FTP或HTTP从远程服务器上的安装树中安装. --url= 安装用到的URL。支持的协议有HTTP, HTTPS, FTP, file等。 --mirrorlist= 安装用到的镜像URL。在该URL中完成$releasever和$basearch的变量替换 --proxy= 指定安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名 # 例如:url --url http:///或url --url ftp://:@/ liveimg --url= [--proxy=] [--checksum=] [--noverifyssl] #使用磁盘映像而不是软件包安装。映像文件可以是取自实时 ISO 映像的 squashfs.img 文件，压缩 tar 文件（.tar、.tbz、.tgz、.txz、.tar.bz2、.tar.gz 或者 .tar.xz） #或者安装介质可以挂载的任意文件系统。支持的文件系统为 ext2、ext3、ext4、vfat 和 xfs。 # Anaconda预期该镜像包含完成系统安装所需的实用程序。因此，创建磁盘镜像最好的方法是使用livemedia-creator。 # 如果该镜像包含/LiveOS/*.img（这是squashfs.img的构成），LiveOS中的第一个*.img将会被挂载，并用来安装目标系统。 --url= 执行安装的位置。支持的协议为 HTTP、HTTPS、FTP 和 file。 --proxy=[protocol://][username[:password]@]host[:port] 指定在安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替。 --checksum= 可选，镜像文件的sha256校验和。 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名。 reboot/poweroff/shutdown/halt：安装完成后做什么操作 reboot：重启（缺省选项） poweroff：关闭系统并断电.通常,在手工安装过程中,anaconda会显示一条信息并等待用户按任意键来重新启动系统. shutdown：关闭系统. halt：halt选项基本和shutdown -h命令相同. services：设置禁用或允许列出的服务 --disabled 设置服务为禁用 --enabled 启动服务 例：services --disabled autid,cups,smartd,nfslock 服务之间用逗号隔开，不能有空格 selinux： 在系统里设置SELinux状态.在anaconda里,SELinux缺省为enforcing. selinux [--disabled|--enforcing|--permissive] --enforcing,启用SELinux,实施缺省的targeted policy. 注:如果kickstart文件里没有selinux选项,SELinux将被启用并缺省设置为--enforcing. --permissive,输出基于SELinux策略的警告,但实际上不执行这个策略. --disabled,在系统里完全地禁用 SELinux. user：在系统上创建新用户 user --name= [--groups=] [--homedir=] [--password=] [--iscrypted] [--shell=] [--uid=] --name=,提供用户的名字.这个选项是必需的. --groups=,除了缺省的组以外,用户应该属于的用逗号隔开的组的列表. --homedir=,用户的主目录.如果没有指定,缺省为/home/. --password=,新用户的密码.如果没有指定,这个帐号将缺省被锁住. --iscrypted=,所提供的密码是否已经加密？ --shell=,用户的登录shell.如果不提供,缺省为系统的缺省设置. --uid=,用户的UID.如果未提供,缺省为下一个可用的非系统 UID. network：配置系统的网卡信息 --device= 指定要使用network命令配置或者激活的设备。能够以和 ksdevice启动选项相同的方式指定。例如：network --bootproto=dhcp --device=eth0 对于第一个network命令，如果选项没有被指定，它默认是 1)ksdevice启动选项, 2)为了获得kickstart而激活的设备,或者 3)UI上的选择框。对于如下的network命令，需要--device选项。 --ip= 网络接口的IP地址。 --ipv6= 网络接口的IPv6地址。可以是[/]形式的静态地址，例如，3ffe:ffff:0:1::1/128(如果前缀被省略，会被假定为64)，\"auto\"地址分配基于动态的邻居发现协议，而\"dhcp\"会使用DHCPv6协议。 --gateway= 默认网关，是一个IPv4或者IPv6地址。 --nodefroute 组织设备抓取默认路由。在安装器中使用--activate选项激活其它设备时非常有用，从F16起。 --nameserver= 主域名服务器，是一个IP地址。多个域名服务器必须由逗号分隔。 --nodns 并不配置DNS服务器。 --netmask= 安装系统的网络掩码。 --hostname= 安装系统的主机名。 --ethtool= 指定将要传递给ethtool程序的设备附加的低级别设置。 --essid= 无线网网络ID。 --wepkey= 无线网WEP加密密钥。 --wpakey= 无线网WPA加密密钥(从F16起)。 --onboot= 是否在启动时使能该设备。 --dhcpclass= DHCP类别。 --mtu= 设备的MTU。 --noipv4 在该设备上禁用IPv4。 --noipv6 在该设备上禁用IPv6。 --bondslaves 使用该选项指定的网卡作为多网卡绑定的从网卡，虚拟出的网卡的名字由--device指定。例如--bondslaves=eth0,eth1。自Fedora 19开始。 --bondopts 为--bondslaves和--device选项指定的绑定接口指定一个逗号分隔的参数列表。例如：--bondopts=mode=active-backup,primary=eth1。如果一个选项本身以逗号作为分隔符，那么使用分号作为选项之间的分隔符。 --vlanid 使用--device指定的设备作为父设备来创建的vlan设备的Id(802.1q标签)。例如，network --device=eth0 --vlanid=171将会创建vlan设备eth0.171。从Fedora 19起。 logging：该命令控制anaconda安装过程中的错误日志，并不影响安装系统。 --host=,发送日志信息到给定的远程主机,这个主机必须运行配置为可接受远程日志的syslogd进程. --port=,如果远程的syslogd进程没有使用缺省端口,这个选项必须被指定. --level=,debug,info,warning,error或critical中的一个.指定tty3上显示的信息的最小级别.然而,无论这个级别怎么设置,所有的信息仍将发送到日志文件. zerombr ：清除mbr信息，会同时清空系统用原有分区表 clearpart：在建立新分区前清空系统上原有的分区表，默认不删除分区 --all 擦除系统上原有所有分区； --drives 删除指定驱动器上的分区 --initlabel 初始化磁盘卷标为系统架构的默认卷标 --linux 擦除所有的linux分区 --none（default）不移除任何分区 ​ 例：clearpart --drives=hda,hdb --all --initlabel eula：使用这个选项以非用户互动方式接受终端用户许可证协议（End User License Agreement，EULA）。指定这个选项可防止 Initial Setup 在完成安装并第一次重启系统时提示您接受该许可证。 --agreed（强制） - 接受 EULA。必须总是使用这个选项，否则 eula 命令就毫无意义。 ignoredisk:指定安装程序忽略指定的磁盘。如果您使用自动分区并希望忽略某些磁盘的话，这就很有用 --only-use - 指定安装程序要使用的磁盘列表。忽略其他所有磁盘。 例如1：要在安装过程使用磁盘 sda，并忽略所有其他磁盘：ignoredisk --only-use=sda 要包括不使用 LVM 的多路经设备： ignoredisk --only-use=disk/by-id/dm-uuid-mpath-2416CD96995134CA5D787F00A5AA11017 要包括使用 LVM 的多路径设备： ignoredisk --only-use=disk/by-id/scsi-58095BEC5510947BE8C0360F604351918 --interactive - 允许手动导航高级存储页面。 repo：配置用于软件包安装来源的额外的yum库.可以指定多个repo行. repo --name=repoid [--baseurl=|--mirrorlist=url] [options] --name= - 该库的 id。这个选项是必选项。如果库名称与另一个之前添加的库冲突，则会忽略它。因为这个安装程序使用预先配置的库列表，就是说您无法添加名称与预先配置的库相同的库。 --baseurl= - 程序库的 URL。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --mirrorlist，但不能同时使用这两个选项。 --mirrorlist= - URL 指向该程序库的一组镜像。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --baseurl，但不能同时使用这两个选项。 --install - 将所安装系统提供的存储库配置保存在/etc/yum.repos.d/目录中。不使用这个选项，在 Kickstart 文件中配置的程序库只能在安装过程中使用，而无法在安装的系统中使用。 --cost= - 为这个库分配的 cost 整数值。如果多个库提供同样的软件包，这个数字就是用来规定那个库优先使用，cost 较低的库比 cost 较高的库优先。 --excludepkgs= - 逗号分开的软件包名称列表，同时一定不能从这个存储库中提取该软件包名称。如果多个库提供同样的软件包，且要保证其来自某个特定存储库。 可接受完整软件包名称（比如 publican）和 globs（比如 gnome-*）。 --includepkgs= - 逗号分开的软件包名称列表，同时一定要从这个存储库中提取 glob。如果多个存储库提供同样的软件包，且要保证其来自某个特定存储库，这个选项就很有用了。 --proxy=[protocol://][username[:password]@]host[:port] - 指定只有这个存储库使用的 HTTP/HTTPS/FTP 代理服务器。 这个设置不会影响其他存储库，也不会影响将 install.img 附加到 HTTP 安装的方法。 --ignoregroups=true - 组成安装树时使用这个选项，且对安装过程本身没有影响。它告诉组合工具在镜像树时不要查看软件包组信息，这样就不会镜像大量无用数据。 --noverifyssl - 连接到 HTTPS 服务器时禁止 SSL 验证。 interactive：在安装过程中使用kickstart文件里提供的信息,但允许检查和修改给定的值.将遇到安装程序的每个屏幕以及kickstart文件里给出的值. autostep：通常 Kickstart 安装会跳过不必要的页面。这个选项可让安装程序浏览所有页面，并摘要显示每个页面。部署系统时不应使用这个选项，因为它会干扰软件包安装 --autoscreenshot 在安装的每一步均截屏。这些截屏将在安装过程中保存在 /tmp/anaconda-screenshots 中，并在安装完成后保存在 /root/anaconda-screenshots 中。 sshpw：安装过程中是否开启SSH与安装进程进行交互与监控 sshpw --username=name password [--iscrypted|--plaintext] [--lock] --username --iscrypted --plaintext --lock autopart：自动生成分区（autopart 选项不能与 part/partition, raid、logvol 或者 volgroup 在同样的 Kickstart 文件中一同使用。） --type= - 选择您要使用的预先定义的自动分区方案之一。可接受以下值： lvm: LVM 分区方案。 btrfs: Btrfs 分区方案。 plain: 不附带 LVM 或者 Btrfs 的常规分区。 thinp: LVM 精简分区方案。 --fstype= - 选择可用文件系统类型之一。可用值为 ext2、ext3、ext4、xfs 和 vfat。默认系统为 xfs。有关使用这些文件系统的详情，请查看 第 6.14.4.1.1 节 “文件系统类型”。 --nolvm - 不使用 LVM 或者 Btrfs 进行自动分区。这个选项等同于 --type=plain。 --encrypted - 加密所有分区。这等同于在手动图形安装过程的起始分区页面中选中 加密分区 复选框。 # 注意：加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。 # 如果要在虚拟机中安装系统，则可添加 virtio-rng 设备（虚拟随机数生成器）， --passphrase= - 为所有加密设备提供默认的系统范围内的密码短语。 --escrowcert=URL_of_X.509_certificate - 将所有加密卷数据加密密码保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷的密码都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时这个选项才有意义。 --cipher= - 如果指定 Anaconda 默认 aes-xts-plain64 无法满足需要，则可以指定要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 《Red Hat Enterprise Linux 7 安全指南》中有可用加密类型列表，但 Red Hat 强烈推荐您使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 firewall：配置系统防火墙选项 firewall –enable|--disable [ --trust ] [ --port= ] --enable 拒绝回应输出要求的进入连接，比如 DNS 答复或 DHCP 请求。如果需要访问在这台机器中运行的服务，可以选择通过防火墙允许具体的服务。 --disable 不配置任何iptables防御规则； --trust 在这里列出设备,比如em1,允许所有流量通过该防火墙进出那个设备.要列出一个以上的设备,请使用--trust em1 --trust em2。不要使用逗号分开的格式，比如 --trust em1, em2。 --port 可以用端口:协议（port:protocal）格式指定允许通过防火墙的端口。例如，如果想允许 IMAP 通过您的防火墙，可以指定 imap:tcp。还可以具体指定端口号码，要允许 UDP 分组 在端口 1234 通过防火墙，输入 1234:udp。要指定多个端口，用逗号将它们隔开。 --service= 这个选项提供允许服务通过防火墙的高级方法。有些服务（比如 cups、avahi 等等）需要开放多个端口，或者另外有特殊配置方可工作。 您应该使用 --port 选项指定每个具体端口，或者指定 --service= 并同时打开它们 incoming - 使用以下服务中的一个或多个来替换，从而允许指定的服务通过防火墙。 --ssh --smtp --http --ftp ​ 示例：firewall --enable --trust eth0 --trust eth1 --port=80:tcp group:在系统中生成新组。如果某个使用给定名称或者 GID 的组已存在，这个命令就会失败。另外，该 user 命令可用来为新生成的用户生成新组 group --name=name [--gid=gid] --name= - 提供组名称。 --gid= - 组的 UID。如果未提供，则默认使用下一个可用的非系统 GID。 volgroup：创建逻辑卷组vg volgroup name partition [options] ​ --noformat - 使用现有卷组，且不进行格式化。 --useexisting - 使用现有卷组并重新格式化。如果使用这个选项，请勿指定 partition。例如：volgroup rhel00 --useexisting --noformat --pesize= - 以 KiB 为单位设定卷组物理扩展大小。默认值为 4096 (4 MiB)，最小值为 1024 (1 MiB)。 --reserved-space= - 以 MB 为单位指定在卷组中预留的未使用空间量。只适用于新生成的卷组。 --reserved-percent= - 指定卷组中预留未使用空间的比例。只适用于新生成的卷组。 ​ 注意1 ：不要在逻辑卷和卷组名称中使用小横线（-）。如果使用这个字符，会完成安装，但 /dev/mapper/ 目录列出这些卷和卷组时，小横线会加倍。例如：某个卷组名为 volgrp-01，包含名 为 logvol-01 逻辑卷，该逻辑卷会以 /dev/mapper/volgrp--01-logvol--01 列出。这个限制只适用于新创建的逻辑卷和卷组名。如果您使用 --noformat 选项重复使用现有名称， 它们的名称就不会更改。 注意2: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷。例如： part pv.01 --size 10000 volgroup volgrp pv.01 logvol / --vgname=volgrp --size=2000 --name=root logvol：创建逻辑卷lv logvol mntpoint --vgname=name --name=name [options] mntpoint — 是该分区挂载的位置，且必须是以下格式之一： 1. /path 例如：/ 或者 /home 2.swap 该分区被用作交换空间。要自动决定 swap 分区的大小，使用 --recommended 选项：swap --recommended 使用 --hibernation 选项自动决定 swap 分区的大小，同时还允许 您的系统有附加空间以便可以休眠：swap --hibernation分配的分区大小将与 --recommended 加上系统 RAM 量相等。 这些选项如下所示： --noformat- 使用现有逻辑卷且不要对其进行格式化。 --useexisting 使用现有逻辑卷并重新格式化它。 --fstype= 为逻辑卷设置文件系统类型。有效值有：xfs、ext2、ext3、ext4、swap 和 vfat。 --fsoptions= 指定在挂载文件系统时所用选项的自由格式字符串。将这个字符串复制到安装的系统的 /etc/fstab 中，并使用括号括起来。 --mkfsoptions= 指定要提供的附加参数，以便在这个分区中建立文件系统。没有对任何参数列表执行任何操作，因此必须使用可直接为 mkfs 程序提供的格式。 就是说可使用逗号分开或双引号分开的多个选项，要看具体文件系统。 --label= 为逻辑卷设置标签。 --grow 会让逻辑卷使用所有可用空间（若有），或使用设置的最大值（如果指定了最大值）。必须给出最小值，可使用 --percent= 选项或 --size= 选项。 --size= 以 MB 单位的逻辑卷大小。这个选项不能与 --percent= 选项一同使用。 --percent= 考虑任何静态大小逻辑卷时的逻辑卷大小，作为卷组中剩余空间的百分比。这个选项不能与 --size= 选项一同使用。 #重要:创建新逻辑卷时，必须使用 --size= 选项静态指定其大小，或使用 --percent= 选项指定剩余可用空间的百分比。不能再同一逻辑卷中同时使用这些选项。 --maxsize= - 当将逻辑卷被设置为可扩充时以 MB 为单位的最大值。在这里指定一个整数值，如500（不要在数字后添加单位）。 --recommended - 创建 swap 逻辑卷时可采用这个选项，以根据您的系统硬件自动决定这个卷的大小。 --resize - 重新定义逻辑卷大小。如果使用这个选项，则必须还指定 --useexisting 和 --size。 --encrypted - 指定该逻辑卷应该用 --passphrase= 选项提供的密码进行加密。如果没有指定密码短语，安装程序将使用 autopart --passphrase 命令指定默认系统级密码， 如果没有设定默认密码则会停止安装，并提示输入密码短语。 # 注意:加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。如果要在虚拟机中安装系统，则可 # 添加 virtio-rng 设备（虚拟随机数生成器） --passphrase= - 指定在加密这个逻辑卷时要使用的密码短语。必须与 --encrypted 选项一同使用，单独使用这个选项无效。 --cipher= - 指定如果对 Anaconda 默认 aes-xts-plain64 不满意时要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 推荐使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 --escrowcert=URL_of_X.509_certificate 将所有加密卷数据加密密钥作为文件保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷 的密钥都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时 这个选项才有意义。 --thinpool - 创建精简逻辑卷。（使用 none 挂载点）。 --metadatasize=size - 为新的精简池设备指定元数据大小（单位 MiB）。 --chunksize=size - 为新的精简池设备指定块大小（单位 KiB）。 --thin - 创建精简逻辑卷。（要求使用 --poolname） --poolname=name - 指定在其中创建精简逻辑卷的精简池名称。需要 --thin 选项。 --profile=name 指定与精简逻辑卷配合使用的配置文件名称。如果使用此选项，还要用于给定逻辑的卷元数据中包含该名称。默认情况下，可使用的配置文件为在 /etc/lvm/profile 目录中定 义的 default 和 thin-performance。详情请查看 lvm(8) 手册页。 --cachepvs= - 用逗号分开的物理卷列表，应作为这个卷的缓存使用。 --cachemode= - 指定应使用哪种模式缓存这个逻辑卷 - 可以是 writeback，也可以是 writethrough。 #注意:有关缓存的逻辑卷及其模式的详情，请查看 lvmcache(7) 手册页。 --cachesize= - 附加到该逻辑卷的缓存大小，单位为 MiB。这个选项需要 --cachepvs= 选项。 ​ 注意1: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷以占据逻辑组里剩余的 90% 空间。例如：： part pv.01 --size 1 --grow volgroup myvg pv.01 logvol / --vgname=myvg --name=rootvol --percent=90 timezone：设置系统的时区 timezone [ --utc ] 例：timezone --utc Asia/Shanghai 软件包的选择 在 Kickstart 文件中使用 %packages 命令列出要安装的软件包。 可以根据环境、组或者其软件包名称指定软件包。 安装程序定义包含相关软件包的几个环境和组。有关环境和组列表请查看安装光盘中的 repodata/*-comps-variant.architecture.xml 文件。 *-comps-variant.architecture.xml 文件包含描述可用环境（使用 标签标记）和组（ 标记）的结构。每个组都有一个 ID、用户可见性数值、名称、描述和软件包列表。如果未安装选择该组，那么就会安装该软件包列表中标记为 mandatory 的软件包；如果未明确指定，也会安装标记为 default 的软件包，而标记为 optional 的软件包必须在明确指定后方可安装。 您可以使用 ID（ 标签）或者名称（ 标签）指定软件包组或者环境。 %packages 部分必须以 %end 命令结尾。 除组外，您还要指定要安装的整体环境 %packages @^Infrastructure Server %end 指定组，每个条目一行，以 @ 符号开始，接着是空格，然后是完整的组名或 *-comps-variant.architecture.xml 中指定的组 id。 %packages @X Window System @Desktop @Sound and Video %end 根据名称指定独立软件包，每行一个条目。您可以在软件包名称中使用星号（*）作为通配符。 %packages sqlite curl aspell docbook* %end 使用小横线（-）开头指定安装中不使用的软件包或组。 %packages -@Graphical Internet -autofs -ipa*fonts %end 常用软件包选择选项 以下选项可用于 %packages。要使用这个选项，请将其添加到软件包选择部分的开始。例如： %packages --multilib --ignoremissing ​ --default 安装默认软件包组。这与在互动安装过程中的 软件包选择 页面中没有其他选择时要安装的软件包组对应。 --excludedocs 不要安装软件包中的任何文档。大多数情况下，这样会排除一般安装在 /usr/share/doc* 目录中的所有文件，但要排除的具体文件取决于各个软件包。 --ignoremissing 忽略所有在这个安装源中缺少的软件包、组及环境，而不是暂停安装询问是应该放弃还是继续安装。 --instLangs= 指定要安装的语言列表。注：这与软件包组等级选择不同。这个选项不会告诉您应该安装哪些软件包组，而是通过设置 RPM 宏控制应该安装独立软件包中的哪些事务文件。 --multilib 为 multilib 软件包配置已安装的系统（即允许在 64 位系统中安装 32 位软件包），并安装在这一部分指定的软件包。通常在 AMD64 和 Intel 64 系统中，只安装用于整个架构 （标记为 x86_64）的软件包以及用于所有架构（标记为 noarch）软件包。使用这个选项时，将自动安装用于 32 位 AMD 系统 Intel（标记为 i686）的软件包。这只适用于在 %packages 部分明确指定的软件包。对于那些仅作为相依性安装而没有在 Kickstart 文件中指定的软件包，将只安装其所需架构版本，即使有更多可用架构也是如此。 --nocore 禁用默认总被安装的 @Core 软件包组。禁用 @Core 软件包组应只用于创建轻量级的容器；用 --nocore 安装桌面或服务器系统将导致系统不可用。 具体软件包组参数项 这个列表中的选项只用于单一软件包组。不是在 Kickstart 文件的 %packages 命令中使用，而是在组名称中添加条目。例如： %packages @Graphical Internet --optional %end ​ --nodefaults 只安装该组的强制软件包，不是默认选择。 --optional 除安装默认选择外，还要安装在 *-comps-variant.architecture.xml 文件组定义中标记为自选的软件包。 注：有些软件包组，比如 Scientific Support，没有指定任何强制或默认软件包 - 只有自选软件包。在这种情况下必须使用 --optional 选项，否则不会安装这个组中的任何软件包。 安装前脚本 ks.cfg文件被解析后马上加入要运行的命令.这个部分必须处于kickstart文件的最后(在命令部分之后)而且 必须用%pre命令开头，%end结尾. 可以在%pre部分访问网络；然而,此时命名服务还未被配置,所以只能使用IP地址. 预安装脚本不会在 chroot 环境中运行 --interpreter= 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例 %pre #!/bin/sh hds=\"\" mymedia=\"\" for file in /proc/ide/h* do mymedia=`cat $file/media` if [ $mymedia == \"disk\" ] ; then hds=\"$hds `basename $file`\" fi done set $hds numhd=`echo $#` drive1=`echo $hds | cut -d' ' -f1` drive2=`echo $hds | cut -d' ' -f2` ​ #Write out partition scheme based on whether there are 1 or 2 hard drives if [ $numhd == \"2\" ] ; then #2 drives echo \"#partitioning scheme generated in %pre for 2 drives\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75 --ondisk hda\" >> /tmp/part-include echo \"part / --fstype xfs --size 1 --grow --ondisk hda\" >> /tmp/part-include echo \"part swap --recommended --ondisk $drive1\" >> /tmp/part-include echo \"part /home --fstype xfs --size 1 --grow --ondisk hdb\" >> /tmp/part-include else #1 drive echo \"#partitioning scheme generated in %pre for 1 drive\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75\" >> /tmp/part-include echo \"part swap --recommended\" >> /tmp/part-include echo \"part / --fstype xfs --size 2048\" >> /tmp/part-include echo \"part /home --fstype xfs --size 2048 --grow\" >> /tmp/part-include fi %end 安装后脚本 post-install 脚本是在 chroot 环境里运行的.因此,某些任务如从安装介质复制脚本或RPM将无法执行. --nochroot 允许指定想在chroot环境之外运行的命令 --interpreter 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例 %post --nochroot cp /etc/resolv.conf /mnt/sysimage/etc/resolv.conf %end 四、示例 **全新安装系统** install #在文本模式下安装 text #指定安装过程中语言为英语 lang en_US.UTF-8 #指定键盘类型为US布局 keyboard us ​ auth --useshadow --passalgo=sha512 # url --url=\"http://192.168.1.80/CentOS7\" #设置root用户密码 rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ #开启SELinux selinux --enforcing #关闭防火墙 firewall --disabled #设置网络信息。网卡1设置手动获取IP，不初始化IPV6,不设置为默认路由。网卡2设置DHCP获取IP，不初始化IPV6,设置为默认路由。 network --bootproto=static --ip=192.168.10.6 --device=enp0s3 --activate --nodefroute --noipv6 --nameserver=114.114.114.114 --netmask=24 --gateway=192.168.10.1 network --bootproto=dhcp --device=enp0s8 --activate --nameserver=114.114.114.114 --noipv6 #设置主机名 network --hostname=test #安装完成后重启 reboot #设置时区为上海的时区 timezone Asia/Shanghai --isUtc --nontp #将BootLoader安装在sda磁盘上 bootloader --location=mbr --boot-drive=sda #安装时清理sda磁盘上所有的分区 clearpart --all --drives=sda #将SSH和NetworkManager设置为开机自启动 services --enabled=NetworkManager,sshd firstboot --enable #只使用sda磁盘 ignoredisk --only-use=sda #对sda磁盘进行分区。单位是MB part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 ​ %packages @^minimal @core %end #安装后要执行的脚本 %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 五、验证KS文件的语法正确性 yum install -y pykickstart ksvalidator ks文件 六、比较OS不同版本间的KS语法差异 ksverdiff -f RHEL6 -t RHEL7 # -f 指定要比较的第一个发行本，-t 指定要比较的最后一个发行本 参考连接 ★★★★★： https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/installation_guide/sect-kickstart-syntax ★★★★★：https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-commands ★★★★★ https://blog.csdn.net/yanghua1012/article/details/80426659 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-23 13:56:41 "},"origin/pxe-引导配置文件参数详解.html":{"url":"origin/pxe-引导配置文件参数详解.html","title":"PXE引导配置文件参数详解","keywords":"","body":"default ks 　　　　#默认启动的是 'label ks' 中标记的启动内核 prompt 1 #显示 'boot: ' 提示符。为 '0' 时则不提示，将会直接启动 'default' 参数中指定的内容。 timeout 6 　　　　 #在用户输入之前的超时时间，单位为 1/10 秒。 display boot.msg #显示某个文件的内容，注意文件的路径。默认是在/var/lib/tftpboot/ 目录下。也可以指定位类似 '/install/boot.msg'这样的，路径+文件名。 F1 boot.msg 　　　 #按下 'F1' 这样的键后显示的文件。 F2 options.msg F3 general.msg F4 param.msg F5 rescue.msg label linux #'label' 指定你在 'boot:' 提示符下输入的关键字，比如boot: linux[ENTER]，这个会启动'label linux' 下标记的kernel 和initrd.img 文件。 kernel vmlinuz #kernel 参数指定要启动的内核。 append initrd=initrd.img #append 指定追加给内核的参数，能够在grub 里使用的追加给内核的参数，在这里也都可以使用。 label text kernel vmlinuz append initrd=initrd.img text label ks kernel vmlinuz append ks=http://192.168.111.130/ks.cfg initrd=initrd.img #告诉系统，从哪里获取ks.cfg文件 label local localboot 1 label memtest86 kernel memtest append - default menu.c32 prompt 1 timeout 10 menu title ########## PXE Boot Menu ########## label 1 menu label ^1) Install CentOS 7 x64 with Local Repo menudefault kernel centos7/vmlinuz append initrd=centos7/initrd.img text ks=ftp://192.168.100.1/pub/ks.cfg label 2 menu label ^2) Install CentOS 7 x64 with http://mirror.centos.org Repo kernel centos7/vmlinuz append initrd=centos7/initrd.img method=http://mirror.centos.org/centos/7/os/x86_64/ devfs=nomount ip=dhcp label 3 menu label ^3) Install CentOS 7 x64 with Local Repo using VNC kernel centos7/vmlinuz append initrd=centos7/initrd.img method=ftp://192.168.100.1/pub devfs=nomount inst.vnc inst.vncpassword=password Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-23 13:43:15 "},"origin/tool-SublimeText.html":{"url":"origin/tool-SublimeText.html","title":"Sublime Text 3","keywords":"","body":"Sublime Text使用总结 一、简介 Sublime Text是一款具有代码高亮、语法提示、自动完成且反应快速的编辑器软件 Sublime Text具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。 二、安装 Sublime Text官网：http://www.sublimetext.com/3 三、插件管理器Package Control Package Control：https://packagecontrol.io/installation 使用Ctrl + `打开Sublime Text控制台 将下面的代码粘贴到控制台里 Sublime Text 3 import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by) Sublime Text 2 import urllib2,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), 'wb' ).write(by) if dh == h else None; print('Error validating download (got %s instead of %s), please try manual install' % (dh, h) if dh != h else 'Please restart Sublime Text to finish installation') 给Package Controller设置代理 Sublime Text > Preferences > Package Settings > Package Control > Settings - User 编辑 Package Control.sublime-settings，添加两行: \"http_proxy\": \"http://代理IP地址:3128\", \"https_proxy\": \"http://代理IP地址:3128\", 解决无法安装插件问题 使用第三方Channel，见附件 Preference-->Package Settings-->Package Control-->Settings User { \"bootstrapped\": true, \"channels\": [ \"D:/Sublime Text 3/data/channel_v3.json\" ] } 四、常用快捷键 快捷键 功能 Ctrl+H 查找替换 Ctrl+F 查找内容 Ctrl+Shift+F 在文件夹内查找内容，可进行替换 Ctrl+L 选择一行 Ctrl+Shift+D 复制当前行到下行 Ctrl+K+U 大写光标所在词 Ctrl+K+L 小写光标所在词 Ctrl+Shift+D 复制光标所在整行到下一行 Ctrl+Shift+L 鼠标选中多行（按下快捷键），即可同时编辑这些行 Ctrl+Shift+↑/↓ 可以移动此行代码，与上/下行互换 Ctrl+Shift+←/→ 向右/向右单位性地选中文本 Alt+Shift+1 窗口分屏，恢复默认1屏（非小键盘的数字） Alt+Shift+2 左右分屏-2列 Alt+Shift+3 左右分屏-3列 Alt+Shift+4 左右分屏-3列 Alt+Shift+5 等分4屏 Alt+Shift+8 垂直分屏-2屏 Alt+Shift+9 垂直分屏-3屏 Ctrl+K+B 开启/关闭侧边栏 Ctrl+/ 注释单行 Ctrl+Shift+/ 注释多行 Ctrl+K+K 从光标处开始删除代码至行尾 Ctrl+Shift+K 删除整行 Tab 向右缩进 Shift+Tab 向左缩进 Ctrl+J 合并选中的多行代码为一行 Shift+↑/↓/←/→ 向上/下/左/右选中文本 Ctrl+K+0 展开所有折叠代码 Ctrl+M 光标移动至括号内结束或开始的位置 Ctrl+Shift+M 选择括号内的内容 Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑 Ctrl+G 跳转到第几行 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+V 粘贴并格式化 Ctrl+X 删除当前行 Ctrl+Z 撤销 Ctrl+Y 恢复撤销 Ctrl+U 软撤销 Ctrl+T 左右字母互换 Ctrl+Tab 按文件浏览过的顺序，切换当前窗口的标签页 Ctrl+PageDown 向左切换当前窗口的标签页 Ctrl+PageUp 向右切换当前窗口的标签页 Ctrl+W 关闭当前打开文件 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+P 打开命令面板 Ctrl+： 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 Ctrl+R 打开搜索框，自动带@，输入关键字，查找文件中的函数名 Ctrl+P 打开搜索框。1、输入当前项目中的文件名，快速搜索文件2、@和关键字，查找文件中函数名3、：和数字，跳转到文件中该行代码4、#和关键字，查找变量名 五、常用插件 插件名 功能 描述 DeleteBlankLines 去除文本中的空白行 Windows: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank LinesLinux: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank Lines ChineseLocalizations 汉化Sublime Text 请使用主菜单的 帮助/Language 子菜单来切换语言。 目前支持 简体中文 繁体中文 日本語。 要换回英语不需要卸载本插件，请直接从菜单切换英文。 HTML-CSS-JS Prettify HTML/CSS/JS代码格式化 GBK Encoding Support 支持gbk编码 Alignment 代码格式的自动对齐 默认快捷键Ctrl+Alt+A Clipboard History 粘贴板历史记录，方便使用复制/剪切的内容 Ctrl+alt+v：显示历史记录Ctrl+alt+d：清空历史记录Ctrl+shift+v：粘贴上一条记录（最旧）Ctrl+shift+alt+v：粘贴下一条记录（最新） ConvertToUTF8 编辑并保存目前编码不被 Sublime Text 支持的文件 IMESupport 支持中文输入法跟随光标 AutoFileName 自动完成文件名的输入，如图片选取 Trailing spaces 检测并一键去除代码中多余的空格 一键删除多余空格：CTRL+SHITF+T（需配置），更多配置请点击标题。快捷键配置：在Preferences / Key Bindings – User加上{ \"keys\": [\"ctrl+shift+t\"], \"command\": \"delete_trailing_spaces\" } FileDiffs 比较当前文件与选中的代码、剪切板中代码、另一文件、未保存文件之间的差别。可配置为显示差别在外部比较工具，精确到行。 右键标签页，出现FileDiffs Menu或者Diff with Tab…选择对应文件比较即可 DocBlockr 生成优美注释 标准的注释，包括函数名、参数、返回值等，并以多行显示，手动写比较麻烦。输入/、/*然后回车，还有很多用法，请参照https://sublime.wbond.net/packages/DocBlockr SideBarEnhancements 增强型侧边栏 Terminal 接使用终端打开你的项目文件夹，并支持使用快捷键。 默认调用系统自带的PowerShell ctrl+shift+t 打开文件所在文件夹，ctrl+shift+alt+t 打开文件所在项目的根目录文件夹 SFTP 快速编辑远程服务器上的文件 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/windows-cmd发送SMTP邮件.html":{"url":"origin/windows-cmd发送SMTP邮件.html","title":"CMD发送SMTP邮件","keywords":"","body":"Preflight 邮箱开启POP3/SMTP和IMAP/SMTP服务 一、操作 1. windows开启telnet服务 打开控制面板，找到“打开或关闭windows功能”（在“程序”里面），选中对话框中的Telnet客户端，然后确定，等待完成。这时就开启了telnet功能。 2. 在命令刚窗口输入 telnet smtp.163.com 25 3. 向服务器表明身份 helo 163.com # 如果成功，服务器返回 250 OK 4. 登录认证 auth login # 用户名的Base64加密字符。如果成功，服务器返回一串字符，类似于：334 UGFzc3dvcmQ6（334 是不变的，后面的字母可能会变） ***** # 密码的Base64加密字符，如果登录成功，服务器返回一串字符：235 Authentication successful表示登录成功，如果不能成功登录，请检查账号密码是否正确。 ***** # 对于字符串的Base64加密可使用CMD中的“certutil -encode 包含想要加密字符串的文本文件 Base64加密后输出文本文件” 5. 填写发件人和收件人邮箱地址 mail from: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 rcpt to: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 6. 编写邮件 data # 服务器返回 354 End data with . To:******@163.com From:******@163.com Subject:test mail From:******@163.com test body 123 . # 服务器返回 250 Ok: queues as ... 表示邮件已经发送 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/windows-小技巧.html":{"url":"origin/windows-小技巧.html","title":"Windows小技巧","keywords":"","body":"1. CMD下的换行符 在CMD下,可以用^作为换行符,类似于Linux下的\\ 2. CMD下查看端口使用情况 netstat -ano |findstr 8080 3. CMD下杀掉进程 taskkill /pid 8080 -t -f 4. CMD下校验文件的MD5、SHA1、SHA256值 certutil -hashfile yourfilename.ext MD5 certutil -hashfile yourfilename.ext SHA1 certutil -hashfile yourfilename.ext SHA256 5. CMD下激活windows系统 以管理员身份运行CMD 卸载之前的激活密钥 slmgr -upk 设置KMS服务器 slmgr -skms KMS服务器 ​ 常用的KMS服务器 kms.03k.org kms.chinancce.com kms.lotro.cc cy2617.jios.org kms.shuax.com kms.luody.info kms.cangshui.net zh.us.to 122.226.152.230 kms.digiboy.ir kms.library.hk kms.bluskai.com 输入新的密钥 slmgr -ipk 激活密钥 密钥 win10专业版密钥 VK7JG-NPHTM-C97JM-9MPGT-3V66T NPPR9-FWDCX-D2C8J-H872K-2YT43 W269N-WFGWX-YVC9B-4J6C9-T83GX NYW94-47Q7H-7X9TT-W7TXD-JTYPM NJ4MX-VQQ7Q-FP3DB-VDGHX-7XM87 MH37W-N47XK-V7XM9-C7227-GCQG9 VK7JG-NPHTM-C97JM-9MPGT-3V66T ​ 激活 slmgr -ato 6. PowerShell下载文件 $client = new-object System.Net.WebClient $client.DownloadFile('#1', '#2') # #1为下载链接 #2为文件保存的路径 Note： 一定要在路径中写上保存的新文件的全名（包括后缀） 建议保存的文件格式与下载的文件格式一致 7. 离线安装.NET Framework 3.5 Preflight windows 10 的系统ISO镜像 以管理员身份运行的CMD 将ISO镜像中source/sxs目录拷贝到某个路径下（以桌面为例） 在以管理员身份运行的CMD执行以下命令 dism.exe /online /enable-feature /featurename:netfx3 /Source:C:\\Users\\user\\Desktop\\sxs 8. 添加开机自启动bat脚本 方法一：（推荐） 将脚本放置“C:\\Users\\Curiouser\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup”路径下 方法二： 9. 修改远程桌面的默认端口3389 Windows+R,输入regedit，打开注册表，修改一下注册表的值(十进制)，然后重启远程桌面 HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\\PortNumber 防火墙放行新指定的远程桌面端口 10. 防火墙放行指定端口 11、CMD下的用户管理 net user：查看目前系统存在的用户 net user username：查看用户的详细信息 whoami：查看计算机当前登陆的用户 query user：查看已登陆用户的详细信息 logoff+空格+ID号：注销用户 net user 用户名 密码 /add：新增本地用户 net localgroup administrators 用户名 /add：将本地用户加入管理员用户组 net user 用户名 /del：删除用户 runas /user:用户 cmd：以某个用户运行命令 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-08 17:43:50 "},"origin/linux-小技巧.html":{"url":"origin/linux-小技巧.html","title":"Linux小技巧","keywords":"","body":"1、Linux SSH安全设置 只允许某用户从指定IP地址登陆 sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\ systemctl restart sshd 修改会话保持时间 #ClientAliveInterval 0 #ClientAliveCountMax 3 修改成 ClientAliveInterval 30 #（每30秒往客户端发送会话请求，保持连接） ClientAliveCountMax 3 #（去掉注释即可，3表示重连3次失败后，重启SSH会话） 增加ssh登陆的验证次数 MaxAuthTries 20 允许root用户登录 sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config ;\\ systemctl restart sshd 设置登录方式 #AuthorizedKeysFile .ssh/authorized_keys //公钥公钥认证文件 #PubkeyAuthentication yes //可以使用公钥登录 #PasswordAuthentication no //不允许使用密码登录 2、bash不显示路径 命令行会变成-bash-3.2$主要原因可能是用户主目录下的配置文件丢失 # 方式一 cp -a /etc/skel/. ~ # 方式二 echo \"export PS1='[\\u@\\h \\W]\\$'\" >> ~/.bash_profile ;\\ source ~/.bash_profile 3、同时监控多个文件 tail -f file1 file2 4、查看网卡 # 方式一 ifconfig -a # 方式二 cat /proc/net/dev 5、cp目录下的带隐藏文件的子目录 cp -R /home/test/* /tmp/test /home/test下的隐藏文件都不会被拷贝，子目录下的隐藏文件倒是会的 cp -R /home/test/. /tmp/test cp的时候有重复的文件需要覆盖时会让不停的输入yes来确认，可以使用yes| yes|cp -r /home/test/. /tmp/test 6、查看CPU占用最多的前10个进程 ps auxw|head -1;ps auxw|sort -rn -k3|head -10 7、查看内存消耗最多的前10个进程 ps auxw|head -1;ps auxw|sort -rn -k4|head -10 8、查看虚拟内存使用最多的前10个进程 ps auxw|head -1;ps auxw|sort -rn -k5|head -10 9、获取出口IP地址 curl http://members.3322.org/dyndns/getip curl https://ip.cn curl cip.cc curl myip.ipip.net curl ifconfig.me 10、ISO自动挂载 echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh 11、查看系统版本号和内核信息 cat /proc/version uname -a lsb_release -a cat /etc/redhat-release cat /etc/issue rpm -q redhat-release 12、查看物理CPU个数、核数、逻辑CPU个数 CPU总核数 = 物理CPU个数 每颗物理CPU的核数 总逻辑CPU数 = 物理CPU个数 每颗物理CPU的核数 * 超线程数 # 查看CPU信息（型号） cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c # 查看物理CPU个数 cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l # 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep \"cpu cores\"| uniq # 查看逻辑CPU的个数 cat /proc/cpuinfo| grep \"processor\"| wc -l 13、Linux缓存 cached是cpu与内存间的，buffer是内存与磁盘间的，都是为了解决速度不对等的问题。buffer是即将要被写入磁盘的，而cache是被从磁盘中读出来的 buff：作为buffer cache的内存，是块设备的读写缓冲区 cache：作为page cache的内存，文件系统的cache。Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中。 pagecache：页面缓存（pagecache）可以包含磁盘块的任何内存映射。这可以是缓冲I/O，内存映射文件，可执行文件的分页区域——操作系统可以从文件保存在内存中的任何内容。Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。 dentries：表示目录的数据结构 inodes：表示文件的数据结构 #内核配置接口 /proc/sys/vm/drop_caches 可以允许用户手动清理cache来达到释放内存的作用，这个文件有三个值：1、2、3（默认值为0） #释放pagecache $> echo 1 > /proc/sys/vm/drop_caches #释放dentries、inodes $> echo 2 > /proc/sys/vm/drop_caches #释放pagecache、dentries、inodes $> echo 3 > /proc/sys/vm/drop_caches 14、设置代理 $> bash -c 'cat >> /etc/profile 15、查看网卡UUID nmcli con | sed -n '1,2p' 16、时间戳与日期 日期与时间戳的相互转换 #将日期转换为Unix时间戳 date +%s #将Unix时间戳转换为指定格式化的日期时间 date -d @1361542596 +\"%Y-%m-%d %H:%M:%S\" date日期操作 date +%Y%m%d #显示前天年月日 date -d \"+1 day\" +%Y%m%d #显示前一天的日期 date -d \"-1 day\" +%Y%m%d #显示后一天的日期 date -d \"-1 month\" +%Y%m%d #显示上一月的日期 date -d \"+1 month\" +%Y%m%d #显示下一月的日期 date -d \"-1 year\" +%Y%m%d #显示前一年的日期 date -d \"+1 year\" +%Y%m%d #显示下一年的日期 获得毫秒级的时间戳 在linux Shell中并没有毫秒级的时间单位，只有秒和纳秒其实这样就足够了，因为纳秒的单位范围是（000000000..999999999），所以从纳秒也是可以的到毫秒的 current=`date \"+%Y-%m-%d %H:%M:%S\"` #获取当前时间，例：2015-03-11 12:33:41 timeStamp=`date -d \"$current\" +%s` #将current转换为时间戳，精确到秒 currentTimeStamp=$((timeStamp*1000+`date \"+%N\"`/1000000)) #将current转换为时间戳，精确到毫秒 echo $currentTimeStamp 17、设置时区 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 18、生成文件的MD值 在网络传输、设备之间转存、复制大文件等时，可能会出现传输前后数据不一致的情况。这种情况在网络这种相对更不稳定的环境中，容易出现。那么校验文件的完整性，也是势在必行的。 在网络传输时，我们校验源文件获得其md5sum，传输完毕后，校验其目标文件，并对比如果源文件和目标文件md5 一致的话，则表示文件传输无异常。否则说明文件在传输过程中未正确传输。 md5值是一个128位的二进制数据，转换成16进制则是32（128/4）位的进制值。 md5校验，有很小的概率不同的文件生成的md5可能相同。比md5更安全的校验算法还有SHA*系列的。 Linux的md5sum命令 md5sum命令用于生成和校验文件的md5值。它会逐位对文件的内容进行校验。是文件的内容，与文件名无关，也就是文件内容相同，其md5值相同。 #md5sum命令的详解 $> md5sum --h Usage: md5sum [OPTION]... [FILE] With no FILE, or when FILE is -, read standard input. -b, --binary 二进制模式读取文件 -c, --check 从文件中读取、校验MD5值 --tag 创建一个BSD-style风格的校验值 -t, --text 文本模式读取文件（默认） #校验文件MD5值使用的参数 The following four options are useful only when verifying checksums: --quiet don't print OK for each successfully verified file --status don't output anything, status code shows success --strict exit non-zero for improperly formatted checksum lines -w, --warn warn about improperly formatted checksum lines --help display this help and exit --version output version information and exit #生成的MD5值重定向到文件中 $>md5sum filename > filename.md5 #生成的MD5值重定向追加到文件中 $> md5sum filename >>filename.md5 #多个文件输出到一个md5文件中，这要使用通配符* $> md5sum *.iso > iso.md5 #同时计算多个文件的MD5值 $> md5sum filetohashA.txt filetohashB.txt filetohashC.txt > hash.md5 #校验MD5:把下载的文件file和该文件的file.md5报文摘要文件放在同一个目录下 $> md5sum -c file.md5 #创建一个BSD风格的校验值 $> md5sum --tag file.md5 MD5 (file.md5) = 9192e127b087ed0ae24bb12070f3051a Python生成MD5值 # 方式一：使用md5包 import md5 src = 'this is a md5 test.' m1 = md5.new() m1.update(src) print m1.hexdigest() # 方式二：使用hashlib（推荐） import hashlib m2 = hashlib.md5() m2.update(src) print m2.hexdigest() # 加密常见的问题： 1：Unicode-objects must be encoded before hashing 　　解决方案：import hashlib 　　　　　　　m2 = hashlib.md5() 　　　　　　　m2.update(src．encode('utf-8')) 　　　　　　　print m2.hexdigest() Java生成MD5值 import java.security.MessageDigest; public static void main(String[] args) { String password = \"123456\"; try { MessageDigest instance = MessageDigest.getInstance(\"MD5\");// 获取MD5算法对象 byte[] digest = instance.digest(password.getBytes());// 对字符串加密,返回字节数组 StringBuffer sb = new StringBuffer(); for (byte b : digest) { int i = b & 0xff;// 获取字节的低八位有效值 String hexString = Integer.toHexString(i);// 将整数转为16进制 // System.out.println(hexString); if (hexString.length() 19、添加用户 useradd (选项) （参数） #选项 －c：加上备注文字，备注文字保存在passwd的备注栏中 －d：指定用户登入时的启始目录 －D：变更预设值 －e：指定账号的有效期限，缺省表示永久有效 －f：指定在密码过期后多少天即关闭该账号 －g：指定用户所属的起始群组 －G：指定用户所属的附加群组 －m：自动建立用户的登入目录 －M：不要自动建立用户的登入目录 －n：取消建立以用户名称为名的群组 －r：建立系统账号 －s：指定用户登入后所使用的shell －u：指定用户ID号 20、su 与 sudo su : switch to another user 切换用户 sudo : superuser do 允许用户使用superuser的身份执行命令 su username ：切换为username，需要输入username密码 su : 切换为root用户，需要输入root密码 su - : 切换为root用户，需要输入root密码，且环境变量也改变 su - -c \"command\" ：使用root身份执行命令，完成后即退出root身份 sudo command : 与su -c相似，需要输入当前用户（superuser，/etc/sudoers中指定）密码 sudo su -：使用当前用户密码实现root身份的切换 su - hdfs -c command 切换用户并以某用户的身份去执行一条命令 su - hdfs test.sh 切换用户并以某用户的身份去执行一个shell文件 21、重新开启SELinux 如果在使用setenforce命令设置selinux状态的时候出现这个提示：setenforce: SELinux is disabled。那么说明selinux已经被彻底的关闭了,如果需要重新开启selinux vi /etc/selinux/config 更改为：SELINUX=1 必须重启linux，不重启是没办法立刻开启selinux的 重启完以后，使用getenforce,setenforce等命令就不会报“setenforce: SELinux is disabled”了。这时，我们就可以用setenforce命令来动态的调整当前是否开启selinux。 22、检查软件是否已安装，没有就自动安装 rpm -qa |grep \"jq\" if [ $? -eq 0 ] ;then echo \"jq hava been installed \" else yum -y install epel-release && yum -y install jq fi 23、使用privoxy代理http，https流量使用socket连接ShadowSocks服务器 echo \"安装ShadowSocks\" && \\ yum -y install epel-release && yum -y install python-pip && pip install shadowsocks && \\ bash -c 'cat > /etc/shadowsocks.json /etc/systemd/system/shadowsocks.service > /etc/profile && \\ echo \"export https_proxy=http://127.0.0.1:8118\" >> /etc/profile && \\ source /etc/profile && \\ curl www.google.com 24、批量打通指定主机SSH免密钥登录脚本 CentOS $> bash -c 'cat > ./HitthroughSSH.sh ./hosts.txt 25、硬盘自动分区，格式化，开机自动挂载到/data $> disk=/dev/sdc;\\ bash -c \"fdisk ${disk}>/etc/fstab ;\\ sed -i '$ s/$/ \\/data ext4 defaults 0 0/' /etc/fstab ;\\ mkdir /data ;\\ mount -a ;\\ df -h 26、在hosts文件中添加IP地址与主机名的域名映射 ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 27、禁用透明大页 Redhat sed -i '$a echo nerver > /sys/kernel/mm/redhat_transparent_hugepage/defrag\\necho nerver > /sys/kernel/mm/redhat_transparent_hugepage/enabled' CentOS echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled ;\\ sed -i '/GRUB_CMDLINE_LINUX/ s/\"$/ transparent_hugepage=never\"/' /etc/default/grub ;\\ grub2-mkconfig -o /boot/grub2/grub.cfg 28、安装JDK环境 Prerequisite： JDK安装包已下载在内网HTTP服务器中 wget http://192.168.1.2/jdk/jdk-8u111-linux-x64.tar.gz;\\ tar -zxvf jdk-8u111-linux-x64.tar.gz -C /opt;\\ rm -rf jdk-8u111-linux-x64.tar.gz;\\ ln -s /opt/jdk1.8.0_111 /opt/jdk;\\ sed -i '$a export JAVA_HOME=/opt/jdk\\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\\nexport PATH=$PATH:$JAVA_HOME/bin' /etc/profile;\\ source /etc/profile;\\ ln -s /opt/jdk/bin/java /usr/bin/java;\\ java -version;\\ javac -version 29、安装Tomcat，并由systemctl托管 Prerequisite： 已安装JDK Tomcat安装包已下载在内网HTTP服务器中 wget http://192.168.1.2/tomcat/apache-tomcat-8.5.20.tar.gz;\\ tar -zxvf apache-tomcat-8.5.20.tar.gz -C /opt;\\ rm -rf apache-tomcat-8.5.20.tar.gz;\\ ln -s /opt/apache-tomcat-8.5.20 /opt/tomcat;\\ bash -c 'cat > /lib/systemd/system/tomcat.service 30、安装Nginx bash -c 'cat > /etc/yum.repos.d/nginx.repo 31、安装单机版的Zookeeper Prerequisite： 已安装JDK version=3.4.14 curl https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/zookeeper-$version.tar.gz -o /opt/zookeeper-$version.tar.gz tar -zxvf /opt/zookeeper-*.tar.gz -C /opt/ ;\\ rm -rf /opt/zookeeper-*.tar.gz ;\\ ln -s /opt/zookeeper-$version/ /opt/zookeeper ;\\ sed -i '$a export ZOOKEEPER_HOME=/opt/zookeeper\\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin' /etc/profile ;\\ source /etc/profile ;\\ mv /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg ;\\ sed -i -e '/dataDir/d' -e '/dataLogDir/d' /opt/zookeeper/conf/zoo.cfg ;\\ sed -i -e '$a dataDir=/data/zookeeper/data\\ndataLogDir=/data/zookeeper/logs\\nserver.1=127.0.0.1:2888:3888\\nautopurge.purgeInterval=24\\nautopurge.purgeInterval=5' /opt/zookeeper/conf/zoo.cfg ;\\ mkdir -p /data/zookeeper/{data,logs} ;\\ echo \"1\" > /data/zookeeper/data/myid ;\\ zkServer.sh start ;\\ zkServer.sh status ;\\ jps -l 32、安装单机版的Kafka Prerequisite： 已安装Zookeeper version=2.12-2.2.0 curl https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.2.0/kafka_$version.tgz -o /opt/kafka_$version.tgz ;\\ tar -zxvf /opt/kafka_$version.tgz -C /opt;\\ rm -rf /opt/kafka_$version.tgz ;\\ ln -s /opt/kafka_$version /opt/kafka ;\\ sed -i '$a export KAFKA_HOME=/opt/kafka\\nexport PATH=$PATH:$KAFKA_HOME/bin' /etc/profile ;\\ source /etc/profile ;\\ sed -i -e 's/log.dirs=\\/tmp\\/kafka\\/logs/log.dirs=\\/data\\/kafka\\/logs/g' -e 's/log.retention.hours=168/log.retention.hours=1/g' -e '$a auto.create.topics.enable=true\\ndelete.topic.enable=true' /opt/kafka/config/server.properties ;\\ mkdir -p /data/kafka/{logs,data} ;\\ kafka-server-start.sh -daemon /opt/kafka/config/server.properties ;\\ jps -l 33、安装Hadoop客户端 以hadoop 2.8.3版本为例 wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz ;\\ tar -xvf hadoop-2.8.3.tar.gz -C /opt ;\\ rm -rf hadoop-2.8.3.tar.gz ;\\ ln -s /opt/hadoop-2.8.3 /opt/hadoop ;\\ sed -i '$a export HADOOP_HOME=/opt/hadoop\\nexport PATH=$PATH:$HADOOP_HOME/bin' /etc/profile ;\\ source /etc/profile #然后在/opt/hadoop-2.8.3/etc/hadoop/core-site.xml配置文件标签中填写HDFS NameNode节点的IP地址及端口号 fs.default.name hdfs://172.16.3.10:9000 hdfs dfs -ls / 34、安装Maven环境 curl https://mirrors.tuna.tsinghua.edu.cn/apache/maven/binaries/apache-maven-3.2.2-bin.tar.gz -o /opt/apache-maven-3.2.2-bin.tar.gz && \\ tar -zxvf /opt/apache-maven-*.tar.gz -C /opt/ && \\ rm -rf /opt/apache-maven-*.tar.gz && \\ ln -s /opt/apache-maven-3.2.2 /opt/maven && \\ sed -i '$a export M2_HOME=/opt/maven\\nexport PATH=$PATH:$M2_HOME/bin' /etc/profile && \\ source /etc/profile && \\ mvn version 35、安装NodeJS环境 wget https://nodejs.org/dist/v8.9.4/node-v8.9.4-linux-x64.tar.xz ;\\ tar -xvf node-v8.9.4-linux-x64.tar.xz -C /opt/ ;\\ rm -rf node-v8.9.4-linux-x64.tar.xz ;\\ ln -s /opt/node-v8.9.4-linux-x64 /opt/nodejs ;\\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile;\\ source /etc/profile;\\ yum install gcc-c++ make -y;\\ npm config set registry https://registry.npm.taobao.org ;\\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ ;\\ npm version 36、安装Docker，并设置新硬盘LVM成docker的数据目录 wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo ;\\ yum makecache ;\\ yum install docker-ce-17.12.1.ce -y ;\\ systemctl enable docker ;\\ mkdir /etc/docker ;\\ touch /etc/docker/daemon.json ;\\ bash -c ' tee /etc/docker/daemon.json > /etc/fstab ;\\ df -mh ;\\ systemctl start docker ;\\ ls /var/lib/docker/ ;\\ docker info |grep \"Insecure Registries:\" -A 4 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-10 14:56:42 "},"origin/linux-文本处理.html":{"url":"origin/linux-文本处理.html","title":"文本处理","keywords":"","body":"一、awk 1、获取匹配关键字后的内容 awk '{ if(match($0,\"关键字\")) {print substr($0,RSTART+RLENGTH) }}'文件 #示例 # 原始文本 2018-07-31T09:33:08.160102Z 1 [Note] A temporary password isgenerated for root@localhost: oco4Pr&a!o;v # 命令 awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTAR+RLENGTH) }}' test.log # 结果 oco4Pr&a!o;v 2、去除文本中的空行 awk NF test.txt # NF代表当前行的字段数，空行的话字段数为0,被awk解释为假，因此不进行输出。 3、获取匹配关键字后多少的位字符串 # 样本 a=\"Location: https://allinone.okd311.curiouser.com:8443/oauth/token/implicit#access_token=FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ&expires_in=86400&scope=user%3Afull&token_type=Bearer\" # 获取\"access_token=\"的值 # 方式一 echo $a | grep \"access_token=\" |awk -F\"access_token=\" '/access_token=/{printf substr($2,0,43)}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ # 方式二 echo $a | grep \"access_token=\" |awk '{ if(match($0,\"access_token=\")) {print substr($0,RSTART+RLENGTH) }}'| awk -F '&' '{print $1}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ 二、sed sed [-hnV][-e][-f][文本文件] 参数说明： -e 或 --expression= 以选项中指定的script来处理输入的文本文件。 -f 或 --file= 以选项中指定的script文件来处理输入的文本文件。 -h 或 --help 显示帮助。 -n 或 --quiet或--silent 仅显示script处理后的结果。 -V 或 --version 显示版本信息。 动作说明 a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 1、新增内容到末尾行的末尾 sed '$ s/$/新增内容/' file_path 2、去除文本中空行和开头\"##\"的行 sed '/^$/d;/^##/d' file_path 3、去除文本中的空行 sed '/^\\s*$/d' test.txt 4、多个匹配规则 sed -i -e '/hah/a lala\\nhehe' -e '/lala/d' test 5、在查找到匹配行后的操作 sed -i '/hah/a lallalla' test #在查找到匹配行后添加一行 sed -i '/hah/a lala\\nhehe' test #在查找到匹配行后添加多行 sed -i '/hah/d' test #删除查找到匹配行 6、在查找匹配行的末首或末尾添加内容 sed -i '/ha/ s/^/la' test #在查找包含\"ha\"的行首追加\"la\",\"laha\" sed -i '/ha/ s/$/la' test #在查找包含\"ha\"的行末追加\"la\"，\"hala\" 7、去掉文本中开头带#号注释的行 sed -i -c -e '/^$/d;/^#/d' file 8、去除文本中的换行符^M Windows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误 sed 's/^M//' 原文件>新文件 # 注意，^M = Ctrl v + Ctrl m，而不是手动输入^M 三、grep grep [OPTION]... PATTERN [FILE]... -r 是递归查找 -n 是显示行号 -R 查找所有文件包含子目录 -i 忽略大小写 -l 只列出匹配的文件名 -L 列出不匹配的文件名 -w 只匹配整个单词，而不是字符串的一部分 -C 匹配的上下文分别显示[number]行 1、统计某文件夹下文件的个数 ls -l /data|grep \"^-\"|wc -l #不包含子目录 ls -lR /data|grep \"^-\"|wc -l #不含子目录 2、统计某文件夹下目录的个数 ls -l /data |grep \"^ｄ\"|wc -l #不包含子目录 ls -lR /data|grep \"^ｄ\"|wc -l #包含子目录 3、统计某目录(包含子目录)下的所有某种类型的文件 ls -lR /data|grep txt|wc -l 4、去除文本中的空行 grep -v '^\\s*$' test.txt 5、多个匹配规则 grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的 grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。 6、xargs配合grep查找 find -type f -name '*.php'|xargs grep 'GroupRecord' 7、查找路径下含有某字符串的所有文件 grep -rn \"hello,world!\" * 四、egrep 1、只显示文本中的非空行和非注释行 egrep -v '^$|#' file_path 五、cut 1、获取硬盘某个分区的UUID号追加到fstab blkid | grep /dev/sdb5 | cut -d ' ' -f 2 >>/etc/fstab;sed -i '$ s/$/ data ext4 defaults 0 0/' /etc/fstab 六、wc 1、统计某个目录下某种文件内总共多少行 find mapred/ -name \"*.java\" -print | xargs cat | wc -l 七、dos2unix dos2unix是将Windows格式文件转换为Unix、Linux格式的实用命令。Windows格式文件的换行符为\\r\\n ,而Unix&Linux文件的换行符为\\n. dos2unix命令其实就是将文件中的\\r\\n 转换为\\n。而unix2dos则是和dos2unix互为孪生的一个命令，它是将Linux&Unix格式文件转换为Windows格式文件的命令。 安装 yum install dos2unix -y #会安装dos2Unix、unix2dos、unix2mac这三条命令 用法 dos2unix [options] [-c convmode] [-o file ...] [-n infile outfile ...] -h 显示命令dos2unix联机帮助信息。 -k 保持文件时间戳不变 -q 静默模式，不输出转换结果信息等 -V 显示命令版本信息 -c 转换模式 -o 在源文件转换，默认参数 -n 保留原本的旧档，将转换后的内容输出到新档案.默认都会直接在原来的文件上修改， 1、一次转换多个文件 $> dos2unix filename1 filename2 filename3 2、默认情况下会在源文件上进行转换，如果需要保留源文件，那么可以使用参数-n dos2unix -n oldfilename newfilename 3、保持文件时间戳不变 $ ls -lrt dosfile -rw-r--r-- 1 root root 67 Dec 26 11:46 dosfile $ dos2unix dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile $ dos2unix -k dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile 4、静默模式格式化文件 unix2dos -q dosfile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-12 14:58:32 "},"origin/linux-htpasswd.html":{"url":"origin/linux-htpasswd.html","title":"htpasswd","keywords":"","body":"一、Overviews htpasswd命令是Apache的Web服务器内置工具，用于创建和更新储存用户名、域和用户基本认证的密码文件,主要用于对基于http用户的认证。 二、安装 yum install -y httpd-tools 三、语法 htpasswd(选项)(参数) 选项 -c：创建一个加密文件 -n：不更新加密文件，只将加密后的用户名密码显示在屏幕上 -m：默认采用MD5算法对密码进行加密 -d：采用CRYPT算法对密码进行加密 -p：不对密码进行进行加密，即明文密码 -s：采用SHA算法对密码进行加密 -b：在命令行中一并输入用户名和密码而不是根据提示输入密码 -D：删除指定的用户 参数 用户：要创建或者更新密码的用户名 密码：用户的新密码 四、常见操作 1、利用htpasswd命令添加用户 htpasswd .passwd -bc www.linuxde.net php # 在bin目录下生成一个.passwd文件，用户名www.linuxde.net，密码：php，默认采用MD5加密方式 2、在原有密码文件中增加下一个用户 htpasswd .passwd -b Jack 123456 #去掉-c选项，即可在第一个用户之后添加第二个用户，依此类推。 3、不更新密码文件，只显示加密后的用户名和密码 htpasswd -nb Jack 123456 # 不更新.passwd文件，只在屏幕上输出用户名和经过加密后的密码 4、利用htpasswd命令删除用户名和密码 htpasswd .passwd -D Jack 5、利用htpasswd命令修改密码 htpasswd .passwd -D Jack htpasswd .passwd -b Jack 123456 # 即先使用htpasswd删除命令删除指定用户，再利用htpasswd添加用户命令创建用户即可实现修改密码的功能。 参考链接 http://man.linuxde.net/htpasswd Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 10:23:49 "},"origin/linux-shyaml.html":{"url":"origin/linux-shyaml.html","title":"YAML文本处理工具shyaml","keywords":"","body":"Linux下YAML文本处理工具shyaml 一、Overviews 通过 shyaml，可以直接获取键、值、键值对或对应的类型 二、安装 pip install shyaml 三、语法 cat | shyaml ACTION KEY [DEFAULT] ACTION get-type：获取相应的类型 get-value：获取值 get-values{,-0}：对序列类型来说，获取值列表 keys{,-0}：返回键列表 values{,-0}：返回值列表 key-values,{,-0}：返回键值对 Note： 结果默认是加\\n换行符，若用-0形式则以NUL字符填充 KEY为要查询的键，如不提供，则使用DEFAULT 四、示例 --- idc_group: name: bx bx: news_bx: news_bx web3_bx: web3_php-fpm_bx 如果要获取idc_group.name的值则可以执行 cat file.yaml | shyaml get-value idc_group.name 想获取idc_group.bx的键值对可执行 cat file.yaml | shyaml key-values idc_group.bx 参考链接 https://www.linuxidc.com/Linux/2016-04/130403.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-05 10:46:49 "},"origin/linux-jq.html":{"url":"origin/linux-jq.html","title":"JSON文本处理工具jq","keywords":"","body":"Linux下JSON文本处理工具jq 一、Overviews jq 是一款命令行下处理 JSON 数据的工具。其可以接受标准输入，命令管道或者文件中的 JSON 数据，经过一系列的过滤器(filters)和表达式的转后形成我们需要的数据结构并将结果输出到标准输出中。jq 的这种特性使我们可以很容易地在 Shell 脚本中调用它。 二、安装 yum install -y epel-release ;\\ yum install -y jq 三、jq命令参数 jq [options] [file...] options: -c 使输出紧凑，而不是把每一个JSON对象输出在一行。; -n 不读取任何输入，过滤器运行使用null作为输入。一般用作从头构建JSON数据。; -e set the exit status code based on the output; -s 读入整个输入流到一个数组(支持过滤); -r 如果过滤的结果是一个字符串，那么直接写到标准输出（去掉字符串的引号）; -R read raw strings, not JSON texts; -C 打开颜色显示; -M 关闭颜色显示; -S sort keys of objects on output; --tab use tabs for indentation; --arg a v jq 通过该选项提供了和宿主脚本语言交互的能力。该选项将值(v)绑定到一个变量(a)上。在后面的 filter 中可以直接通过变量引用这个值。例如，filter '.$a'表示查询属性名称等于变量 a 的值的属性。; --argjson a v set variable $a to JSON value ; --slurpfile a f set variable $a to an array of JSON texts read from ; 参考链接 https://www.ibm.com/developerworks/cn/linux/1612_chengg_jq/index.html?ca=drs-&utm_source=tuicool&utm_medium=referral Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-05 10:47:08 "},"origin/linux-curl.html":{"url":"origin/linux-curl.html","title":"Curl命令详解","keywords":"","body":"一、Curl命令详解 语法：curl [options] [URL...] 参数： Options: (H) means HTTP/HTTPS only, (F) means FTP only --anyauth 可以使用“任何”身份验证方法 -a, --append FTP/SFTP上传文件时，curl将追加到目标文件，而非覆盖 --basic 使用HTTP基本验证 --cacert FILE 指定CA证书文件(SSL) --capath DIR 指定CA目录 (SSL) -E, --cert CERT[:PASSWD] Client certificate file and password (SSL) --cert-type 指定证书文件类型 (DER/PEM/ENG) (SSL) --ciphers LIST 指定SSL密码 --compressed 响应压缩格式 (deflate/gzip) -K, --config FILE 后接参数文件，参数文件中可以定义HTTP请求的相关的内容（URL、HEAD、DATA） --connect-timeout SECONDS 设置最大请求时间 -C, --continue-at OFFSET 断点续转 -b, --cookie STRING/FILE 设置cookies -c, --cookie-jar FILE 操作结束后把cookie写入到文件中 --create-dirs 建立本地目录层次结构 --crlf 上传时把LF转变成CRLF --crlfile FILE Get a CRL list in PEM format from the given file -d, --data DATA HTTP POST data (H) --data-ascii DATA 以ascii的方式post数据 --data-binary DATA 以二进制的方式post数据 --data-urlencode DATA HTTP POST data url encoded (H) --delegation STRING GSS-API delegation permission --digest 使用HTTP数字身份验证 --disable-eprt 禁止使用EPRT或LPRT --disable-epsv 禁止使用EPSV -D, --dump-header FILE 把header信息写入到文件中 --egd-file FILE 为随机数据(SSL)设置EGD socket路径 --engine ENGINGE 指定加密引擎(SSL). \"--engine list\" for list -f, --fail 连接失败时不显示http错误 -F, --form CONTENT form表单提交 --form-string STRING 模拟http表单提交数据 --ftp-account DATA Account data string (F) --ftp-alternative-to-user COMMAND String to replace \"USER [name]\" (F) --ftp-create-dirs 如果远程目录不存在，创建远程目录 --ftp-method [MULTICWD/NOCWD/SINGLECWD] 控制CWD的使用 --ftp-pasv 使用 PASV/EPSV 代替端口 -P, --ftp-port ADR Use PORT with given address instead of PASV (F) --ftp-skip-pasv-ip Skip the IP address for PASV (F) --ftp-pret Send PRET before PASV (for drftpd) (F) --ftp-ssl-ccc Send CCC after authenticating (F) --ftp-ssl-ccc-mode ACTIVE/PASSIVE Set CCC mode (F) --ftp-ssl-control Require SSL/TLS for ftp login, clear for transfer (F) -G, --get 使用get请求发送 -d参数指定的数据 -g, --globoff 禁用网址序列和范围使用{}和[] -H, --header LINE 增加Head头 -I, --head 只显示文档信息 -h, --help 显示帮助信息 --hostpubmd5 MD5 Hex encoded MD5 string of the host public key. (SSH) -0, --http1.0 强制使用HTTP 1.0协议 --ignore-content-length 忽略的HTTP头信息的长度 -i, --include 输出响应Head头 -k, --insecure 允许curl使用非安全的ssl连接并且传输数据（证书不受信） --interface INTERFACE 使用指定网络接口/地址 -4, --ipv4 解析域名为ipv4地址(域名有多个ip时) -6, --ipv6 解析域名为ipv6地址(域名有多个ip时) -j, --junk-session-cookies 读取文件时忽略session cookie (H) --keepalive-time SECONDS 设置连接的保活时间 --key KEY 私钥文件名(SSL/SSH) --key-type TYPE 私钥文件类型 (DER/PEM/ENG) (SSL) --krb LEVEL 使用指定安全级别的krb (F) --libcurl FILE Dump libcurl equivalent code of this command line --limit-rate RATE 指定最大的传输速率 -l, --list-only 列出ftp目录下的文件名称(F) --local-port RANGE 强制使用本地端口号 -L, --location curl自动重定向（3xx） --location-trusted like --location and send auth to other hosts (H) -M, --manual 显示全手动 --mail-from FROM 指定发信人邮箱(SMTP) --mail-rcpt TO 指定收信人邮箱(SMTP) --mail-auth AUTH Originator address of the original email --max-filesize BYTES 允许下载文件的最大大小 --max-redirs NUM Maximum number of redirects allowed (H) -m, --max-time SECONDS 设置整个操作的允许消耗的最大时间，对于在延时网络下的批量操作有利 --metalink Process given URLs as metalink XML file --negotiate 使用HTTP Negotiate身份验证(H) -n, --netrc 从netrc文件中读取用户名和密码 --netrc-optional 使用 .netrc 或者 URL来覆盖-n --netrc-file FILE 指定.netrc文件 -N, --no-buffer 禁用输出流缓冲区 --no-keepalive 连接不保活 --no-sessionid Disable SSL session-ID reusing (SSL) --noproxy List of hosts which do not use proxy --ntlm 使用 HTTP NTLM 身份验证 -o, --output FILE 将响应数据输出到指定文件，后接文件参数 --pass PASS 私钥密码 (SSL/SSH) --post301 301重定向后不切换至GET请求 (H) --post302 302重定向后不切换至GET请求 (H) --post303 303重定向后不切换至GET请求 (H) -#, --progress-bar 对发送和接收进行简单的进度条展示 --proto PROTOCOLS Enable/disable specified protocols --proto-redir PROTOCOLS Enable/disable specified protocols on redirect -x, --proxy [PROTOCOL://]HOST[:PORT] 设置代理 --proxy-anyauth 选择任一代理身份验证方法 (H) --proxy-basic 在代理上使用基本身份验证 (H) --proxy-digest 在代理上使用数字身份验证 (H) --proxy-negotiate 在代理上使用Negotiate身份验证 (H) --proxy-ntlm 在代理上使用ntlm身份验证 (H) -U, --proxy-user USER[:PASSWORD] 设置代理用户名和密码 --proxy1.0 HOST[:PORT] 使用HTTP/1.0的代理 -p, --proxytunnel Operate through a HTTP proxy tunnel (using CONNECT) --pubkey KEY 公钥文件 (SSH) -Q, --quote CMD 文件传输前，发送命令到服务器 (F/SFTP) --random-file FILE File for reading random data from (SSL) -r, --range RANGE 检索来自HTTP/1.1或FTP服务器字节范围 --raw Do HTTP \"raw\", without any transfer decoding (H) -e, --referer 发送\"Referer Page\"到服务器 -J, --remote-header-name Use the header-provided filename (H) -O, --remote-name 把输出写到文件中，保留远程文件的文件名 --remote-name-all Use the remote file name for all URLs -R, --remote-time 在本地生成文件时，保留远程文件时间 -X, --request COMMAND 指定HTTP请求方法 --resolve HOST:PORT:ADDRESS 强制解析HOST:PORT到某个ADDRESS --retry NUM 传输出现问题时，重试的次数 --retry-delay SECONDS 传输出现问题时，设置重试间隔时间 --retry-max-time SECONDS 传输出现问题时，设置最大重试时间 -S, --show-error 显示错误信息 -s, --silent 静默模式。不输出任何东西 --socks4 HOST[:PORT] 用socks4代理给定主机和端口 --socks4a HOST[:PORT] 用socks4a代理给定主机和端口 --socks5 HOST[:PORT] 用socks5代理给定主机和端口 --socks5-basic socks5代理开启username/password认证 --socks5-gssapi socks5代理开启GSS-API认证 --socks5-hostname HOST[:PORT] SOCKS5 proxy, pass host name to proxy --socks5-gssapi-service NAME SOCKS5 proxy service name for gssapi --socks5-gssapi-nec Compatibility with NEC SOCKS5 server -Y, --speed-limit RATE 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止 -y, --speed-time SECONDS 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止。默认30s --ssl Try SSL/TLS (FTP, IMAP, POP3, SMTP) --ssl-reqd Require SSL/TLS (FTP, IMAP, POP3, SMTP) -2, --sslv2 使用SSLv2的（SSL） -3, --sslv3 使用SSLv3的（SSL） --ssl-allow-beast Allow security flaw to improve interop (SSL) --stderr FILE 指定错误信息输出文件 --tcp-nodelay 使用TCP_NODELAY选项 -t, --telnet-option OPT=VAL Telnet选项设置 --tftp-blksize VALUE 设置TFTP BLKSIZE(必须大于512) -z, --time-cond TIME 传送时间设置 -1, --tlsv1 强制使用TLS version 1.x --tlsv1.0 使用TLSv1.0 (SSL) --tlsv1.1 使用TLSv1.1 (SSL) --tlsv1.2 使用TLSv1.2 (SSL) --trace FILE dump出输入输出数据至文件 --trace-ascii FILE 跟'--trace'一样，但是没有hex输出 --trace-time 跟踪/详细输出时，添加时间戳 --tr-encoding Request compressed transfer encoding (H) -T, --upload-file FILE 上传文件 --url URL URL to work with -B, --use-ascii 使用ASCII文本传输 -u, --user USER[:PASSWORD] 设置服务端用户和密码 --tlsuser USER TLS用户名 --tlspassword STRING TLS密码 --tlsauthtype STRING TLS认证类型(default SRP) --unix-socket FILE Connect through this UNIX domain socket -A, --user-agent STRING 发送用户代理给服务器 (H) -v, --verbose 获取更多输入输出相关的内容，对于debug非常有用 -V, --version 显示当前的curl版本 -w, --write-out FORMAT 指定完成请求以后输出什么信息 --xattr Store metadata in extended file attributes -q If used as the first parameter disables .curlrc 二、实例详解 1、通过-o/-O选项保存下载的文件到指定的文件中 -o：将文件保存为命令行中指定的文件名的文件中 -O：使用URL中默认的文件名保存文件到本地 # 将文件下载到本地并命名为mygettext.html curl -o mygettext.html http://www.gnu.org/software/gettext/manual/gettext.html # 将文件保存到本地并命名为gettext.html curl -O http://www.gnu.org/software/gettext/manual/gettext.html 2、显示response中的Headers或Body -i：显示response header 和 body -I：只显示response header curl -i https://www.baidu.com curl -I https://www.baidu.com 3、同时获取多个文件 curl -O URL1 -O URL2 4、设置代理 -x：为CURL设置代理 curl -x 192.168.1.2:3128 http://google.com/ 5、允许重定向 -L：允许重定向 curl -L -x 192.168.1.2:3128 http://google.com/ 6、限速 --limit-rate： 对CURL的最大网络使用进行限制 curl --limit-rate 1000B -O http://www.gnu.org/software/gettext/manual/gettext.html 7、添加认证信息 -u: 在访问需要认证的页面时，可通过-u选项提供用户名和密码进行授权 curl -u username:password URL # 通常的做法是在命令行只输入用户名，之后会提示输入密码，这样可以保证在查看历史记录时不会将密码泄露 curl -u username URL 8、获取更多信息 -v 和 -trace：获取更多信息 curl -v -L -x 192.168.1.2:3128 http://google.com/ 9、自定义HTTP请求 -X: 可以指定curl发送HTTP请求的方法，例如GET(默认),PUT,POST,DELETE等 -H：添加请求的Header信息 -d/--data: 添加请求的Body curl -XPUT \"http://127.0.0.1:9200/test/test/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"id\": \"191\", \"prd_id\": \"4\", \"mer_id\": \"1000005\", \"data_status\": \"0\", \"datachange_createtime\": \"1543915326\", \"datachange_lasttime\": \"1543915368\" }' 10、断点续传 -C: 可对大文件使用断点续传功能 curl -C -O http://www.gnu.org/software/gettext/manual/gettext.html 11、模仿浏览器 -A：指定浏览器去访问网站(有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本) curl -A \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)\" http://google.com/ 12、显示下载进度条 -# ：显示下载进度条 curl -# -O http://www.linux.com/dodo1.JPG 13、伪造referer（盗链） 很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了 -e: 设定referer curl -e \"www.linux.com\" http://mail.linux.com # 这样就会让服务器其以为你是从www.linux.com点击某个链接过来的 14、保存与使用Cookie -D: 保存Cookie -b: 使用Cookie # 将网站的cookies信息保存到sugarcookies文件中 curl -D sugarcookies http://localhost/sugarcrm/index.php # 使用上次保存的cookie信息 curl -b sugarcookies http://localhost/sugarcrm/index.php 15、忽略证书不受信问题 -k: 忽略HTTPS证书不受信问题 curl -k https://allinone.okd311.curiouser.com:8443 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-06 09:56:34 "},"origin/linux-lvm.html":{"url":"origin/linux-lvm.html","title":"LVM原理及使用","keywords":"","body":"LVM原理及使用 一、原理简介 LVM是 Logical Volume Manager(逻辑卷管理)的简写，它由Heinz Mauelshagen在Linux 2.4内核上实现。 LVM将一个或多个硬盘的分区在逻辑上集合，相当于一个大硬盘来使用，当硬盘的空间不够使用的时候，可以继续将其它的硬盘的分区加入其中，这样可以实现磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的磁盘存储。它使系统管理员可以更方便的为应用与用户分配存储空间。在LVM管理下的存储卷可以按需要随时改变大小与移除(可能需对文件系统工具进行升级)。LVM也允许按用户组对存储卷进行管理，允许管理员用更直观的名称(如\"sales'、'development')代替物理磁盘名(如'sda'、'sdb')来标识存储卷 LVM功能实际是通过内核中的dm模块（device mapper）实现，它将一个或多个底层块设备组织成一个逻辑设备的模块，在/dev/目录下以dm-#形式展现 只要是块设备都可以用于创建LVM2。注意分区时ID号要是8e 物理存储介质（The physical media）：指系统的存储设备--硬盘，如：/dev/hda1、/dev/sda等等，是存储系统最低层的存储单元 物理卷PV（physical volume）：物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数 卷组VG（volume group）：在较低的逻辑层从多个PV中抽象出来的卷组，由一个或多个物理卷组成 PE（physical extend）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB 逻辑卷LV（logical volume）：由多个LV“块”组成可供挂载使用的设备文件 二、使用步骤 1、安装相关软件包 yum install -y lvm2 2、创建PV pvcreate /dev/sdc pvdisplay \"/dev/sdc\" is a new physical volume of \"100.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdc VG Name PV Size 100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID KiXHSv-PbKj-kOiM-yXhN-ntiw-ULpt-JhvgnB 3、创建VG # vgcreate命令用法 vgcreate -s [N[mgt]] VG名称 PV名称 # -s 指定VG中的PE大小，单位：MB,GB,TB vgcreate -s 16M docker /dev/sdc 4、查看VG vgdisplay --- Volume group --- VG Name docker System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 199.99 GiB PE Size 4.00 MiB Total PE 51198 Alloc PE / Size 38399 / 5、创建LV #lvcreate命令参数 lvcreate -l PE个数 -n LV名称 VG名称 ​ lvcreate -l 6399 -n docker-lib docker 6、查看LV容量 lvdisplay --- Logical volume --- LV Path /dev/docker/docker LV Name docker VG Name docker LV UUID hlbSQl-RfGK-PpUZ-u7Vx-5t3X-WOX7-dxLomX LV Write Access read/write LV Creation host, time node7.test.openshift.com, 2018-09-07 16:11:37 +0800 LV Status available # open 1 LV Size 7、格式化LV mkfs.ext3 LV_Name mkfs.ext4 LV_Name mkfs.xfs LV_Name 8、挂载LV echo \"LV_Name 挂载目录点 文件系统格式 defaults 0 0\" >> /etc/fstab mount -a 三、扩容VG和LV VG已无PE可用 新增硬盘 在线扩容（不卸载umount） 1、创建PV pvcreate /dev/sdd 2、将PV添加到VG中。之后可看PE数量增加 vgextend VG_Name /dev/sdd 3、扩容LV(之后可看LV容量增加) lvresize -l +6399 LV_Name 或者 lvresize -L +50G LV_Name 4、检查并修复文件系统 e2fsck -f LV_Name 5、将扩容后的LV完整地扩充到文件系统中 LV文件系统是ext4时 resize2fs LV_Name LV文件系统是xfs时 xfs_growfs LV_Name Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/linux-交换分区.html":{"url":"origin/linux-交换分区.html","title":"Linux交换分区","keywords":"","body":"交换分区概念及管理 一、什么是交换分区呢？ Linux divides its physical RAM (random access memory) into chucks of memory called pages. Swapping is the process whereby a page of memory is copied to the preconfigured space on the hard disk, called swap space, to free up that page of memory. The combined sizes of the physical memory and the swap space is the amount of virtual memory available. Swap space in Linux is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space. While swap space can help machines with a small amount of RAM, it should not be considered a replacement for more RAM. Swap space is located on hard drives, which have a slower access time than physical memory.Swap space can be a dedicated swap partition (recommended), a swap file, or a combination of swap partitions and swap files. Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。 二、Swap交换分区对性能的影响 我们知道Linux可以使用文件系统中的一个常规文件或独立分区作为Swap交换空间，相对而言，交换分区要快一些。但是和RAM比较而言，Swap交换分区的性能依然比不上物理内存，目前的服务器上RAM基本上都相当充足，那么是否可以考虑抛弃Swap交换分区，是否不需要保留Swap交换分区呢？这个其实是我的疑问之一。在这篇What Is a Linux SWAP Partition, And What Does It Do?博客中，作者给出了swap交换空间的优劣 Advantages: Provides overflow space when your memory fills up completely Can move rarely-needed items away from your high-speed memory Allows you to hibernate Disadvantages: Takes up space on your hard drive as SWAP partitions do not resize dynamically Can increase wear and tear to your hard drive Does not necessarily improve performance (see below) 其实保留swap分区概括起来可以从下面来看： 首先，当物理内存不足以支撑系统和应用程序（进程）的运作时，这个Swap交换分区可以用作临时存放使用率不高的内存分页，把腾出的内存交给急需的应用程序（进程）使用。有点类似机房的UPS系统，虽然正常情况下不需要使用，但是异常情况下， Swap交换分区还是会发挥其关键作用。 其次，即使你的服务器拥有足够多的物理内存，也有一些程序会在它们初始化时残留的极少再用到的内存分页内容转移到 swap 空间，以此让出物理内存空间。对于有发生内存泄漏几率的应用程序（进程），Swap交换分区更是重要，因为谁也不想看到由于物理内存不足导致系统崩溃。 最后，现在很多个人用户在使用Linux，有些甚至是PC的虚拟机上跑Linux系统，此时可能常用到休眠（Hibernate），这种情况下也是推荐划分Swap交换分区的。 其实少量使用Swap交换空间是不会影响性能，只有当RAM资源出现瓶颈或者内存泄露，进程异常时导致频繁、大量使用交换分区才会导致严重性能问题。另外使用Swap交换分区频繁，还会引起kswapd0进程（虚拟内存管理中, 负责换页的）耗用大量CPU资源，导致CPU飙升。 关于Swap分区的优劣以及是否应该舍弃，我有点恶趣味的想到了这个事情：人身上的两个器官，阑尾和扁桃体。切除阑尾或扁桃体是否也是争论不休。另外，其实不要Swap交换分区，Linux也是可以正常运行的（有人提及过这个问题） 三、Swap分区大小设置建议 系统的Swap分区大小设置多大才是最优呢？ 关于这个问题，应该说只能有一个统一的参考标准，具体还应该根据系统实际情况和内存的负荷综合考虑，像ORACLE的官方文档就推荐如下设置，这个是根据物理内存来做参考的。 RAM Swap Space Up to 512 MB 2 times the size of RAM Between 1024 MB and 2048 MB 1.5 times the size of RAM Between 2049 MB and 8192 MB Equal to the size of RAM More than 8192 MB 0.75 times the size of RAM 另外在其它博客中看到下面一个推荐设置，当然我不清楚其怎么得到这个标准的。是否合理也无从考证。可以作为一个参考。 4G以内的物理内存，SWAP 设置为内存的2倍。 4-8G的物理内存，SWAP 等于内存大小。 8-64G 的物理内存，SWAP 设置为8G。 64-256G物理内存，SWAP 设置为16G。 四、什么时候使用Swap分区空间? 系统在什么情况或条件下才会使用Swap分区的空间呢？ 其实是Linux通过一个参数swappiness来控制的。当然还涉及到复杂的算法。 这个参数值可为 0-100，控制系统 swap 的使用程度。高数值可优先系统性能，在进程不活跃时主动将其转换出物理内存。低数值可优先互动性并尽量避免将进程转换处物理内存，并降低反应延迟。默认值为 60。注意：这个只是一个权值，不是一个百分比值，涉及到系统内核复杂的算法。关于该参数请参考这篇文章[转载]调整虚拟内存，在此不做过多赘述。下面是关于swappiness的相关资料 The Linux 2.6 kernel added a new kernel parameter called swappiness to let administrators tweak the way Linux swaps. It is a number from 0 to 100. In essence, higher values lead to more pages being swapped, and lower values lead to more applications being kept in memory, even if they are idle. Kernel maintainer Andrew Morton has said that he runs his desktop machines with a swappiness of 100, stating that \"My point is that decreasing the tendency of the kernel to swap stuff out is wrong. You really don't want hundreds of megabytes of BloatyApp's untouched memory floating about in the machine. Get it out on the disk, use the memory for something useful.\" Swappiness is a property of the Linux kernel that changes the balance between swapping out runtime memory, as opposed to dropping pages from the system page cache. 有两种临时修改swappiness参数的方法，系统重启后失效 方法1： echo 10 > /proc/sys/vm/swappiness 方法2: sysctl vm.swappiness=10 永久修改swappiness参数的方法就是在配置文件/etc/sysctl.conf里面修改vm.swappiness的值，然后重启系统 echo 'vm.swappiness=10' >>/etc/sysctl.conf 如果有人会问是否物理内存使用到某个百分比后才会使用Swap交换空间，可以明确的告诉你不是这样一个算法，如下截图所示，及时物理内存只剩下8M了，但是依然没有使用Swap交换空间，而另外一个例子，物理内存还剩下19G，居然用了一点点Swap交换空间。 五、交换分区管理 1、查看Swap分区大小 $ free -mh total used free shared buff/cache available Mem: 31G 21G 256M 8.4M 9.6G 9.4G Swap: 4.0G 0B 4.0G $ swapon -s 或者 cat /proc/swaps Filename Type Size Used Priority /dev/vdb partition 4194300 0 -1 2、释放Swap分区空间 swapon -s 3、使用swapoff关闭交换分区 swapoff /dev/vdb 4、使用swapon启用交换分区 swapon /dev/vdb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/linux-硬盘读写性能测试.html":{"url":"origin/linux-硬盘读写性能测试.html","title":"Linux硬盘读写性能测试","keywords":"","body":"Linux硬盘读写性能测试 Linux下可使用dd命令来测试硬盘读写速度 一、语法简介 dd if=path/to/input_file of=/path/to/output_file bs=block_size count=number_of_blocks ​参数 if=file 　　　　　　　　　　　　　　　　输入文件名，缺省为标准输入 of=file 　　　　　　　　　　　　　　　　输出文件名，缺省为标准输出 ibs=bytes 　　　　　　　　　　　　　　　一次读入 bytes 个字节(即一个块大小为 bytes 个字节) obs=bytes 　　　　　　　　　　　　　　　一次写 bytes 个字节(即一个块大小为 bytes 个字节) bs=bytes 　　　　　　　　　　　　　　　 同时设置读写块的大小为 bytes ，可代替 ibs 和 obs cbs=bytes 　　　　　　　　　　　　　　　一次转换 bytes 个字节，即转换缓冲区大小 skip=blocks 　　　　　　　　　　　　　 从输入文件开头跳过 blocks 个块后再开始复制 seek=blocks 　　　　　　　　　　 从输出文件开头跳过 blocks 个块后再开始复制(通常只有当输出文件是磁盘或磁带时才有效) count=blocks 　　　　　　　　　　　　　仅拷贝 blocks 个块，块大小等于 ibs 指定的字节数 conv=conversion[,conversion...] 用指定的参数转换文件。 iflag=FLAGS　　　　　　　　　　　　　　指定读的方式FLAGS，参见“FLAGS参数说明” oflag=FLAGS　　　　　　　　　　　　　　指定写的方式FLAGS，参见“FLAGS参数说明” ​ #conv 转换参数： ascii 　　　　　　　　　　　　　　　　　转换 EBCDIC 为 ASCII ebcdic 　　　　　　　　　　　　 　 转换 ASCII 为 EBCDIC ibm 　　　　　　　　　　　　　　　　　　转换 ASCII 为 alternate EBCDIC block 　　　　　　　　　　　　　　　　 把每一行转换为长度为 cbs 的记录，不足部分用空格填充 unblock 　　　　　　　　　　　　　　　 使每一行的长度都为 cbs ，不足部分用空格填充 lcase 　　　　　　　　　　　　　　　　 把大写字符转换为小写字符 ucase 　　　　　　　　　　　　　　　　 把小写字符转换为大写字符 swab 　　　　　　　　　　　　　　　　 交换输入的每对字节 noerror 　　　　　　　　　　　　　　　 出错时不停止 notrunc 　　　　　　　　　　　　　　　 不截短输出文件。 sync 　　　　　　　　　　　　　　　　　 把每个输入块填充到ibs个字节，不足部分用空(NUL)字符补齐 FLAGS 参数说明：​ append -append mode (makes sense only for output; conv=notrunc sug-gested) direct　　　　　　　　　　　　　　　 读写数据采用直接IO方式 directory　　　　　　　　　　　　　　读写失败除非是directory dsync　　　　　　　　　　　　　　　　 读写数据采用同步IO sync　　　　　　　　　　　　　　　　　同上，但是针对是元数据 fullblock　　　　　　　　　　　　　　堆积满block（accumulate full blocks of input ）(iflag only) nonblock　　　　　　　　　　　　　　 读写数据采用非阻塞IO方式 noatime　　　　　　　　　　　　　　　 读写数据不更新访问时间 二、time+dd 测磁盘读写速度 1、相关参数 time有计时作用，dd用于复制，从if读出，写到of if=/dev/zero（产生字符）不产生IO，因此可以用来测试纯写速度 同理of=/dev/null（回收站、无底洞）不产生IO，可以用来测试纯读速度 将/tmp/test拷贝到/var则同时测试了读写速度 bs是每次读或写的大小，即一个块的大小，count是读写块的数量 当写入到驱动盘的时候，我们简单的从无穷无用字节的源 /dev/zero 读取，当从驱动盘读取的时候，我们读取的是刚才的文件，并把输出结果发送到无用的 /dev/null。在整个操作过程中， DD 命令会跟踪数据传输的速度并且报告出结果。 2、测试磁盘写能力 time dd if=/dev/zero of=/testw.dbf bs=4k count=100000 因为/dev//zero是一个伪设备，它只产生空字符流，对它不会产生IO，所以，IO都会集中在of文件中，of文件只用于写，所以这个命令相当于测试磁盘的写能力。命令结尾添加oflag=direct将跳过内存缓存，添加oflag=sync将跳过hdd缓存。 3、测试磁盘读能力 time dd if=/dev/sdb of=/dev/null bs=4k 因为/dev/sdb是一个物理分区，对它的读取会产生IO，/dev/null是伪设备，相当于黑洞，of到该设备不会产生IO，所以，这个命令的IO只发生在/dev/sdb上，也相当于测试磁盘的读能力。（Ctrl+c终止测试） 4、测试同时读写能力 time dd if=/dev/sdb of=/testrw.dbf bs=4k 在这个命令下，一个是物理分区，一个是实际的文件，对它们的读写都会产生IO（对/dev/sdb是读，对/testrw.dbf是写），假设它们都在一个磁盘中，这个命令就相当于测试磁盘的同时读写能力。 5、测试纯写入性能 dd if=/dev/zero of=test bs=8k count=10000 oflag=direct 6、测试纯读取性能 dd if=test of=/dev/null bs=8k count=10000 iflag=direct 注意：dd 只能提供一个大概的测试结果，而且是连续 I/O 而不是随机 I/O，理论上文件规模越大，测试结果越准确。 同时，iflag/oflag 提供 direct 模式，direct 模式是把写入请求直接封装成 I/O 指令发到磁盘，非 direct 模式只是把数据写入到系统缓存就认为 I/O 成功，并由操作系统决定缓存中的数据什么时候被写入磁盘。 参考链接 http://www.360doc.com/content/15/0906/17/8737500_497292503.shtml Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-06 13:49:26 "},"origin/vim-小技巧.html":{"url":"origin/vim-小技巧.html","title":"Vim小技巧","keywords":"","body":"vim常用配置 bash -c 'cat > ~/.vimrc 命令行模式 功能 命令 描述 水平分屏 ：sp 水平分屏打开另一个文件 垂直分屏 ：vsp 垂直分屏打开另一个文件 多屏间切换 Ctrl+W+W 多屏退出 ：qall 排序 ：sort 去除重复行 ：sort u 移动到文本开头 gg 移动到文本结尾 G 移动到行首 0 即行首有空格的情况，会移动到空格之前 移动到行末 $ 即行末有空格的情况，会移动到空格之后 向下翻页 Ctrl+f 向上翻页 Ctrl+b 以word为单位移动 单词数+W/b,B/b,E/e 2w表示向后移动2个word； 2b表示向前移动2个word； 2e表示向后移动2个word(但是会移动到word字符之后) 如果想忽略标点符号的word，就用大写 W B E 行内查找字符 f+字符 向后移动到某字符 F+字符 向前移动到字符a处 全文查找当前光标处的单词 * 向后查找 #+字符 从文件开头到文件尾开始查找匹配字符 ?+字符 从文件尾倒着到文件开头开始查找匹配字符 光标右边最近数字进行自加 Ctrl+A 光标右边最近数字进行自减 Ctrl+X 删除文本中的空行 ：g/^$/d 注释文本行 v进入视图模式，选择要注释的行，然后Ctrl+v进入块选择模式，然后大写I插入#或者/，再ESC退出 ：起始行号,结束行号s/^注释符//g 在10 - 20行添加 // 注释 :10,20s#^#//#g 在10 - 20行添加 # 注释 :10,20s/^/#/g 快速搜索光标所在单词 Shift+* 显示匹配个数 :%s/xxx//gn 插入模式 功能 命令 描述 删除光标前面的单词 Ctrl+W 删除光标前面的一行 Ctrl+U 在光标前面插入一个tab Ctrl+I 将光标以下所有内容向上提 Ctrl+H 将光标以下所有内容向下提 Ctrl+J/M 向下联想 Ctrl+N 向上联想 Ctrl+P 示例 1、行首或行尾加字符 #每行行首加“#” :%s/^/#/g #每行行尾加\" ;\\\" :%s/$/ ;\\\\/g #第二行到第十五行的行首添加“==” :2,15 s/^/==/g #第二行到文本末行的行首添加“==” :2,$ s/^/==/g #第二行到文本首行的行首添加“==” :2,1 s/^/==/g 2、将文本中相同数字进行自增 原始文本 docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 1.tar docker.io/skydive/skydive:latest \\ :g/1.tar/ s//\\=line('.').'.tar'/ 效果文本 docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 2.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 3.tar docker.io/skydive/skydive:latest ;\\ 3、去除文本中的换行符^M Windows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误 进入命令模式 %s/^M//g (注意，^M = Ctrl v + Ctrl m，而不是手动输入^M) # ^M 表示清除成功 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-16 11:56:21 "},"origin/linux-yum.html":{"url":"origin/linux-yum.html","title":"Yum详解","keywords":"","body":"YUM详解 一、Overviews Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 Yum的关键之处是要有可靠的repository，顾名思义这就是软件的仓库，它可以是http或者ftp站点，也可以是本地的软件池，但是必须包含rpm的header，rmp的header包括了rmp的各种信息，包括描述、功能、提供的文件、依赖性等，正是收集了这些信息，才能自动化的完成余下的任务。 repo文件是yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的细节内容 RPM包的名称规则示例：ttpd-manual- 2.0.40-21.i386.rpm，ttp-manual是软件包的名称，2是主版本号；0是次版本号；40是修正号；21是编译的次数；i386是适合的平台 二、YUM命令详解 yum的命令形式一般是如下：yum [选项] [参数] [package ...] 选项 -h, --help 显示此帮助消息并退出 -t, --tolerant 忽略错误 -C, --cacheonly 完全从系统缓存运行，不升级缓存 -c [config file], --config=[config file] 配置文件路径 -R [minutes], --randomwait=[minutes] 命令最长等待时间 -d [debug level], --debuglevel=[debug level] 调试输出级别 --showduplicates 在 list/search 命令下，显示源里重复的条目 -e [error level], --errorlevel=[error level] 错误输出级别 --rpmverbosity=[debug level name] RPM 调试输出级别 -q, --quiet 静默执行 -v, --verbose 详尽的操作过程 -y, --assumeyes 回答全部问题为是 --assumeno 回答全部问题为否 --version 显示 Yum 版本然后退出 --installroot=[path] 设置安装根目录 --enablerepo=[repo] 启用一个或多个软件源(支持通配符) --disablerepo=[repo] 禁用一个或多个软件源(支持通配符) -x [package], --exclude=[package] 采用全名或通配符排除软件包 --disableexcludes=[repo] 禁止从主配置，从源或者从任何位置排除 --disableincludes=[repo] disable includepkgs for a repo or for everything --obsoletes 更新时处理软件包取代关系 --noplugins 禁用 Yum 插件 --nogpgcheck 禁用 GPG 签名检查 --disableplugin=[plugin] 禁用指定名称的插件 --enableplugin=[plugin] 启用指定名称的插件 --skip-broken 忽略存在依赖关系问题的软件包 --color=COLOR 配置是否使用颜色 --releasever=RELEASEVER 在 yum 配置和 repo 文件里设置 $releasever 的值 --downloadonly 仅下载而不更新 --downloaddir=DLDIR 指定一个其他文件夹用于保存软件包 --setopt=SETOPTS 设置任意配置和源选项 --bugfix Include bugfix relevant packages, in updates --security Include security relevant packages, in updates --advisory=ADVS, --advisories=ADVS Include packages needed to fix the given advisory, in updates --bzs=BZS Include packages needed to fix the given BZ, in updates --cves=CVES Include packages needed to fix the given CVE, in updates --sec-severity=SEVS, --secseverity=SEVS Include security relevant packages matching the severity, in updates 参数 check 检查 RPM 数据库问题 check-update 检查是否有可用的软件包更新 clean 删除缓存数据 deplist 列出软件包的依赖关系 distribution-synchronization 已同步软件包到最新可用版本 downgrade 降级软件包 erase 从系统中移除一个或多个软件包 fs Acts on the filesystem data of the host, mainly for removing docs/lanuages for minimal hosts. fssnapshot Creates filesystem snapshots, or lists/deletes current snapshots. groups 显示或使用、组信息 help 显示用法提示 history 显示或使用事务历史 info 显示关于软件包或组的详细信息 install 向系统中安装一个或多个软件包 langavailable Check available languages langinfo List languages information langinstall Install appropriate language packs for a language langlist List installed languages langremove Remove installed language packs for a language list 列出一个或一组软件包 load-transaction 从文件名中加载一个已存事务 makecache 创建元数据缓存 provides 查找提供指定内容的软件包 reinstall 覆盖安装软件包 repo-pkgs 将一个源当作一个软件包组，这样我们就可以一次性安装/移除全部软件包。 repolist 显示已配置的源 search 在软件包详细信息中搜索指定字符串 shell 运行交互式的 yum shell swap Simple way to swap packages, instead of using shell update 更新系统中的一个或多个软件包 update-minimal Works like upgrade, but goes to the 'newest' package match which fixes a problem that affects your system updateinfo Acts on repository update information upgrade 更新软件包同时考虑软件包取代关系 version 显示机器和/或可用的源版本。 常用命令 清除缓存 yum clean [headers, packages, metadata, dbcache, plugins, expire-cache, rpmdb, all] headers--清除缓存目录(/var/cache/yum)下的 headers packages--清除缓存目录(/var/cache/yum)下的软件包 all--清除所有缓存 yum update与yum upgrade的区别 yum update 只更新软件，不更新内核 yum upgrade 升级所有包，不改变软件设置和系统设置，系统版本升级，内核不改变 三、repo文件 [serverid] #serverid是用于区别各个不同的repository，必须有一个独一无二的名称。若重复了，是前面覆盖后面--还是反过来呢？？？用enabled 测试是后面覆盖前面 name=Some name for this server #name，是对repository的描述，支持像$releasever $basearch这样的变量; name=Fedora Core $releasever - $basearch - Released Updates baseurl=url://path/to/repository/ # 1. 格式: baseurl=url://server1/path/to/repository/, url支持的协议有 http:// ftp:// file://三种。 # 2. baseurl后可以跟多个url，你可以自己改为速度比较快的镜像站，但#baseurl只能有一个 # 3. 其中url指向的目录必须是这个repository header目录的上一级，它也支持$releasever $basearch这样的变量。 gpgcheck=1 exclude=gaim failovermethod=priority #failovermethode有两个选项roundrobin和priority，意思分别是有多个url可供选择时，yum选择的次序，roundrobin是随机选择，如果连接失 败则使用下一个，依次循环，priority则根据url的次序从第一个开始。如果不指明，默认是roundrobin。 enabled=[1 or 0] # 1. 当某个软件仓库被配置成 enabled=0 时，yum 在安装或升级软件包时不会将该仓库做为软件包提供源。使用这个选项，可以启用或禁用软件仓库。 # 2. 通过 yum 的 --enablerepo=[repo_name] 和 --disablerepo=[repo_name] 选项，或者通过 PackageKit 的\"添加/删除软件\"工具，也能够方便地启用和禁用指定的软件仓库 变量解释： # $releasever 发行版的版本，从[main]部分的distroverpkg获取，如果没有，则根据redhat-release包进行判断。 # $arch cpu体系，如i686,athlon等 # $basearch cpu的基本体系组，如i686和athlon同属i386，alpha和alphaev6同属alpha。 四、yum-fastestmirror插件 yum-fastestmirror插件，它会自动选择最快的mirror。它的配置文件/etc/yum/pluginconf.d/fastestmirror.conf，yum镜像的速度测试记录文件/var/cache/yum/x86_64/7/timedhosts.txt** 禁用插件配置 修改插件的配置文件 sed -i 's/enabled=1/enabled=0/' /etc/yum/pluginconf.d/fastestmirror.conf enabled = 1//由1改为0，禁用该插件 修改yum的配置文件 # sed -i 's/plugins=1/plugins=0/' /etc/yum.conf plugins=1 //改为0，不使用插件 五、YUM源的创建 1、使用Nexus的YUN格式仓库作为YUM镜像源 详见：Nexus中yum仓库的配置与使用 2、Createrepo创建本地YUM镜像源 将CentOS版本系统镜像中的Packages并上传到主机上的某一目录下 安装createrepo用来创建软件包的索引。或者将系统镜像中repodata目录放到rpm包路径下 yum install createrepo -y createrepo /data/localrepo/Office 会在创建repodata索引文件夹 在/etc/yum.repos.d/目录下创建repo文件local.repo [local] name=Local Yum Office Repository 仓库名 baseurl=file:///data/localrepo/Office 仓库中rpm包存放路径 gpgcheck=1 是否检查GPG-KEY，0为不检查，1为检查 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 GPG-KEY的存放路径 enabled=1 设置为1表示启用本Repo Createrepo命令参数详解 -u --baseurl 指定Base URL的地址 -o --outputdir 指定元数据的输出位置 -x --excludes 指定在形成元数据时需要排除的包 -i --pkglist 指定一个文件，该文件内的包信息将被包含在即将生成的元数据中，格式为每个包信息独占一行，不含通配符、正则，以及范围表达式。 -n --includepkg 通过命令行指定要纳入本地库中的包信息，需要提供URL或本地路径。 -q --quiet 安静模式执行操作，不输出任何信息。 -g --groupfile 指定本地软件仓库的组划分，范例如下：createrepo -g comps.xml /path/to/rpms注意：组文件需要和rpm包放置于同一路径下。 -v --verbose 输出详细信息。 -c --cachedir 指定一个目录，用作存放软件仓库中软件包的校验和信息。 当createrepo在未发生明显改变的相同仓库文件上持续多次运行时，指定cachedir会明显提高 其性能。 --update 如果元数据已经存在，且软件仓库中只有部分软件发生了改变或增减， 则可用update参数直接对原有元数据进行升级，效率比重新分析rpm包依赖并生成新的元数据要 高很多。 -p --pretty 以整洁的格式输出xml文件。 -d --database 该选项指定使用SQLite来存储生成的元数据，默认项。 3、HTTPD+Createrepo创建YUM镜像源 第二种方法创建的镜像源只能在本地使用，要是能在局域网中提供公共的服务，需要一个能提供HTTP服务的容器，可使用HTTPD（又称Apache），步骤省略。 六、Reposync同步YUM远程仓库的安装包 1、安装 yum install yum-utils -y 2、命令参数 Usage: Reposync is used to synchronize a remote yum repository to a local directory using yum to retrieve the packages. /usr/bin/reposync [options] Options: -h, --help show this help message and exit -c CONFIG, --config=CONFIG config file to use (defaults to /etc/yum.conf) -a ARCH, --arch=ARCH act as if running the specified arch (default: current arch, note: does not override $releasever. x86_64 is a superset for i*86.) --source operate on source packages -r REPOID, --repoid=REPOID secify repo ids to query, can be specified multiple times (default is all enabled) -e CACHEDIR, --cachedir=CACHEDIR directory in which to store metadata -t, --tempcache Use a temp dir for storing/accessing yum-cache -d, --delete delete local packages no longer present in repository -p DESTDIR, --download_path=DESTDIR Path to download packages to: defaults to current dir --norepopath Don't add the reponame to the download path. Can only be used when syncing a single repository (default is to add the reponame) -g, --gpgcheck Remove packages that fail GPG signature checking after downloading -u, --urls Just list urls of what would be downloaded, don't download -n, --newest-only Download only newest packages per-repo -q, --quiet Output as little as possible -l, --plugins enable yum plugin support -m, --downloadcomps also download comps.xml --download-metadata download all the non-default metadata 3、示例 $> bash -c 'cat > ceph.repo 七、Yumdownloadonly下载RPM包及依赖包 1、下载yumdownloadonly插件 yum install yum-plugin-downloadonly 2、下载到指定目录（依赖包会一起下载） yum install --downloadonly --downloaddir=/root/httpd httpd 附录：清华镜像站中常见的软件源 1、VirtualBox [virtualbox] name=Oracle Linux / RHEL / CentOS-$releasever / $basearch - VirtualBox baseurl=http://download.virtualbox.org/virtualbox/rpm/el/$releasever/$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://www.virtualbox.org/download/oracle_vbox.asc 2、Nginx [nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 3、EPEL [epel] name=Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch failovermethod=priority enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 4、Ceph [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 5、ELK Stack [ELK-Stack-5.x] name=ELK Stack repository for 5.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/5.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md [ELK-Stack-6.x] name=ELK Stack repository for 6.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/6.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md 6、MySQL [MySQL-Community-5.6] name=MySQL Community 5.6 baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql56-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 [MySQL-Community-5.7] name=MySQL Community 5.7 baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql57-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 [MySQL-Community-Connectors] name=MySQL Community Connectors baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-connectors-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 [MySQL-Community-Tools] name=MySQL Community Tools baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 7、MongoDB bash -c 'cat > /etc/yum.repos.d/mongoDb.repo Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-09 13:22:40 "},"origin/linux-zsh.html":{"url":"origin/linux-zsh.html","title":"ZSH","keywords":"","body":"Linux zsh && oh-my-zsh 一、简介 Zsh 也许是目前最好用的 shell，是 bash 替代品中较为优秀的一个。 Zsh 官网：http://www.zsh.org/ Zsh具有以下主要优势： 完全兼容bash，之前bash下的使用习惯，shell脚本都可以完全兼容 更强大的tab补全 更智能的切换目录 命令选项、参数补齐 大小写字母自动更正 有着丰富多彩的主题 更强大的alias命令 智能命令错误纠正 集成各种类型的插件 oh-my-zsh 是最为流行的 zsh 配置文件，提供了大量的主题和插件，极大的拓展了 zsh 的功能，推动了 zsh 的流行，有点类似于 rails 之于 ruby。 二、安装zsh CentOS yum install -y zsh Ubuntu apt-get install -y zsh 检查下系统的 shell：$ cat /etc/shells，你会发现多了一个：/bin/zsh 设置用户的默认shell # 给root用户设置 chsh -s /bin/zsh root # 给普通账户设置 chsh -s /bin/zsh 用户名 #　恢复bash chsh -s /bin/bash [user] 三、安装oh-my-zsh oh-my-zsh 帮我们整理了一些常用的 Zsh 扩展功能和主题，我们无需自己去捣搞 Zsh，直接用 oh-my-zsh 就足够了。 oh-my-zsh 官网：https://ohmyz.sh/ oh-my-zsh Github：https://github.com/robbyrussell/oh-my-zsh curl sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" wget sh -c \"$(wget -O- https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 手动 curl -Lo install.sh https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh sh install.sh 四、oh-my-zsh配置 oh-my-zsh的配置文件路径为~/.zshrc # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # 指定oh-my-zsh的安装路径 export ZSH=\"/root/.oh-my-zsh\" # 设置主题。如果设置为\"random\", 每次启动oh-my-zsh会随机加载主题,可使用`echo $RANDOM_THEME`查看每次加载的主题 ZSH_THEME=\"alanpeabody\" # 设置随机的主题 # ZSH_THEME_RANDOM_CANDIDATES=( \"robbyrussell\" \"agnoster\" ) # 大小写是否敏感 # CASE_SENSITIVE=\"true\" # Uncomment the following line to use hyphen-insensitive completion. # Case-sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\"true\" # 设置是否自动更新 # DISABLE_AUTO_UPDATE=\"true\" # 设置自动更新的天数。默认13天 # export UPDATE_ZSH_DAYS=13 # 设置是否开启`ls`进行颜色显示 # DISABLE_LS_COLORS=\"true\" # 设置是否显示终端标题 # DISABLE_AUTO_TITLE=\"true\" # 设置是否开启语法修正 # ENABLE_CORRECTION=\"true\" # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\"true\" # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\"true\" # 历史输入命令的时间展示格式 # HIST_STAMPS=\"mm/dd/yyyy\" # 设置自定义配置文件的路径 # ZSH_CUSTOM=/path/to/new-custom-folder # 配置要加载的插件（配置的插件要能在 ~/.oh-my-zsh/plugins/* 下找到，自定义的插件目录为 ~/.oh-my-zsh/custom/plugins/ ）.注意：插件安装的越多，zsh的启动速度越慢，选择使用率最高的插件才是最好的选择 plugins=( git zsh-autosuggestions zsh-syntax-highlighting kubectl docker sudo extract ) source /etc/profile source $ZSH/oh-my-zsh.sh # 用户配置 # 设置man文档的环境变量 # export MANPATH=\"/usr/local/man:$MANPATH\" # 设置语言环境变量 # export LANG=en_US.UTF-8 # 设置本地和远程sessions的首选编辑器 # if [[ -n $SSH_CONNECTION ]]; then # export EDITOR='vim' # else # export EDITOR='mvim' # fi # 设置编译标志 # export ARCHFLAGS=\"-arch x86_64\" # 设置别名 alias ll=\"ls -alh\" alias k='kubectl' . ~/.oh-my-zsh/custom/oc_zsh_completion 五、oh-my-zsh常用插件 git：可以使用git缩写，默认自带 git add --all => gaa 查看所有缩写：alias | grep git autojump：快速跳转文件夹 last-working-dir：可以记录上一次退出命令行时候的所在路径，并且在下一次启动命令行的时候自动恢复到上一次所在的路径。 wd：快速地切换到常用的目录 wd add web相当于给当前目录做了一个标识，标识名叫做 web ，我们下次如果再想进入这个目录，只需输入：wd web catimg：将图片的内容输出到命令行 catimg demo.jpg zsh-syntax-highlighting：命令高亮 正确路径自带下划线 安装：git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions：自动补全可能的路径 安装：git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions sudo：连按两次Esc添加或去掉sudo extract：功能强大的解压插件 执行x demo.tar.gz git-open：在终端里打开当前项目的远程仓库地址 安装：git clone https://github.com/paulirish/git-open.git $ZSH_CUSTOM/plugins/git-open 六、oh-my-zsh常用主题 官方主题：https://github.com/robbyrussell/oh-my-zsh/wiki/Themes Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-16 12:01:44 "},"origin/正反向代理服务的区别.html":{"url":"origin/正反向代理服务的区别.html","title":"代理服务器","keywords":"","body":"正反向代理服务的区别 一、代理的作用 首先要明确代理服务器的作用: 代理：可以直白地理解为：代理服务器是一种代替谁去访问什么的服务器。代理客户端浏览器去访问客户端浏览器访问不了的服务，代理应用服务器负载均衡地对外提供服务。可以根据代替谁来划分为\"正向代理\"和\"反向代理\" 缓存加速：缓存那些不经常变动的资源，加速访问。 鉴权过滤记录：允许那些认证过的客户端去访问指定的资源或服务，还可以记录下访问记录。 二、正向代理 正向代理（forward proxy）是指代替内部网络的客户端，去访问Internet或其他网络上的服务，并将访问的结果返还给客户端，同时将结果缓存下来，加速访问。 正向代理还可以按客户端是否感知分为透明代理与传统代理。 常见的正向代理服务软件有: Squid Varnish Nginx 三、反向代理 反向代理（Reverse Proxy）方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器；并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 常见的反向代理服务软件有: Nginx Apache HAProxy 未完代待整理更新 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-18 19:49:30 "},"origin/常见正向代理服务软件之间的区别.html":{"url":"origin/常见正向代理服务软件之间的区别.html","title":"正向代理","keywords":"","body":"常见正向代理服务软件的对比区别(挖坑) https://www.zhihu.com/search?type=content&q=%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%A6%82%E5%BF%B5 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-18 19:50:55 "},"origin/squid-简介安装.html":{"url":"origin/squid-简介安装.html","title":"简介安装日志","keywords":"","body":"正向代理服务Squid 一、简介 Squid是一个高性能的正向代理缓存服务器，支持FTP、gopher、HTTPS、HTTP等协议，主要提供了缓存加速、应用层过滤的功能。和一般的代理缓存软件不同，Squid用一个单独的、非模块化的、I/O驱动的进程来处理所有的客户端请求。 squid代理服务器的工作机制： 代理服务器（Proxy Server）是个人网络和Internet服务商之间的中间代理机构，负责转发合法的网络信息，对转发进行控制和登记。其最基本的功能就是连接，此外还包括安全性、缓存，内容过滤，访问控制管理等功能。当客户机通过代理请求Web页面时，执行的代理服务器会先检查自己的缓存，当缓存中有客户机需要访问的页面，则直接将缓存服务器中的页面内容反馈给客户机；如果缓存中没有客户机需要访问的页面，则由代理服务器想Internet发送访问请求，当获得返回的Web页面以后，将页面数据保存到缓存中并发送给客户机。 由于客户机的web访问请求实际上代理服务器来代替完成的，所以隐藏了用户的真实IP地址，从而起到一定的保护作用。 Squid可以基于访问控制列表（ACL）和访问权限列表（ARL）执行内容过滤与权限管理功能，还可以基于多种条件禁止用户访问存在威胁或不适宜的网站资源。 根据实现的方式不同，正向代理模式可以分为： 传统代理：也就是普通的代理服务，需要我们客户端在浏览器、聊天工具等一些程序中设置代理服务器的地址和端口，然后才能使用代理来访问网络，这种方式相比较而言比较麻烦，因为客户机还需手动指定代理服务器，所以一般用于Internet环境。 透明代理：与传统代理实现的功能是一样的，区别在于客户机不需要手动指定代理服务器的地址和端口，而是通过默认路由、防火墙策略将web访问重定向，实际上仍然交给代理服务器来处理，重定向的过程完全是由squid服务器进行的，所以对于客户机来说，甚至不知道自己使用了squid代理服务，因此呢，我们称之为透明模式。透明代理多用于局域网环境，如在Linux网关中启用透明代理后，局域网主机无须进行额外设置就能享受更好的上网速度。 二、安装 YUM yum install squid -y; \\ systemctl enable squid; \\ systemctl start squid 三、传统代理服务配置 配置文件/etc/squid/squid.conf acl localnet src 10.0.0.0/8 # RFC1918 possible internal network acl localnet src 172.16.0.0/12 # RFC1918 possible internal network acl localnet src 192.168.0.0/16 # RFC1918 possible internal network acl localnet src fc00::/7 # RFC 4193 local private network range acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines acl SSL_ports port 443 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025-65535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT http_access allow Safe_ports http_access deny CONNECT !SSL_ports http_access allow localhost manager http_access deny manager http_access allow localnet http_access allow localhost http_access deny all http_port 3128 # Squid代理服务监听端口 cache_dir ufs /data/squid 100 16 256 coredump_dir /data/squid refresh_pattern ^ftp: 1440 20% 10080 refresh_pattern ^gopher: 1440 0% 1440 refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 refresh_pattern . 0 20% 4320 四、常用命令 1、启动等命令 squid reload #不重启服务，生效配置 squid –z #初始化缓存空间 初始化你在 squid.conf 里配置的 cache 目录,只需要第一次的时候执行就可以了 squid -k parse #验证squid.conf的语法和配置 squid -N -d1 #在前台启动squid，并输出启动过程 squid -s #后台运行squid。 squid -k shutdown #停止 squid squid -k reconfigure #载入新的配置文件 squid -k rotate #轮循日志 2、squid命中率分析 # 获取squid运行状态信息 squidclient -p 3128 mgr:info squidclient -p 3128 mgr:5min # 可以看到详细的性能情况,其中PORT是你的proxy的端口，5min可以是60min #获取squid内存使用情况 squidclient -p 3128 mgr:mem #获取squid已经缓存的列表 squidclient -p 3128 mgr:objects use it carefully,it may crash #获取squid的磁盘使用情况 squidclient -p 3128 mgr:diskd #强制更新某个url squidclient -p 3128 -m PURGE http://www.xxx.com/xxx.php #更多的请查看 squidclient -h 或者 squidclient -p 3128 mgr: #查命中率： squidclient -h(具体侦听IP) -p80(具体侦听端口) mgr:info 3、定期清除swap.state内无效数据 当squid应用运行了一段时间以后，cache_dir对应的swap.state文件就会变得越来越大，里面的无效接口数据越来越多，这可能影响squid的响应时间，因此需要使用rotate命令来使squid清理swap.state里面的无效数据，减少swap.state的大小 squid -k rotate -f /path/to/squid/conf_file #添加定时清理任务 vi /etc/crontab 0 0 * * * root /usr/local/sbin/squid -k rotate -f /usr/local/etc/squid/squid1.conf 4、统计客户端个数 netstat -lanp|grep 3128|grep \"ESTABLISHED\"|awk '{print $5}'|awk -F':' '{print $1}'|sort -u|wc -l 5、统计客户端的连接总数 netstat -lanp|grep 3128|grep \"ESTABLISHED\"|wc -l 6、显示传输数据大于指定大小的访问 tailf /var/log/squid/access.log | awk '{if($5>1000)print}'|awk '{print $3 \" \" $5 \" \" $7}' 五、日志默认输出格式 squid日志配置项是在/etc/squid/squid.conf中配置的，默认日志输出文件路径/var/log/squid/access.log 默认的日志输出格式 #1:时间戳 2:响应时间 3:客户端IP 4:结果/状态码 5:传输大小 6:请求方式 7:客户端请求的URL 8:客户端身份 9:对端编码/对端主机 10:内容类型 1531077064.951 81 10.248.2.67 TCP_MISS/200 6277 GET http://bbs.talkop.com/forum.php? - HIER_DIRECT/180.76.184.69 text/xml 时间戳（%tl %ts）: 请求完成时间，以 Unix 时间来记录的（UTC 1970-01-01 00:00:00 开始的时间）它是毫秒级的。squid使用这种格式而不是人工可读的时间格式，是为了简化某些日志处理程序的工作 响应时间（%6tr）: 对HTTP响应来说，该域表明squid花了多少时间来处理请求。在squid接受到HTTP请求时开始计时，在响应完全送出后计时终止。响应时间是毫秒级的。尽管时间值是毫秒级的，但是精度可能是10毫秒。在squid负载繁重时，计时变得没那么精确 客户端地址（%>a）: 该域包含客户端的IP地址，或者是主机名 结果/状态码（%Ss/%03Hs）: 该域包含2个 token，以斜杠分隔。第一个token叫结果码，它把协议和响应结果（例如TCPHIT或UDP_DENIED）进行归类。这些是squid专有的编码，以TCP开头的编码指HTTP请求，以UDP_开头的编码指ICP查询。第2个token是HTTP响应状态码（例如200,304,404等）。状态码通常来自原始服务器。在某些情形下，squid可能有义务自己选择状态码 传输size（%: 该域指明传给客户端的字节数。严格的讲，它是squid告诉TCP/IP协议栈去发送给客户端的字节数。这就是说，它不包括TCP/IP头部的overhead。也请注意，传输size正常来说大于响应的Content-Length。传输size包括了HTTP响应头部，然而Content- Length不包括 请求方式（%rm）: 该域包含请求方式 URI（%ru）: 该域包含来自客户端请求的URI。大多数记录下来的URI实际是URL（例如，它们有主机名）。在记日志时，squid删掉了在第一个问号(?)之后的所有URI字符，除非禁用了strip_query_terms指令 客户端身份: 无 对端编码/对端主机: 对端信息包含了2个token，以斜杠分隔。它仅仅与cache 不命中的请求有关。第一个token指示如何选择下一跳，第二个token是下一跳的地址。当squid发送一个请求到邻居cache时，对端主机地址是邻居的主机名。假如请求是直接送到原始服务器的，则squid会写成原始服务器的IP地址或主机名–假如禁用了log_ip_on_direct。NONE/-这个值指明squid不转发该请求到任何其他服务器 内容类型（%mt）: 原始access.log的默认的最后一个域，是HTTP响应的内容类型。 squid从响应的Content-Type头部获取内容类型值。假如该头部丢失了，squid使用一个横杠(-)代替 假如激活了 log_mime_hdrs 指令，squid在每行追加2个附加的域： HTTP请求头部: Squid 编码HTTP请求头部，并且在一对方括号之间打印它们。方括号是必须的，因为squid不编码空格字符。编码方案稍许奇怪。回车（ASCII 13）和换行（ASCII 10）分别打印成\\r和\\n。其他不可打印的字符以RFC 1738风格来编码，例如Tab（ASCII 9）变成了%09。 HTTP响应头部: Squid编码HTTP响应头部，并且在一对方括号之间打印它们。注意这些是发往客户端的头部，可能不同于从原始服务器接受到的头部。 参考链接 http://www.squid-cache.org/ https://blog.51cto.com/10693404/2149207 https://blog.51cto.com/14154700/2406060 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/squid-acl访问权限控制.html":{"url":"origin/squid-acl访问权限控制.html","title":"ACL访问权限","keywords":"","body":"一、ACL概念 Squid提供了强大的代理控制机制，通过合理设置ACL（Access Control List，访问控制列表）并进行限制，可以针对源地址、目标地址、访问的URL路径、访问的时间等各种条件进行过滤。 ACL访问控制的步骤： 使用acl配置项定义需要控制的条件 通过http_access配置项对已定义的列表做“允许”或“拒绝”访问的控制 二、ACL用法概述 1、定义ACL访问列表 定义格式： acl aclname acltype string1… #acl 列表名称 列表类型 列表内容 ... acl aclname acltype \"File_Path\"… #acl 列表名称 列表类型 \"文件路径\" ... #当使用文件时，该文件的格式为每行包含一个条目。 常用的ACL列表类型: src：指明源地址 acl aclname src ip-address/netmask ... 客户ip地址 acl aclname src addr1-addr2/netmask ... 地址范围 dst：指明目标地址，即客户请求的服务器的IP地址。语法为： acl aclname dst ip-address/netmask ... srcdomain：指明客户所属的域，Squid将根据客户IP反向查询DNS。语法为： acl aclname srcdomain foo.com ... dstdomain：指明请求服务器所属的域，由客户请求的URL决定。语法为： acl aclname dstdomain foo.com ... 此处需要注意的是：如果用户使用服务器IP而非完整的域名时，Squid将进行反向的DNS解析来确定其完整域名，如果失败，就记录为“none”。 time：指明访问时间。语法如下： acl aclname time [day-abbrevs] [h1:m1-h2:m2][hh:mm-hh:mm] 日期的缩写指代关系如下： S：指代Sunday M：指代Monday T：指代Tuesday W：指代Wednesday H：指代Thursday F：指代Friday A：指代Saturday 另外，h1：m1必须小于h2：m2，表达式为[hh：mm-hh：mm]。 port：指定访问端 acl aclname port 80 70 21 ... acl aclname port 0-1024 ... 指定一个端口范围 method：指定请求方法。比如： acl aclname method GET POST ... url_regex：URL规则表达式匹配，语法为： acl aclname url_regex[-i] pattern urlpath_regex：URL-path规则表达式匹配，略去协议和主机名。其语法为： acl aclname urlpath_regex[-i] pattern Notes： acltype可以是任一个在ACL中定义的名称。 任何两个ACL元素不能用相同的名字。 每个ACL由列表值组成。当进行匹配检测的时候，多个值由逻辑或运算连接；换句话说，任一ACL元素的值被匹配，则这个ACL元素即被匹配。 并不是所有的ACL元素都能使用访问列表中的全部类型。 不同的ACL元素写在不同行中，Squid将这些元素组合在一个列表中。 2、http_access访问控制列表使用访问控制 根据访问控制列表允许或禁止某一类用户访问。如果某个访问没有相符合的项目，则默认为应用最后一条项目的“非”。比如最后一条为允许，则默认就是禁止。通常应该把最后的条目设为“deny all”或“allow all”来避免安全性隐患。使用该访问控制列表要注意如下问题： 这些规则按照它们的排列顺序进行匹配检测，一旦检测到匹配的规则，匹配检测就立即结束。 访问列表可以由多条规则组成。 如果没有任何规则与访问请求匹配，默认动作将与列表中最后一条规则对应。 一个访问条目中的所有元素将用逻辑与运算连接（如下所示）： http_access Action声明1 AND 声明2 AND 多个http_access声明间用或运算连接，但每个访问条目的元素间用与运算连接。 列表中的规则总是遵循由上而下的顺序。 三、ACL示例 允许网段10.0.0.124/24以及192.168.10.15/24内的所有客户机访问代理服务器，并且允许在文件/etc/squid/guest列出的客户机访问代理服务器，除此之外的客户机将拒绝访问本地代理服务器： acl clients src 10.0.0.124/24 192.168.10.15/24 acl guests src “/etc/squid/guest” acl all src 0.0.0.0/0.0.0.0 http_access allow clients http_access allow guests http_access deny all 其中，文件“/etc/squid/guest”中的内容为： 172.168.10.3/24 210.113.24.8/16 10.0.1.24/25 允许域名为job.net、gdfq.edu.cn的两个域访问本地代理服务器，其他的域都将拒绝访问本地代理服务器： acl permitted_domain src job.net gdfq.edu.cn acl all src 0.0.0.0/0.0.0.0 http_access allow permitted_domain http_access deny all 使用正则表达式，拒绝客户机通过代理服务器访问包含有诸如“sexy”等关键字的网站： acl deny_url url_regex -i sexy http_access deny deny_url 拒绝客户机通过代理服务器访问文件中指定IP或者域名的网站，其中文件/etc/squid/ deny_ip中存放有拒绝访问的IP地址，文件/etc/squid/deny_dns中存放有拒绝访问的域名： acl deny_ip dst “etc/squid/deny_ip” acl deny_dns dst “etc/squid/deny_dns” http_access deny deny_ip http_access deny deny_dns 允许和拒绝指定的用户访问指定的网站，其中，允许客户1访问网站http://www.sina.com.cn，而拒绝客户2访问网站http://www.163.com： acl client1 src 192.168.0.118 acl client1_url url_regex ^http://www.sina.com.cn acl client2 src 192.168.0.119 acl client2_url url_regex ^http://www.163.com http_access allow client1 client1_url http_access deny client2 client2_url 允许所有的用户在规定的时间内（周一至周四的8：30到20：30）访问代理服务器，只允许特定的用户（系统管理员，其网段为：192.168.10.0/24）在周五下午访问代理服务器，其他的在周五下午一点至六点一律拒绝访问代理服务器： acl allclient src 0.0.0.0/0.0.0.0 acl administrator 192.168.10.0/24 acl common_time time MTWH 8:30-20:30 acl manage_time time F 13:00-18:00 http_access allow allclient common_time http_access allow administrator manage_time http_access deny manage_time Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-10 14:56:42 "},"origin/常见反向代理服务软件之间的区别.html":{"url":"origin/常见反向代理服务软件之间的区别.html","title":"反向代理","keywords":"","body":"常见反向代理服务软件的对比区别(挖坑) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-18 10:08:23 "},"origin/iSCSI-简介配置使用.html":{"url":"origin/iSCSI-简介配置使用.html","title":"群晖Synology的iSCSI","keywords":"","body":"iSCSI的简介配置使用 一、iSCSI简介 iSCSI（Internet Small Computer System Interface），Internet小型计算机系统接口，又称为IP-SAN，是由IBM 下属的两大研发机构一一加利福尼亚AImaden和以色列Haifa研究中心共同开发的，是一个供硬件设备使用的、可在IP协议上层运行的SCSI指令集，是一种开放的基于IP协议的工业技术标准。一种基于因特网及SCSI-3协议下的存储技术，于2003年2月11日成为正式的标准 iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换SCSI命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降 两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存装置 iSCSI target：就是iSCSI的server，可以是一个物理磁阵；也可以是软件实现的iSCSI server。有硬件方式实现的iSCSI target，例如iSCSI的hba卡和带isoe（iSCSI offload engine）网卡（硬件上将iSCSI 包接包和封包） iSCSI initiator：就是iSCSI的客户端，它可以是一个软件，也可以是一个硬件。如果是软件在linux上，用户态实现的是tgt框架（linux scsi target frame）；还有一种内核太实现的架构是iet（iSCSI enterprise target）。在centos上目前已经默认安装了tgt了。 iqn（iSCSI qualified name）：initiator和target通过iqn号来逻辑寻址。一个iqn号由四部分组成： iqn.日期.域名:域名组织分配的名字 例如：iqn.2000-01.com.synology:Synology.Target-1.826d6a066b LUN：全称是Logical Unit Number，中文名是逻辑单元号。LUN是在存储设备上可以被应用服务器识别的独立存储单元。一个LUN的空间来源于存储池Pool，Pool的空间来源于组成磁盘阵列的若干块硬盘。从应用服务器的角度来看，一个LUN可以被视为一块可以使用的硬盘。例如，在Linux系统中，它在/dev/rdsk、/dev/dsk目录下有相应的设备名称；在Windows系统中，格式化后的新LUN会对应一个类似于D E F的盘符。 Thick LUN：中文名是传统非精简LUN，是LUN类型的一种，支持虚拟资源分配，能以较为简便的方式进行创建、扩容和压缩操作。Thick LUN在创建完成后就会从存储池Pool中分配满额的存储空间，即LUN的大小完全等于分配的空间。因此，它拥有较高的可预测性。 Thin LUN：中文名是精简LUN，也是LUN类型的一种，支持虚拟资源分配，能够以较简便的方式进行创建、扩容和压缩操作。Thin LUN在创建的时候，可以设置一个初始分配容量。创建完成后，存储池Pool只会分配这个初始容量大小的空间剩余的空间仍然放在存储池中。当Thin LUN已分配的存储空间的使用率达到阈值时，存储系统才会再从Pool中划分一定的配额给Thin LUN。如此反复，直到达到Thin LUN最初设定的全部容量。因此，它拥有较高的存储空间利用率。 二、群晖Synology的iSCSI存储 创建LUN 创建Target Target关联LUN 三、Windows挂载 参考链接 https://jingyan.baidu.com/article/e4511cf37feade2b845eaff8.html https://blog.csdn.net/M_joy666/article/details/80566705 附录：Thick LUN与Thin LUN的区别 1、空间分配上的区别 Thick LUN在创建时会分配所有需要的空间 Thin LUN是一种按需分配的空间组织方法，它在创建时存储池不会分配所有需要的空间，而是根据使用情况动态分配。二者的空间分配区别如下图所示： 2、空间回收的区别 注：这里的空间回收指的是释放存储池Pool中的资源，并且这些资源可以被其他LUN使用。 Thick LUN没有空间回收的概念，因为它在创建时就占用存储池中所有分配给它的空间，即使Thick LUN中的数据被删除，存储池中分配给它的空间还是被占用，不能被其他的LUN使用。但是如果手动删除不再使用的Thick LUN，则对应的空间会被回收。 Thin LUN不仅能够做到空间占用率增大时自动分配新的存储空间，而且当Thin LUN中的文件删除时也可以实现空间的释放，从而实现存储空间的反复利用，大大提高存储空间的利用率。Thin LUN的空间回收如下图所示： 3、性能的区别 Thick LUN由于在一开始就会拥有所分配的空间，所以Thick LUN在顺序读写的时候拥有较高的性能，但是会造成空间资源的浪费。 Thin LUN由于是实时分配空间，每次扩容时，需要重新增加容量，后台重新格式化，这个时候性能会受到一定影响，而且每次分配空间可能会导致硬盘中存储空间不连续，这样硬盘读写数据时在寻找存放位置上花费的时间会较多，会在顺序读写时对性能有一定影响。 4、使用场景的区别 Thick LUN： ①对性能要求较高的场景 ②对存储空间利用率不太敏感的场景 ③对成本要求不太高的场景 Thin LUN： ①对性能要求一般的场景； ②对存储空间利用率比较敏感的场景； ③对成本比较敏感的场景； ④应用环境很难预 估存储空间的场景 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-09-25 18:12:48 "},"origin/gitbook-简介安装配置.html":{"url":"origin/gitbook-简介安装配置.html","title":"GitBook","keywords":"","body":"GitBook简介安装配置 一、GitBook简介 gitbook 是一个基于node.js的命令行工具 gitbook 支持markdown/asciiDoc语法格式构建书籍 gitbook 支持输出静态网页（可定制和可扩展）和电子书（PDF，ePub或Mobi）等多种格式，其中默认输出静态网页格式 gitbook 不仅支持本地构建书籍,还可以托管在gitbook 官网上，或者Github上 二、GitBook安装 1、安装NodeJs环境 NodeJs官网下载链接:https://nodejs.org/en/download/ Linux 以安装NodeJs 10.16.3为例 wget https://nodejs.org/dist/v10.16.3/node-v10.16.3-linux-x64.tar.xz && \\ tar -xvf node-v10.16.3-linux-x64.tar.xz -C /opt/ && \\ rm -rf node-v10.16.3-linux-x64.tar.xz && \\ ln -s /opt/node-v10.16.3-linux-x64 /opt/nodejs && \\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile && \\ source /etc/profile && \\ yum install gcc-c++ make -y && \\ npm config set registry https://registry.npm.taobao.org && \\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ && \\ node -v && \\ npm version Windows 直接在官网下载MSI格式的安装包进行安装 2、安装Gitbook CLI命令行工具 gitbook-cli 是 gitbook 的一个命令行工具, 通过它可以在电脑上安装和管理多个版本的gitbook. npm install gitbook-cli -g 三、GitBook版本的管理 gitbook-cli 和 gitbook 是两个软件，gitbook-cli 会将下载的 gitbook 的不同版本放到 ~/.gitbook中, 可以通过设置GITBOOK_DIR环境变量来指定另外的文件夹 GitBook可以在本地安装多个版本并在执行命令的时候指定某个版本，如果指定的版本还没安装就会自动下载安装，下载后的GitBook会被放到~/.gitbook目录下。 $ gitbook --help Usage: gitbook [options] [command] Options: -v, --gitbook [version] specify GitBook version to use -d, --debug enable verbose error -V, --version Display running versions of gitbook and gitbook-cli -h, --help output usage information Commands: ls List versions installed locally current Display currently activated version ls-remote List remote versions available for install fetch [version] Download and install a alias [folder] [version] Set an alias named pointing to uninstall [version] Uninstall a version update [tag] Update to the latest version of GitBook help List commands for GitBook * run a command with a specific gitbook version # 查看当前GitBook CLI版本 gitbook -V # 列出本地安装版本 gitbook ls # 列出当前使用版本 gitbook current # 列出远程可用版本 gitbook ls-remote # 安装指定版本(如果安装比较慢的话，将npm镜像源切到国内的CNPM镜像源。可使用NRM管理NPM的镜像源) gitbook fetch [version] # 卸载指定版本 gitbook uninstall [version] # 更新指定版本 gitbook update [tag] 四、GitBook CLI命令 1、gitbook 可用命令 $ gitbook help build [book] [output] 构建书籍 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) --[no-]timing Print timing debug information (Default is false) serve [book] [output] serve the book as a website for testing --port 指定监听端口(默认端口4000) --lrport Port for livereload server to listen on (Default is 35729) --[no-]watch Enable file watcher and live reloading (Default is true) --[no-]live Enable live reloading (Default is true) --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) install [book] 安装所有插件资源 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) parse [book] parse and print debug information about a book --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) init [book] 初始化创建书籍文件结构 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) pdf [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) epub [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) mobi [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) 2、gitbook init初始化创建书籍文件结构 gitbook init # 在当前路径下自动生成README.md 和 SUMMARY.md。也可以先手动创建SUMMARY.md，再执行gitbook init，如果SUMMARY.md中配置的文件夹和文件不存在，就会自动创建文件夹和文件，已经存在的文件夹和文件不会被覆盖。 gitbook init ./directory # 可将书籍初始化到指定目录 3、gitbook build构建gitbook书籍静态HTML资源 gitbook build [book] [output] # 会在书籍的文件夹中生成一个 _book 的文件夹, 里面有生成的静态HTML资源。可将 _book 文件夹下的文件拷贝到nginx、httpd等web服务器内 gitbook build --gitbook=2.0.1 # 指定Gitbook版本 4、gitbook serve启动本地预览书籍服务 gitbook serve [book] [output] 浏览器中打开： http://localhost:4000 预览GitBook书籍 5、输出书籍文件 Prerequisite： ebook-convert：GitBook在生成PDF的过程中使用到calibre的转换功能，没有安装Calibre或安装了Calibre没有配置环境变量都会导致转换PDF失败。Calibre下载地址：https://calibre-ebook.com/download 在 Typora 中安装 Pandoc 进行导出 # 输出书籍为PDF格式文件 gitbook pdf [book] [output] # 输出书籍为epub格式文件 gitbook epub [book] [output] # 输出书籍为mobi格式文件 gitbook mobi [book] [output] 6、gitbook install安装插件样式资源 gitbook install [book] #会在当前路径下生成node_modules文件夹，里面为插件的样式资源 7、gitbook parse 解析电子书 gitbook parse [book] 五、GitBook的文件结构 文件/文件夹 描述 是否必须 README.md 书籍的简介 必须 SUMMARY.md 书籍的目录结构 可选 book.json GitBook的插件样式配置文件 可选 GLOSSARY.md 词汇、术语列表 可选 _book文件夹 GitBook输出的静态HTML文件 node_modules文件夹 插件的样式资源 六、SUMMARY.md编写规则 SUMMARY.md 的格式是一个链接列表。链接的标题将作为章节的标题，链接的目标是该章节文件的路径 向父章节添加嵌套列表将创建子章节 每章都有一个专用页面（part#/README.md），并分为子章节。 目录中的章节可以使用锚点指向文件的特定部分。 目录可以分为以标题或水平线 ---- 分隔的部分 Parts 只是章节组，没有专用页面，但根据主题，它将在导航中显示。 七、book.json编写规则 常规设置 变量 描述 root 包含所有图书文件的根文件夹的路径，除了 book.json structure 指定 Readme，Summary，Glossary 和 Languages 的名称（而不是使用默认名称，如README.md）。这些文件必须在项目的根目录下（或 root 属性指定的根目录）structure.readme：Readme 文件名（默认值是 README.md ）structure.summary：Summary 文件名（默认值是 SUMMARY.md ）structure.glossary：Glossary 文件名（默认值是 GLOSSARY.md ）structure.languages：Languages 文件名（默认值是 LANGS.md ） title 您的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 description 您的书籍的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 author 作者名。在GitBook.com上，这个字段是预填的。 isbn 国际标准书号 ISBN language 本书的语言类型 —— ISO code 。默认值是 en direction 文本阅读顺序。可以是 rtl （从右向左）或 ltr （从左向右），默认值依赖于 language 的值。 gitbook 应该使用的GitBook版本。使用 SemVer 规范，并接受类似于 “> = 3.0.0” 的条件。 links 在左侧导航栏添加链接信息 plugins 要加载的插件列表 pluginsConfig 插件的配置 Gitbook 默认带有 5 个插件： highlight：语法高亮插件 search：搜索插件 sharing：分享插件 font-settings：字体设置插件 livereload：热加载插件 Note：去除插件\"plugins\": [ \"-search\" ] 插件配置示例 { \"author\": \"Curiouser \", \"title\": \"Devops Roadmap\", \"plugins\": [ \"-search\", \"-lunr\", \"-sharing\", \"-highlight\", \"search-pro\", \"splitter\", \"github\", \"popup\", \"sectionx\", \"expandable-chapters\", \"sharing-plus\", \"code\", \"auto-scroll-table\", \"theme-fexa\", \"tbfed-pagefooter\", \"back-to-top-button\", \"emphasize\", \"edit-link\", \"prism\", \"donate\", \"theme-comscore\", \"github-buttons\", \"github-issue-feedback\" ], \"pluginsConfig\": { \"theme-default\": { \"showLevel\": true }, \"github-issue-feedback\": { \"repo\": \"RationalMonster/rationalmonster.github.io\" }, \"github\": { \"url\": \"https://github.com/RationalMonster\" }, \"github-buttons\": { \"buttons\": [{ \"user\": \"RationalMonster\", \"repo\": \"rationalmonster.github.io\", \"type\": \"star\", \"size\": \"small\", \"count\": \"true\" }] }, \"sharing\": { \"weibo\": true, \"qq\": \"true\", \"google\": true, \"all\": [ \"facebook\", \"twitter\" ] }, \"code\": { \"copybuttons\": \"true\" }, \"theme-fexa\":{ \"search-placeholder\":\"搜索文章\", \"logo\": \"assets/logo.png\" }, \"tbfed-pagefooter\": { \"copyright\":\"Copyright Curiouser\", \"modify_label\": \"该文件最后修改时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"edit-link\": { \"base\": \"https://github.com/RationalMonster/rationalmonster.github.io/blob/master\", \"label\": \"ORIGIN In Github\" }, \"prism\": { \"css\": [ \"prismjs/themes/prism-tomorrow.css\" ], \"lang\": { \"flow\": \"typescript\" }, \"ignore\": [ \"mermaid\", \"eval-js\" ] }, \"donate\": { \"wechat\": \"../assets/wechat-donate.jpg\", \"title\": \"\", \"button\": \"赏\", \"wechatText\": \"微信打赏\" } } } 八、GLOSSARY.md 编写规则 GLOSSARY.md 的格式是 h2 标题的列表，以及描述段落 九、忽略文件和文件夹 GitBook将读取 .gitignore，.bookignore 和 .ignore 文件，来过滤不需要进行git版本控制的文件和文件夹。这些文件中的格式遵循 .gitignore 的规则： # This is a comment # Ignore the file test.md test.md # Ignore everything in the directory \"bin\" bin/* ### gitbook ### _node !docs _book node_modules ### IDEA ### .idea/ ### VS Code ### .vscode/ ### OS ### .DS_Store 十、封面 封面用于所有电子书格式。您可以自己提供一个，也可以使用 autocover plugin 生成一个。 要提供封面，请将 cover.jpg 文件放在书本的根目录下。添加一个 cover_small.jpg 将指定一个较小版本的封面。封面应为 JPEG 文件。 好的封面应该遵守以下准则： cover.jpg 的尺寸为 1800x2360 像素，cover_small.jpg 为 200x262 没有边界 清晰可见的书名 任何重要的文字应该在小版本中可见 十一、多语言支持 gitbook 支持构建用多种语言书写的书籍。每种语言应该是一个子目录，遵循正常的gitbook格式，然后需要在根目录下放置一个名为 LANGS.md 的文件，存放下列内容： # Languages * [English](en/) * [French](fr/) * [Español](es/) 注意： 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件 插件的配置必须写在根目录下的 book.json 文件中。然后其他语言的配置可以分别写在各自语言目录下的 book.json 文件中。 LANGS.md 文件中各个语言出现的顺序，就是书籍首页出现的顺利。因此，写在第一位的语言，就自然成为书籍首页打开时的默认语言。 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件。 十二、托管到 Github Pages Github 有个功能： GitHub Pages 。它允许用户在 GitHub 仓库托管你的个人、组织或项目的静态页面（自动识别 html、css、javascript）。 1、建立 xxx.github.io 仓库 要使用这个功能，首先，你必须建立一个严格遵循以下命名要求的仓库：Github账号名.github.io 举例，我的 Github 账号为 dunwu，则这个仓库应该叫 dunwu.github.io。通常，这个仓库被用来作为个人或组织的博客。 2、建立 gh-pages 分支 xxx.github.io 仓库中建立一个名为 gh-pages 的分支。只要 gh-pages 中的内容符合一个静态站点要求，就可以在如下地址中进行访问：https://Github用户名.gitbooks.io/Github 仓库 。 3、自动化发布 方式一：使用 gh-pages 插件 在本地安装插件 npm i -D gh-pages 在 package.json 文件中添加脚本命令： 如下：-d 命令参数后面是要发布的静态站点内容的目录 \"scripts\": { \"deploy\": \"gh-pages -d build\" }, 方式二：使用脚本 cd build \\ git init \\ git checkout -b gh-pages \\ git add . \\ git commit -am \"Update\" \\ git push git@github.com:****4/gitbook-notes gh-pages --force\" 问题 gitbook serve时，偶尔不规律性地出现编译错误,而且每次出现的错误文件还可能不一样，实在是头疼得很，每次修改要编译多次才能成功 修改 C:\\Users\\当前用户名\\.gitbook\\versions\\当前使用的gitbook版本\\lib\\output\\website\\copyPluginAssets.js文件中的112行，将confirm: true改为confirm: false 参考链接 https://www.jianshu.com/p/f38d8ff999cb?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-10-08 17:43:50 "}}