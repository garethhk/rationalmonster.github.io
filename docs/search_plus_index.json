{"./":{"url":"./","title":"简介","keywords":"","body":"Curiouser's Devops Roadmap This gitbook records the technical roadmap of Devops Curiouser. Link GitBook Access URL: https://gitbook.curiouser.top/docs GitHub: https://github.com/RationalMonster What I had done at Openshift or Kubernetes The technical MindMap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 16:13:29 "},"origin/openshift-allinone安装.html":{"url":"origin/openshift-allinone安装.html","title":"Allinone","keywords":"","body":"搭建Allinone全组件Openshift 3.11 一、Overviews Prerequisite IP地址：192.168.1.86 CentOS：7.5.1804 硬盘划分 系统盘60G / 数据盘100G /var/lib/docker ; 100G /data/nfs 开启Selinuxsed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/selinux/config && \\ setenforce 0 Context Docker：版本 1.13，Overlay2(执行Ansible准备脚本时会进行安装) Openshift：版本 3.11 Kubernetes：版本 v1.11.0 二、使用Ansible安装部署 设置主机名并在本地Host文件中添加IP地址域名映射关系ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 配置中科大Openshift的YUM 源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ bash -c ' cat > /etc/yum.repos.d/all.repo 安装基础软件yum install -y git vim net-tools lrzsz unzip bind-utils yum-utils bridge-utils python-passlib wget java-1.8.0-openjdk-headless httpd-tools lvm2 安装Ansible 2.6.5 yum install -y https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm 获取openshift ansible部署脚本代码，禁用ansible脚本中的指定repo git clone https://github.com/openshift/openshift-ansible.git -b release-3.11 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2 (可选)将附件中定制化的OKD登陆页面文件放置/etc/origin/master/custom路径下（自定义的登陆首页）# 路径需要新建 mkdir -p /etc/origin/master/custom 配置Ansible部署Openshift的主机清单/etc/ansible/hosts [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] openshift_ip=192.168.1.86 openshift_public_ip=192.168.1.86 ansible_default_ipv4.address=192.168.1.86 ansible_ssh_user=root openshift_deployment_type=origin deployment_type=origin openshift_release=3.11 openshift_image_tag=v3.11.0 ansible_ssh_pass=**Root用户SSH密码** ######################### Components Cert and CA Expire Days ################# openshift_hosted_registry_cert_expire_days=36500 openshift_ca_cert_expire_days=36500 openshift_node_cert_expire_days=36500 openshift_master_cert_expire_days=36500 etcd_ca_default_days=36500 ####################### Multitenant Network ####################### os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant ####################### OKD ####################### openshift_clock_enabled=true openshift_enable_unsupported_configurations=True openshift_node_groups=[{'name': 'allinone', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage ####################### OKD master config ####################### openshift_master_api_port=8443 openshift_master_cluster_public_hostname=allinone.okd311.curiouser.com openshift_master_cluster_hostname=allinone.okd311.curiouser.com openshift_master_default_subdomain=apps.okd311.curiouser.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin':'$apr1$eG8zNL.C$fvACBzDJ7.N7KdJORT12E0'} openshift_master_oauth_template=custom/login.html openshift_master_session_name=ssn openshift_master_session_max_seconds=3600 ####################### Docker ####################### container_runtime_docker_storage_setup_device=/dev/sdb container_runtime_docker_storage_type=overlay2 openshift_examples_modify_imagestreams=true openshift_docker_options=\"--selinux-enabled -l warn --ipv6=false --insecure-registry=0.0.0.0/0 --log-opt max-size=10M --log-opt max-file=3 --registry-mirror=https://zlsoueh7.mirror.aliyuncs.com\" ####################### Web Console ####################### openshift_web_console_extension_script_urls=[\"https://xhua-static.sh1a.qingstor.com/allinone/allinone-webconsole.js\"] openshift_web_console_extension_stylesheet_urls=[\"https://hermes-uat.mbcloud.com/mbcloud/M00/00/3A/rBACF1vz8NyALOS3AAApT8C9PDY549.css\"] ####################### Registry ####################### openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/data/nfs openshift_hosted_registry_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=10Gi ####################### metrics ####################### openshift_metrics_install_metrics=true openshift_metrics_image_version=v3.11.0 openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/data/nfs openshift_metrics_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=10Gi ####################### logging ####################### openshift_logging_install_logging=true openshift_logging_image_version=v3.11.0 openshift_logging_es_ops_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_es_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_elasticsearch_pvc_size=5Gi openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/data/nfs openshift_logging_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=10Gi ################### Prometheus Cluster Monitoring ################### openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_prometheus_storage_enabled=true openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi ##################### Disable Components ############# openshift_enable_service_catalog=false ansible_service_broker_install=false [masters] allinone.okd311.curiouser.com [etcd] allinone.okd311.curiouser.com [nfs] allinone.okd311.curiouser.com [nodes] allinone.okd311.curiouser.com openshift_node_group_name='allinone' 创建NFS挂载目录 pvcreate /dev/sdc && \\ vgcreate -s 4m data /dev/sdc && \\ lvcreate --size 45G -n nfs data && \\ mkfs.xfs /dev/data/nfs && \\ echo \"/dev/data/nfs /data/nfs xfs defaults 0 0\" >> /etc/fstab && \\ mkdir /data/nfs -p && \\ mount -a && \\ df -mh （可选）预先拉取安装过程中可能使用的镜像docker pull docker.io/openshift/origin-node:v3.11.0 && \\ docker pull docker.io/openshift/origin-control-plane:v3.11.0 && \\ docker pull docker.io/openshift/origin-haproxy-router:v3.11.0 && \\ docker pull docker.io/openshift/origin-deployer:v3.11.0 && \\ docker pull docker.io/openshift/origin-pod:v3.11.0 && \\ docker pull docker.io/openshift/origin-docker-registry:v3.11.0 && \\ docker pull docker.io/openshift/origin-console:v3.11.0 && \\ docker pull docker.io/openshift/origin-service-catalog:v3.11.0 && \\ docker pull docker.io/openshift/origin-web-console:v3.11.0 && \\ docker pull docker.io/cockpit/kubernetes:latest && \\ docker pull docker.io/openshift/oauth-proxy:v1.1.0 && \\ docker pull docker.io/openshift/origin-docker-builder:v3.11.0 && \\ docker pull docker.io/openshift/prometheus-alertmanager:v0.15.2 && \\ docker pull docker.io/openshift/prometheus-node-exporter:v0.16.0 && \\ docker pull docker.io/openshift/prometheus:v2.3.2 && \\ docker pull docker.io/grafana/grafana:5.2.1 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker tag quay-mirror.qiniu.com/coreos/etcd:v3.2.22 quay.io/coreos/etcd:v3.2.22 && \\ docker rmi quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 quay.io/coreos/kube-state-metrics:v1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker tag quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 quay.io/coreos/configmap-reload:v0.0.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker pull quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker tag quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-config-reloader:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 执行OKD Ansible Playbook先执行安装检查的Playbook ansible-playbook /root/openshift-ansible/playbooks/prerequisites.yml 再执行安装Playbook ansible-playbook /root/openshift-ansible/playbooks/deploy_cluster.yml 授予admin用户以管理员权限 oc adm policy add-cluster-role-to-user cluster-admin admin 三、配置Openshift的后端存储 使用Ceph RBD作为后端存储 搭建单节点的Ceph，详见（Ceph RBD单节点安装） 创建Storageclass 使用NFS作为后端存储：详见 参考文章链接 当主机有多网卡时指定组件监听的网卡IP地址：https://github.com/ViaQ/Main/blob/master/README-install.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 10:38:18 "},"origin/openshift-Kubernetes的持久化存储.html":{"url":"origin/openshift-Kubernetes的持久化存储.html","title":"数据持久化","keywords":"","body":"一、Kubernetes持久化存储简介 通常情况下，我们可以认为容器或者Pod的生命周期时短暂的，当容器被销毁时，容器内部的数据也同时被清除。 对于容器，数据持久化存储的重要性不言而喻。Docker有存储卷的概念，用来将磁盘上的或另一个容器中的目录挂载到容器的某一个路径下。即使容器挂掉了，挂载Volume中的数据依旧存在。然而没有对其生命周期进行管理。而Kubernetes提供了多种不同类型资源的Volume存储卷，供POD挂载到容器的不同路径下,常见的有： emptyDir：pod被调度到某个宿主机上的时候才创建，而同一个pod内的容器都能读写EmptyDir中的同一个文件。删除容器并不会对它造成影响，只有删除整个Pod时，它才会被删除，它的生命周期与所挂载的Pod一致 hostPath：将宿主机的文件系统的文件或目录挂接到Pod中 secret：将Kubernetes中secret对象资源挂载到POD中 configMap：将Kubernetes中config对象资源挂载到POD中 persistentVolumeClaim：将PersistentVolume挂接到Pod中作为存储卷。使用此类型的存储卷，用户不需要关注存储卷的详细信息。 nfs glusterfs cephfs vspherevolume iscsi .... 对于以上大部分的volume类型，对使用用户是极其不友好的。理解他们体系中的概念配置是一件复杂的事情，有时我们其实并不关心他们的各种存储实现，只希望能够简单安全可靠地存储数据。所以K8S对存储的供应和使用做了抽象，以API形式提供给管理员和用户使用。因此引入了两个新的API资源：Persistent Volume（持久卷PV）和Persistent Volume Claim（持久卷申请PVC）。 PVC负责定义使用多大的存储空间，什么样的读写方式等常见要求即可，而PV负责抽象各种存储系统的技术细节（例如存储系统IP地址端口，客户端证书密钥等），满足PVC的存储需求，继而作为Kubernetes集群的存储对象资源。 apiVersion: \"v1\" kind: \"PersistentVolumeClaim\" metadata: name: \"ceph-pvc-test\" namespace: \"default\" spec: accessModes: - \"ReadWriteMany\" resources: requests: storage: \"2Gi\" volumeName: \"pv-nfs-test\" # 指定PV PersistentVolumesClaim的属性 Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 Volume Modes：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Resources：指定使用多大的存储空间 Selector：PVC可以指定标签选择器进行更深度的过滤PV，只有匹配了选择器标签的PV才能绑定给PVC。选择器包含两个字段： matchLabels（匹配标签） - PV必须有一个包含该值得标签 matchExpressions（匹配表达式） - 一个请求列表，包含指定的键、值的列表、关联键和值的操作符。合法的操作符包含In，NotIn，Exists，和DoesNotExist。 　　所有来自matchLabels和matchExpressions的请求，都是逻辑与关系的，它们必须全部满足才能匹配上。 Class apiVersion: v1 kind: PersistentVolume metadata: name: ceph-pv-test spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - 192.168.122.133:6789 pool: rbd image: ceph-image user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Retain claimRef: name: \"pvc-test\" namespace: \"default\" PersistentVolumes的属性 Capacity：指定存储容量大小 Volume Mode：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Class: 一个PV可以有一种class，通过设置storageClassName属性来选择指定的StorageClass。有指定class的PV只能绑定给请求该class的PVC。没有设置storageClassName属性的PV只能绑定给未请求class的PVC(过去，使用volume.beta.kubernetes.io/storage-class注解，而不是storageClassName属性。该注解现在依然可以工作，但在Kubernetes的未来版本中已经被完全弃用了) Reclaim Policy Mount Options Node Affinity Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 PersistentVolumes的周期状态 Available: 空闲的，未绑定给PVC Bound: 绑定上了某个PVC Released: PVC已经删除了，但是PV还没有被回收 Failed: PV在自动回收中失败了 PV支持的存储系统: GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CSI FC (Fibre Channel) Flexvolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) Portworx Volumes ScaleIO Volumes StorageOS PV和PVC 之间的关联遵循如下的生命周期： Provisioning-供应: PV的创建阶段，有以下两种创建方式 静态手工：集群管理员通过手工的方式创建pv 动态自动：通过PersistentVolume Controller动态调度，Kubernetes将能够按照用户的需要，根据PVC的资源请求，寻找StorageClasse定义的符合要求的底层存储自动创建其需要的存储卷。 Binding-绑定: PV分配绑定到PVC Using-使用： POD挂载使用PVC类型的Volume Reclaiming-回收：PV释放后的回收利用策略 Retain保留: 保留现场，人工回收 Delete删除: 自动删除，动态删除后端存储。需要IaaS层的支持，目前只有Ceph RBD和OpenStack Cinder支持 Recycle复用：通过rm -rf删除卷上的所有数据。目前只有NFS和HostPath支持（逐渐在抛弃该方式，建议使用） 二、使用StorageClass提供动态存储供应 通常情况下，Kubernetes集群管理员需要手工创建所需的PV存储资源。从Kubernetes 1.2以后可以使用Storageclass实现动态自动地根据用户需求创建某种存储系统类型的PV。同时，可以定义多个 StorageClass ，给集群提供不同存储系统类型的PV资源。 1. 定义创建StorageClass 每一个存储类都必须包含以下参数 provisioner: 决定由哪个Provisioner来创建PV parameters: Provisioner需要的参数,可选项：Delete(Default),Retain reclaimPolicy: PV的回收策略 可选参数： Mount Options Volume Binding Mode Allowed Topologies Note: StorageClass一旦被创建，将不能被更新 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd # 指定Provisioner provisioner: kubernetes.io/rbd parameters: monitors: 10.20.30.40:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" Kubernetes支持的Provisioner Provisioner 是否内置插件 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local StorageClas可以支持第三方的Provisioner，只要该插件符合Kubernetes的规范 内置的Provisioner名称带有“kubernetes.io”前缀 Github仓库：https://github.com/kubernetes-incubator/external-storage 有官方支持的第三方Provisioner 2. 指定StorageClass动态创建PV 在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储； 在Kubernetes v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce # 指定所使用的存储类，此存储类将会自动创建符合要求的PV storageClassName: ceph-rbd resources: requests: storage: 30Gi 3. 指定默认的StorageClass 创建StorageClass时可添加添加storageclass.kubernetes.io/is-default-class注解来指定为默认的存储类。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" # 将此storageclass设置为默认 name: nfs-client-storageclass provisioner: fuseim.pri/ifs parameters: archiveOnDelete: \"true\" 一个集群中，最多只能有一个默认的存储类 如果没有默认的存储类，在PersistentVolumeClaim中也没有显示指定storageClassName，将无法创建PersistentVolume。 参考链接 https://kubernetes.io/docs/concepts/storage/volumes/ https://kubernetes.io/docs/concepts/storage/storage-classes/ https://www.kubernetes.org.cn/4078.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 19:39:23 "},"origin/openshift-Kubernetes-provisioner-nfs-client.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-client.html","title":"NFS Client provisioner","keywords":"","body":"一、NFS Client Provisioner https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client https://www.kubernetes.org.cn/3894.html Provisioner的定义原理: openshift-Kubernetes的持久化存储 二、安装部署 1. 创建NFS服务端 yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ mkdir -p /data/nfs/appstorage-nfs-client-provisioner && \\ echo \"/data/nfs/appstorage-nfs-client-provisioner *(rw,no_root_squash,sync)\" >> /etc/exports && \\ exportfs -a && \\ showmount -e $HOSTNAME 2. 创建RBAC kind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 3. 修改Deployment并以此部署POD 先拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest quay.io/external_storage/nfs-client-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: $HOSTNAME # NFS Server的地址 - name: NFS_PATH value: /data/nfs/appstorage-nfs-client-provisioner # NFS Server要挂载的路径 volumes: - name: nfs-client-root nfs: server: $HOSTNAME #指定NFS Server的地址 path: /data/nfs/appstorage-nfs-client-provisioner #指定NFS Server要挂载的路径 三、使用 1. 创建StorageClass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. =======================================================补充内容========================================================= #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\" 删除PVC时不会保留数据，\"true\"将保留PVC的数据，形成以\"archived-\"开头的文件夹 2. 创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc #当默认storageclass就是nfs-client-storageclass，可不要该注解 annotations: volume.beta.kubernetes.io/storage-class: \"nfs-client-storageclass\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 1. 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX nfs-client-storageclass 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX Delete Bound default/test nfs-client-storageclass 10m 2. 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 3. 查看NFS目录 /data/nfs/k8s-app-nfs-storage/ └── [drwxrwxrwx 32] default-test-pvc-e8a15786-5a09-11e9-ad53-000c296286d8 ├── [-rw-r--r-- 947] 1.log └── [-rw-r--r-- 1.0K] 2.log Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 21:07:00 "},"origin/openshift-Kubernetes-provisioner-nfs-server.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-server.html","title":"NFS Server Provisioner","keywords":"","body":"一、NFS Server Provisioner Github项目地址：https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/nfs Provisioner的定义原理：openshift-Kubernetes的持久化存储 NFS Provisioner的部署文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/deployment.md NFS Provisioner的使用文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/usage.md 二、在Kubernetes上部署 1、（可选）预拉取镜像 docker pull quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest quay.io/kubernetes_incubator/nfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest 2、创建PodSecurityPolicy apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: nfs-provisioner spec: fsGroup: rule: RunAsAny allowedCapabilities: - DAC_READ_SEARCH - SYS_RESOURCE runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - secret - hostPath 3、创建RBAC kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-provisioner apiGroup: rbac.authorization.k8s.io 4、使用deployment创建POD（推荐） apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: # 定义提供者的名称，存储类通过此名称指定提供者 - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 5、（可选）使用StatefulSet创建POD apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: StatefulSet apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner serviceName: \"nfs-provisioner\" replicas: 1 template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner terminationGracePeriodSeconds: 10 containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 三、使用 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" ​ =======================================================补充内容========================================================= ​ #其他参数： gid: # \"none\" or a supplemental group like \"1001\". NFS shares will be created with permissions such that pods running with the supplemental group can read & write to the share, but non-root pods without the supplemental group cannot. Pods running as root can read & write to shares regardless of the setting here, unless the rootSquash parameter is set true. If set to \"none\", anybody root or non-root can write to the share. Default (if omitted) \"none\". rootSquash: # \"true\" or \"false\". Whether to squash root users by adding the NFS Ganesha root_id_squash or kernel root_squash option to each export. Default \"false\". mountOptions: # a comma separated list of mount options for every PV of this class to be mounted with. The list is inserted directly into every PV's mount options annotation/field without any validation. Default blank \"\". ​ #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: #注解 annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" 创建PVC时指定storageclass kind: PersistentVolumeClaim apiVersion: v1 metadata: name: nfs annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX example-nfs 5m3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX Delete Bound default/test example-nfs 5m9s 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 查看/srv目录 /srv ├── [-rw-r--r-- 4.4K] ganesha.log ├── [-rw------- 36] nfs-provisioner.identity ├── [drwxrwsrwx 32] pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 │ ├── [-rw-r--r-- 5.1K] 1.log │ └── [-rw-r--r-- 5.7K] 2.log └── [-rw------- 1.1K] vfs.conf 注意：删除掉PVC，PV也会自动删除，底层的NFS目录也会跟着删除 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 21:07:03 "},"origin/openshift-Kubernetes-provisioner-glusterfs.html":{"url":"origin/openshift-Kubernetes-provisioner-glusterfs.html","title":"Glusterfs Provisioner","keywords":"","body":"一. OKD集群中添加容器化的GlusteFS Prerequisite OKD集群（3.11）至少有三个节点 OKD官方操作指南：https://docs.okd.io/3.11/install_config/persistent_storage/persistent_storage_glusterfs.html#install-config-persistent-storage-persistent-storage-glusterfs GlusterFS官方操作指南：https://docs.gluster.org/en/latest/Administrator%20Guide/overview/ heketi-cli官方操作指南：https://github.com/heketi/heketi 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false ​ [glusterfs] allinone311.okd.curiouser.com glusterfs_devices='[ \"/dev/vdf\" ]' node1.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' node2.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' #至少是三个节点 glusterfs节点上安装软件 yum install glusterfs-fuse && \\ yum update glusterfs-fuse 配置glusterfs节点上的Selinux setsebool -P virt_sandbox_use_fusefs on && \\ setsebool -P virt_use_fusefs on 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 二. 向OKD集群中添加集群外的GlusteFS 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false openshift_storage_glusterfs_is_native=false openshift_storage_glusterfs_heketi_is_native=true openshift_storage_glusterfs_heketi_executor=ssh openshift_storage_glusterfs_heketi_ssh_port=22 openshift_storage_glusterfs_heketi_ssh_user=root openshift_storage_glusterfs_heketi_ssh_sudo=false openshift_storage_glusterfs_heketi_ssh_keyfile=\"/root/.ssh/id_rsa\" ​ [glusterfs] gluster1.example.com glusterfs_ip=192.168.10.11 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster2.example.com glusterfs_ip=192.168.10.12 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster3.example.com glusterfs_ip=192.168.10.13 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 三. 卸载 ansible-playbook -e \"openshift_storage_glusterfs_wipe=true\" /root/openshift-ansible/playbooks/openshift-glusterfs/uninstall.yml 四. OKD中通过storage动态使用glusterfs作为PVC的后端存储 1. 创建storage class 创建storage class(ansible playbook执行过程中会自动创建storageclass) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: 'http://heketi-storage.app-storage.svc:8080' restuser: admin secretName: heketi-storage-admin-secret secretNamespace: app-storage reclaimPolicy: Delete volumeBindingMode: Immediate 如果使用的集群外的Glusterfs集群，需要手动创建storage class。 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: \"http://10.42.0.0:8080\" restauthenabled: \"false\" 2. 创建PVC时使用Glusterfs的storage class apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gluster1 spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi storageClassName: glusterfs-storage 五. 主机上mount挂载使用容器化的GlusterFS 挂载命令格式： mount -t glusterfs GlusterFS容器化pod所在的节点IP地址:/volume_name /mnt/glusterfs 示例： $ mount -t glusterfs 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 /mnt/glusterfs && \\ df -mh 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 10G 136M 9.9G 2% /mnt/glusterfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 21:20:42 "},"origin/openshift-Kubernetes-provisioner-cephfs.html":{"url":"origin/openshift-Kubernetes-provisioner-cephfs.html","title":"Ceph FileSystem Provisioner","keywords":"","body":"相关链接 官方文档： https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs Provisioner的定义原理：Kubernetes的存储--> StorageClass provisioner 姊妹篇： Preflight 1. Openshift创建cephfs命名空间 oc new-project cephfs --display-name=\"Ceph FileSystem Provisioner\" 2. 拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest quay.io/external_storage/cephfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest 一、安装部署 1. 获取Ceph Filesystem Client.admin用户的密钥环 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" 2. 创建Secrets oc create secret generic cephfs-secret-admin --from-literal=key='AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ==' --namespace=cephfs 3. 创建RBAC --- apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 4. 使用Deployment创建Ceph-FileSystem-provisioner的POD apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 二. 使用 1. 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cephfs provisioner: ceph.com/cephfs parameters: monitors: allinone.okd311.curiouser.com:6789 adminId: admin adminSecretName: cephfs-secret-admin adminSecretNamespace: \"cephfs\" claimRoot: /pvc-volumes 2、创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-test spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 21:07:07 "},"origin/openshift-Kubernetes-provisioner-cephrbd.html":{"url":"origin/openshift-Kubernetes-provisioner-cephrbd.html","title":"Ceph RBD Provisioner","keywords":"","body":"一、获取ceph client admin用户的密钥环keyring 查看Ceph集群Admin节点的集群配置文件夹my-cluster下的ceph.client.admin.keyring文件来获取key值 $> cat ceph.client.admin.keyring [client.admin] key = AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA== caps mds = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" ​ $ ceph auth list #获取所有客户端用户 $ ceph auth get client.admin #获取客户端指定用户 二、使用admin的keyring在openshift上创建secret CLI $> oc create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQAil11anEPOORAArxzRkH9iS1IOGKQfK87+Ag==' --namespace=default YAML kind: Secret apiVersion: v1 metadata: name: ceph-secret namespace: default selfLink: /api/v1/namespaces/default/secrets/ceph-secret data: key: QVFDcFNlMWJ0Y3VxSFJBQWlST25zY1VDMWpnTWRwZkRJMFd0THc9PQ== type: kubernetes.io/rbd 三、创建storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd-sc provisioner: kubernetes.io/rbd parameters: monitors: 192.168.0.26:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret #说明:adminId默认值为admin,pool默认值为rbd, userId默认值与adminId一样.所以这三个值可以不填写。 四、可以在console界面创建，也可以通过PVC的YAML配置文件中指定使用Ceph $> cat ceph-rbd-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd-sc 结果如下图： Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 21:06:43 "},"origin/openshift-资源对象常见操作.html":{"url":"origin/openshift-资源对象常见操作.html","title":"资源对象常见操作","keywords":"","body":"常用的资源对象操作 登录 oc project oc login -u 用户名 集群master的URL oc whoami #查看当前登录的用户，加-t参数可查看当前用户的token 切换Project oc project 查看集群节点 oc get node/no oc get node/no node1.test.openshift.com 查看集群节点的详细信息 oc describe node node1.test.openshift.com 查看某个节点上的所有Pods oc adm manage-node node1.test.openshift.com --list-pods 使节点禁止参与调度 oc adm manage-node router1.test.openshift.com --schedulable=false 疏散某个节点上的所有POD oc adm drain router1.test.openshift.com --ignore-daemonsets 清除旧的Build和Deployments历史版（所有namespace） 统计要清除的资源个数 #oc adm prune deployments --keep-younger-than=24h --keep-complete=5 --keep-failed=5|wc -l 确认清除动作 # oc adm prune [deployments|builds|images] --confirm --keep-younger-than=24h --keep-complete=5 --keep-failed=5 参数详解 --confirm 确认操作 --keep-younger-than=1h0m0s Specify the minimum age of a Build for it to be considered a candidate for pruning. --keep-complete=5 Per BuildConfig, specify the number of builds whose status is complete that will be preserved. --keep-failed=1 Per BuildConfig, specify the number of builds whose status is failed, error, or cancelled that will be preserved. --orphans=false If true, prune all builds whose associated BuildConfig no longer exists and whose status is complete, failed, error, or cancelled. 示例： 清理images（在admin用户下执行） # oc adm prune images --keep-younger-than=400m --keep-tag-revisions=10 --registry-url=docker-registry.default.svc:5000 --certificate-authority=/etc/origin/master/registry.crt --confirm 删除所有Namespace中非Running的podsfor i in `oc get po --all-namespaces|grep -v \"Running\"|grep -v \"NAMESPACE\"|awk '{print $1}'|sort -u` ; do a=`oc get po -n $i |grep -v \"Running\"|grep -v \"NAMESPACE\"|awk '{print $1}'`; echo $a;oc delete po $a -n $i; echo \"======\"; done 强制删除PODoc delete po gitlab-ce-16-ntzst --force --grace-period=0 资源的查看#查看当前项目的所有资源 oc get all #查看当前项目的所有资源，外加输出label信息 oc get all --show-labels # 查看指定资源 oc get pod/po oc get service/svc oc get persistentvolumes/pv 通过label选择器删除namespace下所有的资源#如果namespace下所有的资源都打上了“name=test”标签 oc delete all -l name=test 项目的管理#创建项目 oc new-project --display-name=显示的项目名 --description=项目描述 project_name #删除项目 oc delete project 项目名 #查看当前处于哪个项目下 oc project #查看所有项目 oc projects 模板的管理#创建模板(模板文件格式为YAML/JSON.也可以在Openshift的web页面上直接导入) oc create -f #查看模板 oc get templates #编辑模板 oc edit template #删除模板 oc delete template 附录 buildconfigs (aka 'bc') #构建配置 builds #构建版本 certificatesigningrequests (aka 'csr') clusters (valid only for federation apiservers) clusterrolebindings clusterroles componentstatuses (aka 'cs') configmaps (aka 'cm') daemonsets (aka 'ds') deployments (aka 'deploy') deploymentconfigs (aka 'dc') endpoints (aka 'ep') events (aka 'ev') horizontalpodautoscalers (aka 'hpa') imagestreamimages (aka 'isimage') imagestreams (aka 'is') imagestreamtags (aka 'istag') ingresses (aka 'ing') groups jobs limitranges (aka 'limits') namespaces (aka 'ns') networkpolicies nodes (aka 'no') persistentvolumeclaims (aka 'pvc') persistentvolumes (aka 'pv') poddisruptionbudgets (aka 'pdb') podpreset pods (aka 'po') podsecuritypolicies (aka 'psp') podtemplates policies projects replicasets (aka 'rs') replicationcontrollers (aka 'rc') resourcequotas (aka 'quota') rolebindings roles routes secrets serviceaccounts (aka 'sa') services (aka 'svc') statefulsets users storageclasses thirdpartyresources Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-09 17:18:05 "},"origin/openshift-集群节点管理.html":{"url":"origin/openshift-集群节点管理.html","title":"节点管理","keywords":"","body":"一、集群添加Node节点 Ansible脚本有新增节点的Playbook脚本，准备好新增节点的基础环境，在集群的ansible管理节点上执行该Playbook就行。 　Context OKD版本 OS版本 Docker版本 Ansible版本 3.11 CentOS 7.5.1804 1.13.1 2.6.5 1. 新增节点Prerequisite 新增node节点IP地址及主机名：192.168.1.23 node6.okd.curiouser.com 开启seLinux sed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/sysconfig/selinux && \\ setenforce 1 安装docker，jdk及基础软件 yum install -y docker vim lrzsz wget unzip net-tools telnet bind-utils && \\ systemctl enable docker && \\ systemctl start docker && \\ systemctl status docker && \\ yum localinstall -y jdk-8u191-linux-x64.rpm && \\ docker info && \\ java -version 配置DNS，发现集群其他节点的IP地址与域名的映射关系.(注意DNSMasq服务端的iptables是否放行DNS的53 UDP端口) 由于集群内有DNSMasq服务端，配置/etc/resolv.conf echo \"nameserver 192.168.1.22\" >> /etc/resolv.conf && \\ ping allinone311.okd.curiouser.com Note: #DNSMasq服务端放行DNS的53 UDP端口 iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 配置Openshift的YUM源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ scp allinone311.okd.curiouser.com:/etc/yum.repos.d/all.repo /etc/yum.repos.d/ && \\ yum clean all && \\ yum makecache 2. ansible管理节点 打通ansible管理节点到新增node节点的SSH免密通道 ssh-copy-id -i root@node6.okd.curiouser.com && \\ ssh root@node1.okd.curiouser.com ansible管理节点的ansible主机清单文件inventory中添加新增节点相关信息 [OSEv3:children] ... new_nodes [new_nodes] node1.okd.curiouser.com openshift_node_group_name=\"node-config-all-in-one\" ansible管理节点执行新增节点的Ansible Playbookansible-playbook /root/openshift-ansible/playbooks/openshift-node/scaleup.yml 注意1： 当执行脚本时tower主机会把它的dnsmasq配置/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下。由于tower主机的/etc/dnsmasq.d/origin-upstream-dns.conf设置的上游DNS服务器为外网的。不希望新增节点的上游DNS服务器走外网，而是走tower主机，形成集群只有Tower主机一个节点的dns对外，其他主机作为Tower主机dns服务的客户端。所以当tower主机/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下的时候，及时修改上游dns服务器为tower主机。然后重启dnsmasq。有两个明显的坑: ① 无法重启dnsmasq，报以下错误： DBus error: Connection \":1.50\" is not allowed to own the service \"uk.org.thekelleys.dnsmasq\" due to security policies in the configuration file 解决方案：重启dbus，再重启dnsmasq systemctl restart dbus && \\ systemctl restart dnsmasq ②tower主机的iptables服务开启，dns的53端口没有放开，导致新增节点的dns无法连接上游dns服务器（即Tower主机的dns服务） 解决方案：tower主机放行dns服务的UDP 53端口。（可在新增节点尝试nslookup解析域名试一下） iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 注意2： 如果出现收集allinone节点facts超时的报错，出现一下错误提示 The full traceback is: Traceback (most recent call last): File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/basic.py\", line 2853, in run_command cmd = subprocess.Popen(args, **kwargs) File \"/usr/lib64/python2.7/subprocess.py\", line 711, in __init__ errread, errwrite) File \"/usr/lib64/python2.7/subprocess.py\", line 1308, in _execute_child data = _eintr_retry_call(os.read, errpipe_read, 1048576) File \"/usr/lib64/python2.7/subprocess.py\", line 478, in _eintr_retry_call return func(*args) File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/facts/timeout.py\", line 37, in _handle_timeout raise TimeoutError(msg) TimeoutError: Timer expired after 10 seconds TimeoutError: Timer expired after 10 seconds 请在/etc/ansible/ansible.cfg 设置\"gather_subset = !all\"或者\"gather_timeout=300\"。原因可能是已经运行allinone节点上的facts（特别是docker images layer的挂载信息）过多，造成收集facts超时，默认收集facts超时时间是10。 相关连接：https://github.com/ansible/ansible/issues/43884 二、删除节点 疏散要删除节点上的POD oc adm drain [--pod-selector=] --force=true --grace-period=-1 --timeout=5s --delete-local-data=true 删除Node oc delete node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 14:20:25 "},"origin/elasticsearch-集群中elasticsearch的管理.html":{"url":"origin/elasticsearch-集群中elasticsearch的管理.html","title":"集群中elasticsearch的管理","keywords":"","body":"Overview Openshift 3.11如果是使用官方ansible playbook安装的话，可配置安装EFK来收集存储分析Openshift上所有容器的日志。Flentd以Daemonset的形式部署在所有Node节点上，监控采集docker容器目录/var/docker/container（每个容器的日志默认都会以 json-file 的格式存储于 /var/lib/docker/containers//-json.log 下）.将采集的日志数据存放在elasticsearch中。并配置了一个Cronjob定时清理elasticsearch中指定过期的Index。 查看所有的Index oc exec $es-pod-name -- curl -s --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca https://localhost:9200/_cat/indices?v 清理所有3月份的ES索引 oc exec $es-pod-name -- curl -s --cert /etc/elasticsearch/secret/admin-cert --key /etc/elasticsearch/secret/admin-key --cacert /etc/elasticsearch/secret/admin-ca -XDELETE https://localhost:9200/*.2018.03.* 通过环境变量设置Cronjob定时清理超过三天的Index，只保存三天的Index oc edit cj logging-curator -n openshift-logging 相关环境变量: K8S_HOST_URL: https://kubernetes.default.svc.cluster.local ES_HOST: logging-es ES_PORT: \"9200\" ES_CLIENT_CERT: /etc/curator/keys/cert ES_CLIENT_KEY: /etc/curator/keys/key ES_CA: /etc/curator/keys/ca CURATOR_DEFAULT_DAYS: \"3\" CURATOR_SCRIPT_LOG_LEVEL: INFO CURATOR_LOG_LEVEL: ERROR CURATOR_TIMEOUT: \"300\" 参考连接 https://docs.openshift.com/container-platform/3.11/install_config/aggregate_logging.html#aggregate-logging-understanding-the-deployment docker.io/openshift/origin-logging-curator5:v3.11.0 https://github.com/openshift/origin-aggregated-logging Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 10:54:47 "},"origin/openshift-开启router的haproxy-statisc.html":{"url":"origin/openshift-开启router的haproxy-statisc.html","title":"openshift开启router的haproxy-statisc","keywords":"","body":" 设置router POD 所在节点的iptables对1936端口的放行 iptables -I OS_FIREWALL_ALLOW -p tcp -m tcp --dport 1936 -j ACCEPT 获取访问router haproxy statics 页面的用户名密码。 删除掉router dc中的环境变量”ROUTER_METRICS_TYPE“ 这个环境变量默认值为“haproxy”。不删除的话，访问的时候会报一下错误 Forbidden: User \"system:anonymous\" cannot get routers/metrics.route.openshift.io at the cluster scope 将健康检查readiness的HTTP GET URL由“/healthz/ready”改为\"/healthz\"。（不然router POD无法通过健康检查） 验证监听端口80，443，1936 ss -ntl|grep 80 ss -ntl|grep 443 ss -ntl|grep 1936 访问router haproxy statistics 页面。 访问方式是：http://:@router所在节点IP地址:1936 例如：http://admin:MJbJFvODhP@allinone.curiouser.com:1936 相关链接 https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#using-wildcard-routes https://bugzilla.redhat.com/show_bug.cgi?id=1579054 https://github.com/openshift/origin/issues/17025 https://blog.chmouel.com/2016/09/27/how-to-view-openshift-haproxy-stats/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-02 17:18:05 "},"origin/openshift-多租户网络.html":{"url":"origin/openshift-多租户网络.html","title":"openshift的多租户网络","keywords":"","body":"一、Openshift容器网络简介 Openshift容器网络默认是基于Open vSwitch（OVS）实现的。 Openshift提供两种网络方案： ovs-subnet(子网模式)：为集群节点上的容器提供一个扁平化的二层虚拟网路，所有在这个二层网路中容器可直接通信。 ovs-multitenet(多租户模式)：基于项目的网络隔离，即不同项目间的容器之间不能直接通信。启动多租户网络隔离后，每个项目创建后都会被分配一个虚拟网络ID（Virtual Network ID ,VNID）.OVS网桥会为该项目的所有数据流量标记上VNID，在默认情况下，只有数据包上的VNID与目标容器所在项目的VNID匹配上后，数据包才允许被转发到目标容器中。当有些项目的容器应用是通过公共服务的，后期可通过配置将多个项目见的网络连通，或者将项目设置为全局可访问。 二、启动多租户网络 需要将集群中所有的master节点配置文件/etc/origin/master/master-config.yaml和node节点配置文件/etc/origin/node/node-config.yaml中的networkPluginName的属性值从redhat/openshift-ovs-subnet修改为redhat/openshift-ovs-multitenant，然后重启Openshift集群Master节点的origin-master-controllers.service服务和Node节点的origin-node.service服务 三、测试，查看网络隔离 在一个项目中的一个pod的终端中ping/telnet/curl/nslook另一个项目中的pod的ip地址或者对应svc的FQDN（..svc.cluster.local） 查看namespace的Netid是否一致 $ oc get netnamespaces NAME NETID EGRESS IPS default 0 [] kube-public 5899696 [] kube-service-catalog 0 [] demo 13843039 [] dubbo 11344186 [] jenkins 13843039 [] 当NETID相同时，表示这个两个project的网络是相通的 当NETID为0时，表示这个Project的网络全局可访问 四、连通隔离的网络 # project 1,2,3中所有的pod，service可以通过容器IP相互访问（通过service的FQDN不能相互访问） oc adm pod-network join-projects --to= #将某个project中所有的pod和service设置为全局可访问 oc adm pod-network make-projects-global 参考链接 https://docs.okd.io/3.11/admin_guide/managing_networking.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-09 17:19:17 "},"origin/openshift-kubernetes的审计日志功能.html":{"url":"origin/openshift-kubernetes的审计日志功能.html","title":"Kubernetes的审计日志功能","keywords":"","body":"一、Overviews kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题： 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？ kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有： RequestReceived ：apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted ：在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete ：当响应 body 发送完并且不再发送数据。 Panic：内部服务器出错，请求未完成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成 日志记录级别，当前支持的日志记录级别有： None: 符合这条规则的日志将不会记录。 Metadata: 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request: 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse: 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。 输出的审计日志格式 json{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1beta1\", \"metadata\": { \"creationTimestamp\": \"2019-07-23T09:02:19Z\" }, \"level\": \"Request\", \"timestamp\": \"2019-07-23T09:02:19Z\", \"auditID\": \"eb481add-fdac-48a3-a302-1c33d73bfdbf\", \"stage\": \"RequestReceived\", \"requestURI\": \"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\", \"verb\": \"update\", \"user\": { \"username\": \"system:openshift-master\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.1.96\" ], \"objectRef\": { \"resource\": \"configmaps\", \"namespace\": \"kube-system\", \"name\": \"openshift-master-controllers\", \"apiVersion\": \"v1\" }, \"requestReceivedTimestamp\": \"2019-07-23T09:02:19.148057Z\", \"stageTimestamp\": \"2019-07-23T09:02:19.148057Z\" } legacy 2019-07-23T23:50:06.223368641+08:00 AUDIT: id=\"3574e2e0-06b1-44d8-bc6c-5983c402d55e\" stage=\"ResponseComplete\" ip=\"192.168.1.96\" method=\"update\" user=\"system:openshift-master\" groups=\"\\\"system:masters\\\",\\\"system:authenticated\\\"\" as=\"\" asgroups=\"\" namespace=\"kube-system\" uri=\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\" response=\"200\" 审计后端可以将审计事件导出到外部存储。 Kube-apiserver 提供两个后端： Log 后端: 将事件写入到磁盘 Webhook 后端: 将事件发送到外部 API Note: 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 二、openshift开启自定义策略的审计功能 创建审计日志的存储路径 mkdir /etc/origin/master/audit # 注意：审计日志文件的存储路径必须是kube-system命名空间下apiservser pod挂载目录下的子路径。 # ocp 3.11版本的apiserver是以pod的形式运行在kube-system命名空间下的，它所需要的配置文件等Volume资源都是以hostpath的形式挂载上去的，例如ocp节点上的/etc/origin/master目录 编辑/etc/origin/master/master-config.yaml ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 ****省略******** 创建自定义的审计策略配置文件 kind: Policy omitStages: - \"ResponseStarted\" rules: - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"configmaps\",\"secrets\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the metadata level. - level: None verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the request level. - level: None resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # Log login failures from the web console or CLI. Review the logs and refine your policies. - level: Metadata nonResourceURLs: - /login* - /oauth* - level: Metadata userGroups: [\"system:authenticated:oauth\"] verbs: [\"create\", \"delete\"] resources: - group: \"project.openshift.io\" resources: [\"projectrequests\", \"projects\"] omitStages: - RequestReceived 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 三、使用openshift集群的Fluentd收集审计日志到集群内的elasticsearch 配置OCP集群中的Fluentd挂载审计日志的存储目录(OCP集群中日志系统的fluentd是以DaemonSet形式收集节点上容器的日志到elasticsearch的，它是将节点的/var/lib/docker目录以hostpath形式挂载到容器中的) oc set volume ds/logging-fluentd --add --mount-path=/etc/origin/master/audit --name=audit --type=hostPath --path=/etc/origin/master/audit -n openshift-logging 配置OCP集群中的Fluentd监控审计日志目录下的日志 oc edit cm/logging-fluentd -n openshift-logging *****省略******* ## sources *****省略******* @include configs.d/user/input-audit.conf *****省略******* input-audit.conf: | @type tail @id audit-ocp path /etc/origin/master/audit/audit-ocp.log pos_file /etc/origin/master/audit/audit.pos tag audit.requests format json @type copy @type elasticsearch log_level debug host \"#{ENV['OPS_HOST']}\" port \"#{ENV['OPS_PORT']}\" scheme https ssl_version TLSv1_2 index_name .audit user fluentd password changeme client_key \"#{ENV['OPS_CLIENT_KEY']}\" client_cert \"#{ENV['OPS_CLIENT_CERT']}\" ca_file \"#{ENV['OPS_CA']}\" type_name com.redhat.ocp.audit reload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'false'}\" reload_after \"#{ENV['ES_RELOAD_AFTER'] || '100'}\" sniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::ElasticsearchSimpleSniffer'}\" reload_on_failure false flush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '5s'}\" max_retry_wait \"#{ENV['ES_RETRY_WAIT'] || '300'}\" disable_retry_limit true buffer_type file buffer_path '/var/lib/fluentd/buffer-output-es-auditlog' buffer_queue_limit \"#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }\" buffer_chunk_limit \"#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }\" buffer_queue_full_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}\" request_timeout 2147483648 *****省略******* 重启Fluentd oc delete po -l component=fluentd -n openshift-logging 在ocp集群系统的Kibana上添加\".audit*\"的Index Pattern,并在\"Discover\"查看、筛选审计日志 四、将审计日志通过WebHook 发送到OCP外部的Logstash或者Fluentd 接受后端 可使用Logstash或者Fluentd作为后端来接受Api-Server通过web hook方式发送的审计日志。Logstash和Fluentd可以是ocp集群外二进制方式安装运行的，也可以是原生Docker运行的，甚至可以是另外一个集群中容器化的。一个原则就是不要放到审计日志产生集群的内部。防止apiserver启动起来了，有了一些操作，logstash还没有启动起来，丢失审计日志。再者审计日志后端最好选择适合自己的，审计日志落一份，重复记录也没多大意义。 方式一：使用OCP集群外二进制方式安装的Logstash来接受ApiServer通过web hook方式发送过来的审计日志并过滤、存储到本地文件中 安装logstash bash -c 'cat > /etc/yum.repos.d/elasticsearch.repo 设置logstash，/etc/logstash/logstash.yml # ------------ Pipeline Configuration Settings -------------- # Where to fetch the pipeline configuration for the main pipeline path.config: /etc/logstash/conf.d/ *************************省略****************************** # ------------ Data path ------------------ # Which directory should be used by logstash and its plugins for any persistent needs. Defaults to LOGSTASH_HOME/data path.data: /data/logs/logstash/data/ *************************省略****************************** # ------------ Debugging Settings ------------- # Options for log.level: fatal/error/warn/info (default)/debug/trace log.level: info path.logs: /data/logs/logstash/logs 创建监听HTTP 8081端口的pipeline cat /etc/logstash/conf.d/accept-audit-log.conf input{ http{ host => \"0.0.0.0\" port => 8081 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=>[items] # We only need event subelement, remove others. remove_field=>[headers, metadata, apiVersion, kind, \"@version\", host] } mutate{ rename => {items=>event} } } output{ file{ # Audit events from different users will be saved into different files. path=>\"/data/logs/logstash/ocp-audit-logs/ocp-audit-%{[event][user][username]}/audit-%{+YYYY-MM-dd}.log\" } } EOF 启动logstash mkdir -p /data/logs/logstash/{data,logs,ocp-audit-logs} chown -R logstash:logstash /data/logs/logstash system start logstash # 或者 /usr/share/logstash/bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/ 测试logstash的联通性。一是看logstash pipeline监听的HTTP端口是否开启。二是尝试发送一个带有模拟数据的POST请求，看其是否会pipeline指定的数据目录生成日志文件 ss -ntl |grep 8081 curl -X POST \\ http://192.168.1.96:8081 \\ -H 'Accept: */*' \\ -H 'Cache-Control: no-cache' \\ -H 'Connection: keep-alive' \\ -H 'Content-Type: application/json' \\ -H 'accept-encoding: gzip, deflate' \\ -d '{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1beta1\",\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:27:54Z\"},\"level\":\"Request\",\"timestamp\":\"2019-07-23T14:27:54Z\",\"auditID\":\"29bf32ba-4bea-4b4f-a1fb-cd091b2188ff\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"verb\":\"update\",\"user\":{\"username\":\"system:openshift-master\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"resource\":\"configmaps\",\"namespace\":\"kube-system\",\"name\":\"openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"apiVersion\":\"v1\",\"resourceVersion\":\"8285989\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ConfigMap\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-master-controllers\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"resourceVersion\":\"8285989\",\"creationTimestamp\":\"2019-03-09T11:31:08Z\",\"annotations\":{\"control-plane.alpha.kubernetes.io/leader\":\"{\\\"holderIdentity\\\":\\\"allinone.okd311.curiouser.com\\\",\\\"leaseDurationSeconds\\\":15,\\\"acquireTime\\\":\\\"2019-03-09T11:31:01Z\\\",\\\"renewTime\\\":\\\"2019-07-23T14:27:54Z\\\",\\\"leaderTransitions\\\":0}\"}}},\"requestReceivedTimestamp\":\"2019-07-23T14:27:54.894767Z\",\"stageTimestamp\":\"2019-07-23T14:27:54.899643Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}}' 创建audit的webhook配置文件/etc/origin/master/audit-policy.yaml cat /etc/origin/master/audit-policy.yaml apiVersion: v1 clusters: - cluster: server: http://192.168.1.96:8081 name: logstash contexts: - context: cluster: logstash user: \"\" name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF 编辑/etc/origin/master/master-config.yaml，添加webhook相关的参数 ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 #==========以下配置项为添加的webhook参数=========================================================================== webHookKubeConfig: /etc/origin/master/audit-webhook-config.yaml # 指定WebHook的配置文件（同样路径要指定在ApiServer POD已挂载的路径下） webHookMode: batch # 可选参数\"batch\"和\"blocking\" ****省略******** 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 验证，用除\"system:admin\"用户外的其他用户创建project，然后再删除project，最后查看logstash配置的审计日志存储目录下是否生成对应的文件 oc login -u admin -p oc new-project test oc delete project test $ tree -L 2 /data/logs/logstash/ocp-audit-logs/ /data/logs/logstash/ocp-audit-logs/ ├── ocp-audit-admin │ └── audit-2019-07-23.log └── ocp-audit-system:openshift-master └── audit-2019-07-23.log $ cat /data/logs/logstash/ocp-audit-logs/ocp-audit-admin/audit-2019-07-23.log 产生以下内容。显示一次创建成功，另一次创建失败，原因是project已经存在（特意测试），一次删除project等日志。 {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:03Z\"},\"stageTimestamp\":\"2019-07-23T14:45:03.019249Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:02Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:02.964347Z\",\"auditID\":\"eec24884-b70a-4b27-80c1-431111d2f4f5\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:10Z\"},\"stageTimestamp\":\"2019-07-23T14:45:10.842999Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:10Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"reason\":\"AlreadyExists\",\"code\":409},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:10.836487Z\",\"auditID\":\"a7b64f47-94eb-4723-b101-24e112cd0735\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"self-provisioners\\\" of ClusterRole \\\"self-provisioner\\\" to Group \\\"system:authenticated:oauth\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:19Z\"},\"stageTimestamp\":\"2019-07-23T14:45:19.813911Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:19Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projects\",\"namespace\":\"test\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Success\",\"code\":200},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:19.805940Z\",\"auditID\":\"9d3260ca-3bff-49da-9fe9-346043a29991\",\"verb\":\"delete\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projects/test\"}} 方式二：复用ocp集群内日志系统的Fluentd接受ApiServer通过web hook方式发送过来的审计日志并过滤、存储到挂载的文件中 整得太晚了，后续更新。 五、审计策略配置详解 整得太晚了，后续更新。 六、相关链接 https://austindewey.com/2018/10/17/integrating-advanced-audit-with-aggregated-logging-in-openshift-3-11/#test-it-outhttps://www.outcoldsolutions.com/docs/monitoring-openshift/v4/audit/https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-config-advanced-audithttps://docs.openshift.com/container-platform/3.11/security/monitoring.htmlhttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6chttps://www.jianshu.com/p/8117bc2fb966https://cloud.google.com/kubernetes-engine/docs/concepts/audit-policy?hl=zh-cnhttps://github.com/rbo/openshift-examples/tree/master/efk-auditloghttps://github.com/openshift/origin-aggregated-logging/issues/1226 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-12 09:35:46 "},"origin/openshift-elasticsearch容器化部署.html":{"url":"origin/openshift-elasticsearch容器化部署.html","title":"Elasticsearch容器化部署","keywords":"","body":"一、拉取镜像 docker pull docker.io/elasticsearch/elasticsearch:6.6.1 #或者 docker pull docker.elastic.co/elasticsearch/elasticsearch:6.6.1 二、Docker部署 修改系统 echo \"vm.max_map_count=262144\" >> /etc/sysctl.conf sysctl -w vm.max_map_count=262144 Docker单节点部署 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.io/elasticsearch/elasticsearch:6.6.1 Docker compose集群部署 version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch2 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnet volumes: esdata1: driver: local esdata2: driver: local networks: esnet: 三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: elasticsearch name: elasticsearch spec: replicas: 1 selector: app: elasticsearch deploymentconfig: elasticsearch strategy: type: Recreate template: metadata: labels: app: elasticsearch deploymentconfig: elasticsearch spec: containers: - env: - name: discovery.type value: single-node - name: cluster.name value: curiouser - name: bootstrap.memory_lock value: 'true' - name: path.repo value: /usr/share/elasticsearch/snapshots-repository - name: TZ value: Asia/Shanghai - name: ES_JAVA_OPTS value: '-Xms1g -Xmx2g' - name: xpack.monitoring.collection.enabled value: 'true' - name: xpack.security.enabled value: 'true' - name: ELASTIC_USERNAME value: \"elastic\" - name: \"ELASTIC_PASSWORD\" value: \"elastic\" image: 'docker.elastic.co/elasticsearch/elasticsearch:7.1.1' imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 name: elasticsearch ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 80 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 resources: limits: cpu: '2' memory: 3Gi requests: cpu: '1' memory: 2Gi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-data - mountPath: /usr/share/elasticsearch/snapshots-repository name: elasticsearch-snapshots-repository dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: elasticsearch name: elasticsearch spec: ports: - name: 9200-tcp port: 9200 protocol: TCP targetPort: 9200 - name: 9300-tcp port: 9300 protocol: TCP targetPort: 9300 selector: deploymentconfig: elasticsearch sessionAffinity: None type: ClusterIP 数据目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi snapshot repository存储目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-snapshots-repository spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 四. Kubernetes部署 Deployment kind: Deployment apiVersion: apps/v1 metadata: labels: elastic-app: elasticsearch role: master name: elasticsearch-master namespace: elk spec: replicas: 1 revisionHistoryLimit: 10 strategy: type: Recreate selector: matchLabels: elastic-app: elasticsearch role: master template: metadata: labels: elastic-app: elasticsearch role: master spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository && chown -R 1000.0 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository'] volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data - name: elasticsearch-snapshots-repository mountPath: /usr/share/elasticsearch/snapshots-repository containers: - name: elasticsearch-master-data image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: \"cluster.name\" value: \"Curiouser\" - name: \"bootstrap.memory_lock\" value: \"false\" - name: discovery.type value: single-node - name: \"node.master\" value: \"true\" - name: \"node.data\" value: \"true\" - name: \"node.ingest\" value: \"false\" - name: xpack.monitoring.collection.enabled value: \"true\" - name: \"xpack.monitoring.elasticsearch.collection.enabled\" value: \"true\" - name: \"xpack.security.enabled\" value: \"true\" - name: \"path.repo\" value: \"/usr/share/elasticsearch/snapshots-repository\" - name: \"ES_JAVA_OPTS\" value: \"-Xms2048m -Xmx2048m\" - name: TZ value: Asia/Shanghai - name: \"xpack.monitoring.exporters.my_local.type\" value: \"local\" - name: \"xpack.monitoring.exporters.my_local.use_ingest\" value: \"false\" resources: requests: memory: \"2Gi\" cpu: \"2\" limits: memory: \"4096Mi\" cpu: \"3\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 volumeMounts: - name: elasticsearch-data mountPath: \"/usr/share/elasticsearch/data\" - name: elasticsearch-snapshots-repository mountPath: \"/usr/share/elasticsearch/snapshots-repository\" restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository PersistentVolumeClaim --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-data namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-snapshots-repository namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi Service kind: Service apiVersion: v1 metadata: labels: elastic-app: elasticsearch-service name: elasticsearch namespace: elk spec: ports: - port: 9200 targetPort: 9200 protocol: TCP selector: elastic-app: elasticsearch type: ClusterIP Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-02 11:28:43 "},"origin/openshift-Kibana容器化部署.html":{"url":"origin/openshift-Kibana容器化部署.html","title":"Kibana容器化部署","keywords":"","body":"一、拉取镜像 docker pull docker.io/kibana/kibana:6.6.1 #或者 docker pull docker.elastic.co/kibana/kibana:6.6.1 二、Docker部署 单机Docker部署 docker run -p 5601:5601 \\ -e \"ELASTICSEARCH_HOST=http://ElasticSearch_HostIP:9200\" \\ -e \"SERVER_NAME=Curiouser\" \\ docker.elastic.co/kibana/kibana:6.6.1 三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: kibana name: kibana spec: replicas: 1 selector: app: kibana deploymentconfig: kibana strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: kibana deploymentconfig: kibana spec: containers: - env: - name: ELASTICSEARCH_USERNAME value: kibana - name: ELASTICSEARCH_PASSWORD value: uLAWAfW1b7UHZdHEigCW - name: TZ value: Asia/Shanghai image: docker.elastic.co/kibana/kibana:7.1.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 name: kibana ports: - containerPort: 5601 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 resources: limits: cpu: \"1\" memory: 1500Mi requests: cpu: 500m memory: 800Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: kibana name: kibana spec: ports: - name: 5601-tcp port: 5601 protocol: TCP targetPort: 5601 selector: deploymentconfig: kibana sessionAffinity: None type: ClusterIP Route apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: apache-kafka name: kibana spec: port: targetPort: 5601-tcp to: kind: Service name: kibana weight: 100 wildcardPolicy: None 四. Kubernetes上部署 Deployment apiVersion: apps/v1beta2 kind: Deployment metadata: labels: app: kibana name: kibana namespace: elk spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: kibana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: kibana spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com containers: - image: kibana/kibana:7.2.0 imagePullPolicy: IfNotPresent name: kibana envFrom: - secretRef: name: kibana-config-env env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOSTS value: '[\"http://elasticsearch.elk.svc:9200\"]' ports: - containerPort: 5601 name: web protocol: TCP resources: requests: memory: \"1Gi\" cpu: \"0.5\" limits: memory: \"2Gi\" cpu: \"1\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: {} privileged: false procMount: Default readOnlyRootFilesystem: false runAsNonRoot: false stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsConfig: {} dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Secret apiVersion: v1 kind: Secret metadata: labels: app: kibana name: kibana-config-env namespace: elk stringData: ELASTICSEARCH_USERNAME: kibana ELASTICSEARCH_PASSWORD: kibana Service apiVersion: v1 kind: Service metadata: name: kibana namespace: elk labels: app: kibana spec: ports: - port: 5601 name: web selector: app: kibana Ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: elk spec: rules: - host: kibana.apps.k8s.curiouser.com http: paths: - path: / backend: serviceName: kibana servicePort: 5601 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-02 11:33:41 "},"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html":{"url":"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html","title":"Kubernetes Plugin","keywords":"","body":"Jenkins在Kubernetes上使用Kubernetes插件动态创建Slave节点 一、Context 插件GIthub地址：https://github.com/jenkinsci/kubernetes-plugin Jenkins 分布式架构是由一个 Master 和多个 Slave Node组成的分布式架构。在 Jenkins Master 上管理你的项目，可以把你的一些构建任务分担到不同的 Slave Node 上运行，Master 的性能就提高了。Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行构建。一个master（jenkins服务所在机器）可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 传统的 Jenkins Slave 一主多从式会存在一些痛点。比如： 主 Master 发生单点故障时，整个流程都不可用了； 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致 管理起来非常不方便，维护起来也是比较费劲； 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态； 资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 而使用Kubernetes插件可以在Kubernetes上动态创建slave POD作为Slave节点。Jenkins Master 和 Slave 节点以 Docker Container 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。 这种方式的工作流程大致为：当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Docker Container 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且 Docker Container 也会自动删除，恢复到最初状态。这种方式带来的好处有很多： 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。 二、Jenkins与Slave的连接方式 Jenkins的Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。当job被分配到slave上运行的时候，此时master和slave其实是建立的双向字节流的连接，其中连接方法主要有如下几种： SSH：Jenkins内置有ssh客户端实现，可以用来与远程的sshd通信，从而启动slave agent。这是对unix系统的slave最方便的方法，因为unix系统一般默认安装有sshd。在创建ssh连接的slave的时候，你需要提供slave的host名字，用户名和ssh证书。创建public/private keys，然后将public key拷贝到slave的~/.ssh/authorized_keys中，将private key 保存到master上某ppk文件中。jenkins将会自动地完成其他的配置工作，例如copy slave agent的binary，启动和停止slave。 Java web start（JNLP：Java Network Lancher Protocol）：jnlp连接方式有个好处就是不用master和slave之间能够ssh连接，只需要能够ping即可。并且如果slave的机器是windows的话，也是可以的这个其实是非常实用的 WMI+DCOM：对于Windows的Slave，Jenkins可以使用Windows2000及以后内置的远程管理功能（WMI+DCOM），你只需要提供对slave有管理员访问权限的用户名和密码，jenkins将远程地创建windows service然后远程地启动和停止他们。对于windows的系统，这是最方便的方法，但是此方法不允许运行有显示交互的GUI程序。 在Kubernetes上的Jenkins通过Kubernetes插件动态创建的Slave POD节点是通过JNLP的方式与Jenkins Master进行通信的！ 三、Jenkins Kubernetes插件的安装配置 安装 配置 四、定制Slave镜像 Slave镜像中安装的软件信息 工具 版本 说明 Oracel JDK 1.8.0_171 Maven编译打包时使用 Apache Maven 3.6.1 在Slave容器中使用MAVEN编译打包源代码 helm v2.13.1 helm客户端 git 1.8.3.1 git命令 docker client 1.13.1 Dockers客户端，用于在Slave容器中构建应用镜像 sonar-scanner 3.3.0.1492 用于扫描源代码 FROM centos:7.4.1708 ENV TZ=Asia/Shanghai \\ LANG=en_US.UTF-8 \\ JDK_VERSION=Oracle_1.8.0_171 \\ MAVEN_VERSION=Apache_3.6.1 \\ HOME=/home/jenkins \\ MAVEN_HOME=/opt/apache-maven-3.6.1 \\ JAVA_HOME=/opt/jdk1.8.0_171 \\ SONARSCANNER_HOME=/opt/sonar-scanner-3.3.0.1492-linux COPY jdk1.8.0_171 /opt/jdk1.8.0_171 COPY apache-maven-3.6.1 /opt/apache-maven-3.6.1 COPY helm /usr/bin/helm COPY sonar-scanner-3.3.0.1492-linux /opt/sonar-scanner-3.3.0.1492-linux RUN curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo \\ && yum makecache \\ && yum install -y git docker-ce-cli make \\ && yum clean all \\ && groupadd -g 1000 jenkins \\ && useradd -c \"Jenkins user\" -d /home/jenkins -u 1000 -g 0 -m jenkins \\ && mkdir /home/jenkins/.m2 \\ && chown -R 1000.0 /home/jenkins \\ && ln -s /opt/jdk1.8.0_171/bin/java /usr/bin/java \\ && ln -s /opt/apache-maven-3.6.1/bin/mvn /usr/bin/mvn \\ && ln -s /opt/sonar-scanner-3.3.0.1492-linux/bin/sonar-scanner /usr/bin/sonar-scanner USER jenkins WORKDIR /home/jenkins COPY dumb-init /usr/bin/dumb-init ADD run-jnlp-client /usr/bin/ ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/usr/bin/run-jnlp-client\"] 五、Pipeline或者Job中使用验证 Job 创建一个自由风格的Job 点击构建后，会自动创建一个Slave POD，并通过JNLP协议与Jenkins Master的Agent端口5000进行通通信 Declarative Pipeline pipeline { agent { label 'maven' } stages { stage (\"代码编译\") { steps { configFileProvider([configFile(fileId: 'nexus-maven-settings', targetLocation: 'settings.xml')]){ sh 'mvn -s settings.xml compile' } } } stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.projectName=demo-springboot2 \\ -Dsonar.projectKey=demo-springboot2 \\ -Dsonar.sources=src \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=****** \\ -Dsonar.java.binaries=. \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.issuesReport.html.enable=true \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.gitlab.user_token=***** \\ -Dsonar.gitlab.url=http://gitlab.apps.okd311.curiouser.com/ \\ -Dsonar.gitlab.ignore_certificate=true \\ -Dsonar.gitlab.comment_no_issue=true \\ -Dsonar.gitlab.max_global_issues=1000 \\ -Dsonar.gitlab.unique_issue_per_inline=true\" } } stage (\"代码打包\") { steps { sh \"mvn -s settings.xml package\" } } stage(\"上传制品\"){ steps{ script{ def pomfile = readMavenPom file: 'pom.xml' sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u devops:**** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://nexus.apps.okd311.curiouser.com/repository/jenkins-product-repo/${pomfile.artifactId}-${pomfile.version}-${env.GIT_COMMIT}.${pomfile.packaging}\" } } } stage(\"构建应用镜像\"){ steps{ sh 'docker login -p ********** -u unused docker-registry-default.apps.okd311.curiouser.com' sh 'make' } } } post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 参考博客： https://github.com/easzlab/kubeasz/blob/master/docs/guide/jenkins.md https://www.qikqiak.com/k8s-book/docs/36.Jenkins%20Slave.html https://jenkins.io/blog/2018/09/14/kubernetes-and-secret-agents/ https://www.jianshu.com/p/1440b5b4b980 https://www.cnblogs.com/guguli/p/7827435.html https://blog.csdn.net/felix_yujing/article/details/78725142 https://jicki.me/kubernetes/2018/02/08/kubernetes-jenkins/ https://testerhome.com/topics/17251 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 19:43:02 "},"origin/jenkins-Nexus-Platform的使用.html":{"url":"origin/jenkins-Nexus-Platform的使用.html","title":"Nexus Platform Plugin","keywords":"","body":"Preflight 官方插件文档：https://help.sonatype.com/integrations/nexus-and-continuous-integration/nexus-platform-plugin-for-jenkins 安装插件：Pipeline Utility Steps 功能： 一、安装 二、配置 系统管理--> 系统设置--> Sonatype Nexus 三、使用 上传构建后的制品到Nexus的Hosted类型仓库中 Job Declarative Pipeline ```bash stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://nexus-nexus.apps.okd311.curiouser.com/service/rest/v1/search/assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } ``` 四、注意 如果Job再次构建，产生相同的Jar，上传信息还是一样的，Nexus的Release仓库需要设置为\"允许Redeploy\"。不然，仓库中已经相同版本信息的制品，会造成上传失败 参考链接 https://support.sonatype.com/hc/en-us/articles/115009108987-Jenkins-Publish-Using-Maven-Coordinates-from-the-pom-xml https://www.jianshu.com/p/29403ecf7fc2 https://stackoverflow.com/questions/37603619/extract-version-id-from-pom-in-a-jenkins-pipeline Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 22:34:34 "},"origin/jenkins-配置SMTP邮箱服务.html":{"url":"origin/jenkins-配置SMTP邮箱服务.html","title":"Mail Plugin","keywords":"","body":"Jenkins配置SMTP邮箱服务 Prerequisite 自己邮箱运营商设置了开通SMTP服务 Jenkins 安装了Jenkins Mailers Plugin 一、Context Jenkins默认有个插件叫\"Mailer Plugin\"用来发送通知邮件。该插件使用的\"JavaMail \"来进行配置自定义个邮箱服务器 二、配置 系统管理-->系统设置 配置Jenkins的系统管理员邮箱地址 配置SMTP邮件服务器地址 三、使用 Job中 四、问题 当构建不成功时发送的邮件，内容包含构建的日志。 当初次构建成功时会发送邮件通知，当再次重复构建成功时，则不会发送邮件通知，得等到构建失败时才会再次发送通知邮件 功能太弱，可使用\"Mail Extension\"插件进行功能扩展。详见：jenkins-Mailer邮箱功能扩展插件Email-Extension 不知Jenkins的系统管理员邮箱时，发送会报错 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 22:30:02 "},"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html":{"url":"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html","title":"Mail Extension","keywords":"","body":"jenkins Mailer邮箱功能扩展插件Email-Extension 一、Context Jenkins自带的邮件插件功能太弱，有个邮箱扩展插件。 官方文档WIKI：https://wiki.jenkins.io/display/JENKINS/Email-ext+plugin 优势： 邮件格式改为HTML，更美观 使用模板来配置邮件内容 为不同的Job配置不一样的收件人 为不同的事件配置不一样的trigger 在Jenkins pipeline中集成发送邮件通知功能 二、插件安装配置 安装 配置 三、使用 Jobs Pipeline中 pipeline{ ... post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 22:33:00 "},"origin/gitlab-配置SMTP邮件服务.html":{"url":"origin/gitlab-配置SMTP邮件服务.html","title":"配置SMTP邮件服务","keywords":"","body":"1. 修改/etc/gitlab/gitlab.rb ### Email Settings gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = 'rationalmonster@163.com' gitlab_rails['gitlab_email_display_name'] = 'Curiouser163SMTPServer' gitlab_rails['gitlab_email_reply_to'] = 'rationalmonster@163.com' gitlab_rails['gitlab_email_subject_suffix'] = 'Gitlab' gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.163.com\" gitlab_rails['smtp_port'] = 25 gitlab_rails['smtp_user_name'] = \"******@163.com\" gitlab_rails['smtp_password'] = \"******\" gitlab_rails['smtp_domain'] = \"163.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = false gitlab_rails['smtp_tls'] = false 2. 测试发送邮件 # gitlab-rails console Loading production environment (Rails 4.2.10) # irb(main):001:0> Notify.test_email('******@163.com','gitlab send mail test','gitlab test mail').deliver_now Notify#test_email: processed outbound mail in 539.6ms Sent mail to ******@163.com (397.8ms) Date: Thu, 04 Jul 2019 15:07:33 +0000 From: Curiouser163SMTPServer Reply-To: Curiouser163SMTPServer To:******@163.com Message-ID: Subject: gitlab send mail test Mime-Version: 1.0 Content-Type: text/html; charset=UTF-8 Content-Transfer-Encoding: 7bit Auto-Submitted: auto-generated X-Auto-Response-Suppress: All gitlab test mail=> #, >, >, , >, , , , , , > irb(main):002:0> Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 11:01:44 "},"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html":{"url":"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html","title":"SonarScanner-将扫描结果以comment的形式回写到gitlab","keywords":"","body":"一. Context 在Jenkins中做CI过程中,有一个步骤是代码编译完,使用sonar scanner扫描代码,检查静态代码中的语法错误,然后将代码发送到sonarqube,供项目经理查看代码质量. sonarqube可以安装插件gitlab,让sonarscanner扫描完代码,将结果以gitlab注释的方式回写到提交的commit中.方便开发人员排查代码. 以下操作过程各组件的版本 sonarqube: 7.3 (build 15553) sonarscanner: 3.3.0.1492 sonarqube gitlab插件: 4.0.0 gitlab: 10.8.4 ce jenkins: 2.150.2 Jenkins CI流水线是在使用Jenkins Slave(Kubernetes插件动态生成Slave POD)节点中来运行的,所以Sonarscanner,Maven等工具都是在Kubernetes Jenkins Slave镜像中已经安装好的. 二.操作 sonarqube 安装sonar-gitlab-plugin插件 插件Github:https://github.com/gabrie-allaigre/sonar-gitlab-plugin/ Sonarqube生成用户访问Token gitlab创建sonarscanner的用户,并生成AccessKey 在gitlab中将sonarqube加入到对应项目仓库的Members中 在Jenkins Pipeline中使用sonarscanner扫描代码 stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.projectKey=demo-springboot2 \\ -Dsonar.sources=. \\ -Dsonar.host.url=http://sonarqube-sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=058f4d4b905cba6123ffb093fa14b8f1fd9a75b \\ -Dsonar.java.binaries=. \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.gitlab.project_id=4 \\ -Dsonar.analysis.mode=preview \\ -Dsonar.issuesReport.html.enable=true \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.sources=src \\ -Dsonar.gitlab.user_token=e5D1Zo2132ikhGUcmSZZ \\ -Dsonar.gitlab.url=http://gitlab.apps.okd311.curiouser.com/ \\ -Dsonar.gitlab.ignore_certificate=true \\ -Dsonar.gitlab.comment_no_issue=true \\ -Dsonar.gitlab.max_global_issues=1000 \\ -Dsonar.gitlab.unique_issue_per_inline=true\" } } 三. 效果 四. soanarscanner参数详解 Variable Comment Type Version sonar.gitlab.url GitLab url Administration, Variable >= 1.6.6 sonar.gitlab.max_global_issues Maximum number of anomalies to be displayed in the global comment Administration, Variable >= 1.6.6 sonar.gitlab.user_token Token of the user who can make reports on the project, either global or per project Administration, Project, Variable >= 1.6.6 sonar.gitlab.project_id Project ID in GitLab or internal id or namespace + name or namespace + path or url http or ssh url or url or web Project, Variable >= 1.6.6 sonar.gitlab.commit_sha SHA of the commit comment Variable >= 1.6.6 sonar.gitlab.ref Branch name or reference of the commit Variable sonar.gitlab.ref_name Branch name or reference of the commit Variable >= 1.6.6 sonar.gitlab.max_blocker_issues_gate Max blocker issue for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_critical_issues_gate Max critical issues for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_major_issues_gate Max major issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_minor_issues_gate Max minor issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_info_issues_gate Max info issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.ignore_certificate Ignore Certificate for access GitLab, use for auto-signing cert (default false) Administration, Variable >= 2.0.0 sonar.gitlab.comment_no_issue Add a comment even when there is no new issue (default false) Administration, Variable >= 2.0.0 sonar.gitlab.disable_inline_comments Disable issue reporting as inline comments (default false) Administration, Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_file Show issue for commit file only (default false) Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_line Show issue for commit line only (default false) Variable >= 2.1.0 sonar.gitlab.build_init_state State that should be the first when build commit status update is called (default pending) Administration, Variable >= 2.0.0 sonar.gitlab.disable_global_comment Disable global comment, report only inline (default false) Administration, Variable >= 2.0.0 sonar.gitlab.failure_notification_mode Notification is in current build (exit-code) or in commit status (commit-status) (default commit-status) Administration, Variable >= 2.0.0 sonar.gitlab.global_template Template for global comment in commit Administration, Variable >= 2.0.0 sonar.gitlab.ping_user Ping the user who made an issue by @ mentioning. Only for default comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.unique_issue_per_inline Unique issue per inline comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.prefix_directory Add prefix when create link for GitLab Variable >= 2.1.0 sonar.gitlab.api_version GitLab API version (default v4 or v3) Administration, Variable >= 2.1.0 sonar.gitlab.all_issues All issues new and old (default false, only new) Administration, Variable >= 2.1.0 sonar.gitlab.json_mode Create a json report in root for GitLab EE (codeclimate.json or gl-sast-report.json) Project, Variable >= 3.0.0 sonar.gitlab.query_max_retry Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.query_wait Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.quality_gate_fail_mode Quality gate fail mode: error, warn or none (default error) Administration, Variable >= 3.0.0 sonar.gitlab.issue_filter Filter on issue, if MAJOR then show only MAJOR, CRITICAL and BLOCKER (default INFO) Administration, Variable >= 3.0.0 sonar.gitlab.load_rules Load rules for all issues (default false) Administration, Variable >= 3.0.0 sonar.gitlab.disable_proxy Disable proxy if system contains proxy config (default false) Administration, Variable >= 4.0.0 sonar.gitlab.merge_request_discussion Allows to post the comments as discussions (default false) Project, Variable >= 4.0.0 sonar.gitlab.ci_merge_request_iid The IID of the merge request if it’s pipelines for merge requests Project, Variable >= 4.0.0 五. 问题 当项目是私有仓库时 获取项目仓库的ProjectID 参考链接： https://gitlab.com/gitlab-org/gitlab-ce/issues/28342 https://www.cnblogs.com/amyzhu/p/8988519.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 17:25:54 "},"origin/ldap-Jenkins对接LDAP.html":{"url":"origin/ldap-Jenkins对接LDAP.html","title":"Jenkins","keywords":"","body":"一. Context OpenLDAP的条目组织形式 二. Jenkins配置 1. Jenkins安装LDAP插件 安装插件有两种方法： 方法一：后台插件管理里直接安装 优点：简单方便，不需要考虑插件依赖问题 缺点：因为网络等各种问题安装不成功 安装方法：登录Jenkins --> 系统管理 --> 插件管理 --> 可选插件 --> 搜索LDAP --> 选中 --> 直接安装 --> 安装完成重启 方法二：官网下载安装文件后台上传 优点：一定可以安装成功的 缺点：麻烦，要去官网找插件并解决依赖 安装方法：官网下载插件 --> 登录Jenkins --> 系统管理 --> 插件管理 --> 高级 --> 上传插件 --> 选择文件 --> 上传 --> 安装完成后重启 LDAP插件下载地址：https://updates.jenkins.io/download/plugins/ldap/ 2. 登录Jenkins --> 系统管理 --> 全局安全配置 root DN：这里的root DN只是指搜索的根，并非LDAP服务器的root dn。由于LDAP数据库的数据组织结构类似一颗大树，而搜索是递归执行的，理论上，我们如果从子节点（而不是根节点）开始搜索，因为缩小了搜索范围那么就可以获得更高的性能。这里的root DN指的就是这个子节点的DN，当然也可以不填，表示从LDAP的根节点开始搜索 User search base：这个配置也是为了缩小LDAP搜索的范围，例如Jenkins系统只允许ou为Admin下的用户才能登陆，那么你这里可以填写ou=Admin，这是一个相对的值，相对于上边的root DN，例如你上边的root DN填写的是dc=domain,dc=com，那么user search base这里填写了ou=Admin，那么登陆用户去LDAP搜索时就只会搜索ou=Admin,dc=domain,dc=com下的用户了 User search filter：这个配置定义登陆的“用户名”对应LDAP中的哪个字段，如果你想用LDAP中的uid作为用户名来登录，那么这里可以配置为uid={0}（{0}会自动的替换为用户提交的用户名），如果你想用LDAP中的mail作为用户名来登录，那么这里就需要改为mail={0}。在测试的时候如果提示你user xxx does not exist，而你确定密码输入正确时，就要考虑下输入的用户名是不是这里定义的这个值了 Group search base：参考上边User search base解释 Group search filter：这个配置允许你将过滤器限制为所需的objectClass来提高搜索性能，也就是说可以只搜索用户属性中包含某个objectClass的用户，这就要求你对你的LDAP足够了解，一般我们也不配置 Group membership：没配置，没有详细研究 Manager DN：这个配置在你的LDAP服务器不允许匿名访问的情况下用来做认证（详细的认证过程参考文章LDAP落地实战（二）：SVN集成OpenLDAP认证中关于LDAP服务器认证过程的讲解），通常DN为cn=admin,dc=domain,dc=com这样 Manager Password：上边配置dn的密码 Display Name LDAP attribute：配置用户的显示名称，一般为显示名称就配置为uid，如果你想显示其他字段属性也可以这里配置，例如mail Email Address LDAP attribute：配置用户Email对应的字段属性，一般没有修改过的话都是mail，除非你用其他的字段属性来标识用户邮箱，这里可以配置 3. 登录验证 参考链接 https://mp.weixin.qq.com/s/S5ozDJSh4yTSfP_glNoiOQ https://plugins.jenkins.io/ldap https://wiki.jenkins.io/display/JENKINS/LDAP+Plugin#LDAPPlugin-Groupmembership https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_liunx_52_ldap_for_jenkins.html https://blog.csdn.net/wanglei_storage/article/details/52935312 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 16:03:57 "},"origin/ldap-SonarQube对接LDAP.html":{"url":"origin/ldap-SonarQube对接LDAP.html","title":"SonarQube","keywords":"","body":"一、Context OpenLDAP的条目组织形式 Sonaeqube官方文档的操作步骤 二、操作 1、Sonarqube安装LDAP插件 配置--> 应用市场 2、修改配置文件/opt/sonarqube/conf/sonar.properties 如果sonarqube的部署实例是使用Dockers的话，则可通过环境变量的方式注入以下配置 sonar.security.realm=LDAP sonar.forceAuthentication=true ldap.authentication=simple ldap.url=ldap://openldap-service.openldap.svc:389 ldap.bindDn=cn=admin,dc=curiouser,dc=com ldap.bindPassword=****** # User Configuration ldap.user.baseDn=ou=employee,dc=curiouser,dc=com ldap.user.request=(&(memberOf=cn=sonarqube,ou=applications,dc=curiouser,dc=com)(cn={0})) ldap.user.realNameAttribute=sn ldap.user.emailAttribute=mail 相关配置 Property Description Default value Required Example sonar.security.realm Set this to LDAP authenticate first against the external sytem. If the external system is not reachable or if the user is not defined in the external system, authentication will be performed against SonarQube's internal database. none Yes LDAP (only possible value) sonar.authenticator.downcase Set to true when connecting to a LDAP server using a case-insensitive setup. false No ldap.url URL of the LDAP server. If you are using ldaps, you should install the server certificate into the Java truststore. none Yes ldap://localhost:10389 ldap.bindDn The username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory. none No cn=sonar,ou=users,o=mycompany ldap.bindPassword The password of the user to connect with. Leave this blank for anonymous access to the LDAP directory. none No secret ldap.authentication Possible values: simple, CRAM-MD5, DIGEST-MD5, GSSAPI. See the tutorial on authentication mechanisms simple No ldap.realm See Digest-MD5 Authentication, CRAM-MD5 Authentication none No example.org ldap.contextFactoryClass Context factory class. com.sun.jndi.ldap.LdapCtxFactory No ldap.StartTLS Enable use of StartTLS false No ldap.followReferrals Follow referrals or not. See Referrals in the JNDI true 用户配置 Property Description Default value Required Example ldap.user.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for users. None Yes cn=users,dc=example,dc=org ldap.user.request LDAP user request. (&(objectClass=inetOrgPerson)(uid={login})) No (&(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute Attribute in LDAP defining the user’s real name. cn No ldap.user.emailAttribute Attribute in LDAP defining the user’s email. mail No Group Mapping Only groups are supported (not roles). Only static groups are supported (not dynamic groups). For the delegation of authorization, groups must be first defined in SonarQube. Then, the following properties must be defined to allow SonarQube to automatically synchronize the relationships between users and groups. Property Description Default value Required Example for Active Directory ldap.group.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for groups. none No cn=groups,dc=example,dc=org ldap.group.request LDAP group request. (&(objectClass=groupOfUniqueNames)(uniqueMember={dn})) No (&(objectClass=group)(member={dn})) ldap.group.idAttribute Property used to specifiy the attribute to be used for returning the list of user groups in the compatibility mode. cn No sAMAccountName 重启Sonarqube，启动过程中如果出现以下日志，则证明LDAP连接成功 INFO org.sonar.INFO Security realm: LDAP ... INFO o.s.p.l.LdapContextFactory Test LDAP connection: OK 3、登录验证 4、权限控制 将admin用户的管理员权限删除，赋予另一个用户 参考链接 https://hub.docker.com/_/sonarqube?tab=description https://docs.sonarqube.org/latest/instance-administration/delegated-auth/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 16:04:07 "},"origin/ldap-Gitlab对接LDAP.html":{"url":"origin/ldap-Gitlab对接LDAP.html","title":"Gitlab","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、配置 1. 修改/etc/gitlab/gitlab.rb ..................省略............................. gitlab_rails['ldap_enabled'] = true ###! **remember to close this block with 'EOS' below** gitlab_rails['ldap_servers'] = YAML.load 三、测试登录 四、注意 当用户第一次使用LDAP登录GitLab时，如果其LDAP电子邮件地址是现有GitLab用户的电子邮件地址时，那么LDAP DN用户将与现有gitlab用户相关联。如果在GitLab的数据库中没有找到LDAP电子邮件属性，就会创建一个新用户。 换句话说，如果现有的GitLab用户希望自己启用LDAP登录，那么他们应该检查他们的GitLab电子邮件地址是否匹配LDAP电子邮件地址，然后通过他们的LDAP凭证登录GitLab。 https://docs.gitlab.com/ee/administration/auth/ldap.html#enabling-ldap-sign-in-for-existing-gitlab-users 参考链接 https://blog.csdn.net/tongdao/article/details/52538365 https://docs.gitlab.com/ee/administration/auth/ldap.html#configuration https://docs.gitlab.com/ee/administration/auth/how_to_configure_ldap_gitlab_ce/index.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 16:04:16 "},"origin/ldap-Nexus对接LDAP.html":{"url":"origin/ldap-Nexus对接LDAP.html","title":"Nexus","keywords":"","body":"Preflight Nexus 3 OpenLDAP 3.15.2-01 1.2.4 一、Context OpenLDAP的条目组织形式 二. Nexus设置 1. Nexus开启认证Realm 2. 配置LDAP Name：Enter a unique name for the new configuration. LDAP server address：Enter Protocol, Hostname, and Port of your LDAP server. Protocol：Valid values in this drop-down are ldap and ldaps that correspond to the Lightweight Directory Access Protocol and the Lightweight Directory Access Protocol over SSL. Hostname：The hostname or IP address of the LDAP server. Port：The port on which the LDAP server is listening. Port 389 is the default port for the ldap protocol, and port 636 is the default port for the ldaps. Search base：The search base further qualifies the connection to the LDAP server. The search base usually corresponds to the domain name of an organization. For example, the search base could be dc=example,dc=com. Note: If the values in your search base contain spaces, escape them with \"%20\", as in \"dc=example%20corp,dc=com\" You can configure one of four authentication methods to be used when connecting to the LDAP Server with the Authentication method drop-down. Simple Authentication：Simple authentication consists of a Username and Password. Simple authentication is not recommended for production deployments not using the secure ldaps protocol as it sends a clear-text password over the network. Anonymous Authentication：The anonymous authentication uses the server address and search base without further authentication. Digest-MD5：This is an improvement on the CRAM-MD5 authentication method. For more information, see RFC-2831. CRAM-MD5：The Challenge-Response Authentication Method (CRAM) is based on the HMAC-MD5 MAC algorithm. In this authentication method, the server sends a challenge string to the client. The client responds with a username followed by a Hex digest that the server compares to an expected value. For more information, see RFC-2195.For a full discussion of LDAP authentication approaches, see RFC-2829 and RFC-2251. SASL Realm：The Simple Authentication and Security Layer (SASL) realm used to connect to the LDAP server. It is only available if the authentication method is Digest-MD5 or CRAM-MD5. Username or DN：Username or DN (Distinguished Name) of an LDAP user with read access to all necessary users and groups. It is used to connect to the LDAP server. Password：Password for the Username or DN configured above. Base DN：Corresponds to the collection of distinguished names used as the base for user entries. This DN is relative to the Search Base. For example, if your users are all contained in ou=users,dc=sonatype,dc=com and you specified a Search Base of dc=sonatype,dc=com, you use a value of ou=users. User subtree：Check the box if True. Uncheck if False. Values are true if there is a tree below the Base DN that can contain user entries and false if all users are contain within the specified Base DN. For example, if all users are in ou=users,dc=sonatype,dc=com this field should be False. If users can appear in organizational units within organizational units such as ou=development,ou=users,dc=sonatype,dc=com, this field should be True . Object class：This value is a standard object class defined in RFC-2798. It specifies the object class for users. Common values are inetOrgPerson, person, user, or posixAccount. User filter：This allows you to configure a filter to limit the search for user records. It can be used as a performance improvement. User ID attribute：This is the attribute of the object class specified above, that supplies the identifier for the user from the LDAP server. The repository manager uses this attribute as the User ID value. Real name attribute：This is the attribute of the Object class that supplies the real name of the user. The repository manager uses this attribute when it needs to display the real name of a user similar to usage of the internal First name and Last name attributes. Email attribute：This is the attribute of the Object class that supplies the email address of the user. The repository manager uses this attribute for the Email attribute of the user. It is used for email notifications of the user. Password attribute：It can be used to configure the Object class, which supplies the password (\"userPassword\"). If this field is blank the user will be authenticated against a bind with the LDAP server. The password attribute is optional. When not configured authentication will occur as a bind to the LDAP server. Otherwise this is the attribute of the Object class that supplies the password of the user. The repository manager uses this attribute when it is authenticating a user against an LDAP server. Group Base DN：This field is similar to the Base DN field described for User Element Mapping, but applies to groups instead of users. For example, if your groups were defined under ou=groups,dc=sonatype,dc=com, this field would have a value of ou=groups. Group subtree：This field is similar to the User subtree field described for User Element Mapping, but configures groups instead of users. If all groups are defined under the entry defined in Base DN, set the field to false. If a group can be defined in a tree of organizational units under the Base DN, set the field to true. Group object class：This value in this field is a standard object class defined in RFC-2307. The class is simply a collection of references to unique entries in an LDAP directory and can be used to associate user entries with a group. Examples are groupOfUniqueNames, posixGroup or custom values. Group ID attribute：Specifies the attribute of the object class that specifies the group identifier. If the value of this field corresponds to the ID of a role, members of this group will have the corresponding privileges. Group member attribute：Specifies the attribute of the object class which specifies a member of a group. An example value is uniqueMember. Group member format：This field captures the format of the Group Member Attribute, and is used by the repository manager to extract a username from this attribute. An example values is ${dn} . 3. 分配Nexus管理员的角色\"nx-admin\"给LDAP上的一个用户，作为nexus新的管理员。然后将admin用户禁用。 参考链接 https://help.sonatype.com/repomanager3/security/ldap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 16:26:40 "},"origin/ldap-Grafana对接LDAP.html":{"url":"origin/ldap-Grafana对接LDAP.html","title":"Grafana","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、操作 1、修改/etc/grafana/grafana.ini .............省略............. [auth.ldap] enabled = true config_file = /etc/grafana/ldap.toml allow_sign_up = true .............省略............. 2、修改/etc/grafana/ldap.toml .............省略............. # To troubleshoot and get more log info enable ldap debug logging in grafana.ini # [log] # filters = ldap:debug [[servers]] # Ldap server host (specify multiple hosts space separated) host = \"openldap-service.openldap.svc\" # Default port is 389 or 636 if use_ssl = true port = 389 # Set to true if ldap server supports TLS use_ssl = false # Set to true if connect ldap server with STARTTLS pattern (create connection in insecure, then upgrade to secure connection with TLS) start_tls = false # set to true if you want to skip ssl cert validation ssl_skip_verify = false # set to the path to your root CA certificate or leave unset to use system defaults # root_ca_cert = \"/path/to/certificate.crt\" # Authentication against LDAP servers requiring client certificates # client_cert = \"/path/to/client.crt\" # client_key = \"/path/to/client.key\" # Search user bind dn bind_dn = \"cn=admin,dc=curiouser,dc=com\" # Search user bind password，If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" bind_password = '*********' # User search filter, for example \"(cn=%s)\" or \"(sAMAccountName=%s)\" or \"(uid=%s)\" search_filter = \"(&(memberOf=cn=grafana,ou=applications,dc=curiouser,dc=com))\" # An array of base dns to search through search_base_dns = [\"ou=employee,dc=curiouser,dc=com\"] ## For Posix or LDAP setups that does not support member_of attribute you can define the below settings。Please check grafana LDAP docs for examples # group_search_filter = \"(&(objectClass=posixGroup)(memberUid=%s))\" # group_search_base_dns = [\"ou=groups,dc=grafana,dc=org\"] # group_search_filter_user_attribute = \"uid\" # Specify names of the ldap attributes your ldap uses [servers.attributes] name = \"sn\" username = \"cn\" member_of = \"memberOf\" email = \"mail\" # Map ldap groups to grafana org roles [[servers.group_mappings]] #group_dn = \"cn=admins,dc=grafana,dc=org\" #org_role = \"Admin\" # To make user an instance admin (Grafana Admin) uncomment line below # grafana_admin = true # The Grafana organization database id, optional, if left out the default org (id 1) will be used # org_id = 1 [[servers.group_mappings]] group_dn = \"cn=users,dc=grafana,dc=org\" org_role = \"Editor\" [[servers.group_mappings]] # If you want to match all (or no ldap groups) then you can use wildcard group_dn = \"*\" #org_role = \"Viewer\" .............省略............. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 17:18:32 "},"origin/elasticsearch--_cat API.html":{"url":"origin/elasticsearch--_cat API.html","title":"_cat","keywords":"","body":"官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html 1. 查看_cat所有的Endpoint GET /_cat curl -XGET http://127.0.0.1:9200/_cat 　 /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates 查询Endpoint的参数 GET /_cat/health?help curl -XGET \"http://127.0.0.1:9200/_cat/health?help\" 　 　 # 参数全称 | 参数缩写 | 参数详解 ---------------------------------------------------------------------------------------------------- epoch | t,time | seconds since 1970-01-01 00:00:00 timestamp | ts,hms,hhmmss | time in HH:MM:SS cluster | cl | cluster name status | st | health status node.total | nt,nodeTotal | total number of nodes node.data | nd,nodeData | number of nodes that can store data shards | t,sh,shards.total,shardsTotal | total number of shards pri | p,shards.primary,shardsPrimary | number of primary shards relo | r,shards.relocating,shardsRelocating | number of relocating nodes init | i,shards.initializing,shardsInitializing | number of initializing nodes unassign | u,shards.unassigned,shardsUnassigned | number of unassigned shards pending_tasks | pt,pendingTasks | number of pending tasks max_task_wait_time | mtwt,maxTaskWaitTime | wait time of longest task pending active_shards_percent | asp,activeShardsPercent | active number of shards in percent 使用参数控制查询条件 GET /_cat/health?h=st,t #带表头 GET /_cat/health?v&h=st,t 控制查询的输出排序 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date #查询出来的Index将会以store.size的大小降序输出。只输出Index名，store.size大小，创建时间戳 curl -XGET \"http://elasticsearch-service.logger.svc:9200/_cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date\" 控制查询的输出格式 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date&format=yaml yaml - index: \"test-test-2019.05.21\" store.size: \"4.1gb\" creation.date: \"1558432572904\" - index: \".monitoring-es-7-2019.06.17\" store.size: \"1.2gb\" creation.date: \"1560729605158\" json [ { \"index\" : \"test-test-2019.05.21\", \"store.size\" : \"4.1gb\", \"creation.date\" : \"1558432572904\" }, { \"index\" : \".monitoring-es-7-2019.06.17\", \"store.size\" : \"1.2gb\", \"creation.date\" : \"1560729605158\" } ] text (default) index store.size creation.date test-test-2019.05.21 4.1gb 1558432572904 .monitoring-es-7-2019.06.17 1.2gb 1560729605158 cbor smile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-09 16:20:26 "},"origin/elasticsearch-7.1的xpack权限控制.html":{"url":"origin/elasticsearch-7.1的xpack权限控制.html","title":"Xpack","keywords":"","body":"一、Context 之前ELK套装安装X-Pack的安全功能时，只有安装30天的试用许可证时间，以允许访问所有功能。 当许可证到期时，X-Pack将以降级模式运行。可以购买订阅以继续使用X-Pack组件的全部功能（https://www.elastic.co/subscriptions）。但是,最近官方从6.8.0和7.1.0开始ELK开始免费提供安全功能. 本次实验,所有ELK组件版本均为7.1.0,以容器单节点运行 二. Elasticsearch开启Xpack elasticsearch的容器化部署参考笔记: ElasticSearch的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数 xpack.monitoring.collection.enabled(开启自我监控): true path.repo(设置snapshot存储仓库的路径): /usr/share/elasticsearch/snapshots-repository discovery.type(设置当前节点为单节点模式): single-node cluster.name(设置elasticsearch的集群名): curiouser bootstrap.memory_lock: 'true' TZ(设置时区): Asia/Shanghai ES_JAVA_OPTS(设置elasticsearch的JVM堆栈大小): '-Xms1g -Xmx2g' ELASTIC_USERNAME: \"kibana\" ELASTIC_PASSWORD: \"kibana\" xpack.security.enabled: 'true' xpack.security.transport.ssl.enabled: \"true\" xpack.security.transport.ssl.verification_mode: \"certificate\" xpack.security.transport.ssl.keystore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.transport.ssl.truststore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.http.ssl.enabled: \"false\" 查看elasticsearch是否开启xpack的安全验证 curl -XGET 'localhost:9200/_cat/health?v&pretty' # curl -XGET \"http://127.0.0.1:9200/_cat/health?v&pretty\" # 使用上述命令会返回401,提示未授权验证,使用以下命令进行安全验证地访问 curl --user kibana:****kibana用户的密码**** -XGET 'localhost:9200/_cat/health?v&pretty' 三. Kibana开启Xpack kibana的容器化部署详见笔记: Kibana的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数: ELASTICSEARCH_USERNAME: kibana用户 ELASTICSEARCH_PASSWORD: kibana用户的随机密码 TZ(设置时区): Asia/Shanghai 镜像中默认指定的elasticsearch地址为:http://elasticsearch:9200,刚好在open shift中部署的elasticsearch的svc名为\"elasticsearch\",它的访问方式为:http://elasticsearch:9200或者http://elasticsearch.命名空间.svc:9200 登录Kibana进行验证 使用elastic 超级用户进行登录，密码来自 setup-passwords 命令输出的结果 四. Logstash开启Xpack 配置logstash发送监控数据到elasticsearch xpack.monitoring.elasticsearch.hosts: \"http://elasticsearch:9200\" xpack.monitoring.enabled: \"true\" xpack.monitoring.elasticsearch.username: \"logstash_system\" xpack.monitoring.elasticsearch.password: \"***logstash_system用户的密码****\" 在kibana中查看logstash的监控数据 在kibana中创建logstash-pipeline角色,授予\"manage_index_template\",\"monitor\"的集群权限和\"write\",\"delete\",\"create_index\",\"manage_ilm\",\"manage\"的Index权限,然后绑定到logstash-pipeline用户上,用以创建Index并向其中写入数据 在pipeline的elasticsearch output插件中设置用户和密码 output{ elasticsearch{ hosts => \"elasticsearch:9200\" index => \"%{AppID}-%{+YYYY.MM.dd}\" user => \"logstash-pipeline\" password => \"****logstash-pipeline用户密码****\" } } 查看logstash的pipeline是否将数据写入的elasticsearch 五. 附录：Kibana上的角色权限 Cluster相关的角色权限 角色权限 权限描述 all Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. create_snapshot Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. manage Builds on monitor and adds cluster operations that change values in the cluster. This includes snapshotting, updating settings, and rerouting. It also includes obtaining snapshot and restore status. This privilege does not include the ability to manage security. manage_ccr All cross-cluster replication operations related to managing follower indices and auto-follow patterns. It also includes the authority to grant the privileges necessary to manage follower indices and auto-follow patterns. This privilege is necessary only on clusters that contain follower indices. manage_data_frame_transforms All operations on index templates. manage_ilm All operations on index templates. manage_index_templates All operations on index templates. manage_ingest_pipelines All operations on ingest node pipelines. manage_ml All machine learning operations, such as creating and deleting datafeeds, jobs, and model snapshots.Note：Datafeeds that were created prior to version 6.2 or created when security features were disabled run as a system user with elevated privileges, including permission to read all indices. Newer datafeeds run with the security roles of the user who created or updated them. manage_pipeline All operations on ingest pipelines. manage_rollup All rollup operations, including creating, starting, stopping and deleting rollup jobs. manage_saml Enables the use of internal Elasticsearch APIs to initiate and manage SAML authentication on behalf of other users. manage_security All security-related operations such as CRUD operations on users and roles and cache clearing. manage_token All security-related operations on tokens that are generated by the Elasticsearch Token Service. manage_watcher All watcher operations, such as putting watches, executing, activate or acknowledging.Note：Watches that were created prior to version 6.1 or created when the security features were disabled run as a system user with elevated privileges, including permission to read and write all indices. Newer watches run with the security roles of the user who created or updated them. monitor All cluster read-only operations, like cluster health and state, hot threads, node info, node and cluster stats, and pending cluster tasks. monitor_data_frame_transforms All read-only operations related to data frames. monitor_ml All read-only machine learning operations, such as getting information about datafeeds, jobs, model snapshots, or results. monitor_rollup All read-only rollup operations, such as viewing the list of historical and currently running rollup jobs and their capabilities. monitor_watcher All read-only watcher operations, such as getting a watch and watcher stats. read_ccr All read-only cross-cluster replication operations, such as getting information about indices and metadata for leader indices in the cluster. It also includes the authority to check whether users have the appropriate privileges to follow leader indices. This privilege is necessary only on clusters that contain leader indices. read_ilm All read-only index lifecycle management operations, such as getting policies and checking the status of index lifecycle management transport_client All privileges necessary for a transport client to connect. Required by the remote cluster to enable Cross Cluster Search. Index相关的角色权限 角色权限 权限描述 all Any action on an index create Privilege to index documents. Also grants access to the update mapping action.NoteThis privilege does not restrict the index operation to the creation of documents but instead restricts API use to the index API. The index API allows a user to overwrite a previously indexed document. create_index Privilege to create an index. A create index request may contain aliases to be added to the index once created. In that case the request requires the manage privilege as well, on both the index and the aliases names. delete Privilege to delete documents. delete_index Privilege to delete an index. index Privilege to index and update documents. Also grants access to the update mapping action. manage All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate). manage_follow_index All actions that are required to manage the lifecycle of a follower index, which includes creating a follower index, closing it, and converting it to a regular index. This privilege is necessary only on clusters that contain follower indices. manage_ilm All index lifecycle management operations relating to managing the execution of policies of an index This includes operations like retrying policies, and removing a policy from an index. manage_leader_index All actions that are required to manage the lifecycle of a leader index, which includes forgetting a follower. This privilege is necessary only on clusters that contain leader indices. monitor All actions that are required for monitoring (recovery, segments info, index stats and status). read Read-only access to actions (count, explain, get, mget, get indexed scripts, more like this, multi percolate/search/termvector, percolate, scroll, clear_scroll, search, suggest, tv). read_cross_cluster Read-only access to the search action from a remote cluster. view_index_metadata Read-only access to index metadata (aliases, aliases exists, get index, exists, field mappings, mappings, search shards, type exists, validate, warmers, settings, ilm). This privilege is primarily available for use by Kibana users. write Privilege to perform all write operations to documents, which includes the permission to index, update, and delete documents as well as performing bulk operations. Also grants access to the update mapping action. 六. 参考链接 https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html https://www.elastic.co/guide/en/elastic-stack-overview/7.1/get-started-logstash-user.html https://www.elastic.co/guide/en/logstash/current/ls-security.html https://www.elastic.co/guide/en/logstash/current/docker-config.html#docker-env-config Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 10:23:08 "},"origin/elasticSearch-索引的快照备份与恢复.html":{"url":"origin/elasticSearch-索引的快照备份与恢复.html","title":"Snapshots","keywords":"","body":"一、Context shared file system：NFS S3 HDFS 二、使用NFS作为快照仓库后端存储 在es集群中的某一个节点创建NFS文件系统，ES集群节点进行挂载 yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"/data/es/Elastic-SnapShots 172.16.3.0/24(rw,sync,no_root_squash,no_subtree_check) \" >> /etc/exports ;\\ export -r ;\\ showmount -e 127.0.0.1 集群其他节点挂载NFS共享目录 yum install nfs-utils -y ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"172.16.3.5:/data/es/Elastic-SnapShots /data/es/Elastic-SnapShots nfs defaults 0 0\" >> /etc/fstab ;\\ mount -a ;\\ df -mh 给elasticsearch授予共享目录/data/es/Elastic-SnapShots权限 chown -R elasticsearch:elasticsearch /data/es/Elastic-SnapShots ES集群所有节点配置文件设置 echo 'path.repo: [\"/data/es/Elastic-SnapShots\"]' >> /etc/elasticsearch/elasticsearch.yml ;\\ systemctl restart elasticsearch;\\ systemctl status elasticsearch 三、使用HDFS作为快照仓库后端存储 ES版本：5.6.8 HDFS版本：2.6.0 所有ES节点安装repository-hdfs插件 在线安装插件 /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-hdfs 离线安装插件，插件下载地址：https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip wget https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip ;\\ /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-hdfs-5.6.8.zip 重启ES集群所有节点 systemctl restart elasticsearch ;\\ systemctl status elasticsearch 后续创建HDFS类型仓库时遇到的问题 ES会以elasticsearch用户(即启动elasticsearch后台进程的用户)在HDFS的/user下创建文件时提示权限不足。所以修改HDFS上/user的权限 hdfs dfs -chmod -R 777 /user 如果HDFS集群在ES集群外面，ES中的Hadoop客户端向通过Hadoop NameNode节点返回的DataNode节点写数据时会找不到DataNode节点。因为创建仓库时只是指定NameNode节点的外网地址，而返回的DataNode节点IP地址是DataNode向NameNode节点注册的内网IP地址，ES集群根本无法访问到。所以打通两者之间的网络。 四、在 kibana 的 Dev Tools 上管理快照仓库 注册NFS类型的快照仓库 PUT /_snapshot/快照仓库名 { \"type\": \"fs\", \"settings\": { \"compress\": true, \"location\": \"/data/es/Elastic-SnapShots\" } } ##settings的其他参数： ​ #chunk_size Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified in bytes or by using size value notation, i.e. 1g, 10m, 5k. Defaults to null (unlimited chunk size). #max_restore_bytes_per_sec Throttles per node restore rate. Defaults to 40mb per second. #max_snapshot_bytes_per_sec Throttles per node snapshot rate. Defaults to 40mb per second. #readonly Makes repository read-only. Defaults to false. 注册HDFS类型的快照仓库 PUT _snapshot/快照仓库名 { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://172.16.3.10:9000\", \"compress\": true, \"path\": \"elasticsearch/respositories\" } } ##settings的其他参数： ​ #uri The uri address for hdfs. ex: \"hdfs://:/\". (Required) #path The file path within the filesystem where data is stored/loaded. ex: \"path/to/file\". (Required) #load_defaults Whether to load the default Hadoop configuration or not. (Enabled by default) #conf. Inlined configuration parameter to be added to Hadoop configuration. (Optional) Only client oriented properties from the hadoop core and hdfs configuration files will be recognized by the plugin. #compress Whether to compress the metadata or not. (Disabled by default) #chunk_size Override the chunk size. (Disabled by default) #security.principal Kerberos principal to use when connecting to a secured HDFS cluster. If you are using a service principal for your elasticsearch node, you may use the _HOST pattern in the principal name and the plugin will replace the pattern with the hostname of the node at runtime (see Creating the Secure Repository). 删除快照仓库 DELETE /_snapshot/快照仓库名 查看所有的快照仓库 GET _snapshot/_all 五、快照管理 创建包含所有Index的全量快照 PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true 创建中包含指定索引的快照 PUT /_snapshot/快照仓库名/快照名?wait_for_completion=true { \"indices\": \"index-A,index-B\", \"ignore_unavailable\": true, \"include_global_state\": false } 查看仓库中所有的快照 GET _snapshot/快照仓库名/_all GET _cat/snapshots/快照仓库名 删除一个快照 DELETE _snapshot/快照仓库名/快照名 查看多个快照的状态 GET /_snapshot/快照仓库名/快照1,快照2/_status 查看某一个快照状态 GET _snapshot/快照仓库名/快照名/_status 恢复一个快照 POST _snapshot/快照仓库名/快照名/_restore 六、更新 Elasticsearch 7.2.0新版本有了管理Snapshot Repository的新功能 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 10:23:12 "},"origin/elasticsearch-插件管理.html":{"url":"origin/elasticsearch-插件管理.html","title":"插件管理","keywords":"","body":"ES自带的有插件管理脚本命令 以RPM方式安装的ES，插件管理脚本在/usr/share/elasticsearch/bin/elasticsearch-plugin。该脚本能安装，列出，移除插件 $> cd /usr/share/elasticsearch/bin/ $> ./elasticsearch-plugin list #列出所有插件 $> ./elasticsearch-plugin install plugin_name #安装插件 $> ./elasticsearch-plugin remove plugin_name #卸载插件 #该脚本的参数 #-v 输出详细信息 #-s 输出最简信息 The script may return the following exit codes: 0 : everything was OK 64 : unknown command or incorrect option parameter 74 : IO error 70 : any other error 设置代理来安装插件 $ sudo ES_JAVA_OPTS=\"-Dhttp.proxyHost=代理服务器IP地址 -Dhttp.proxyPort=代理服务器端口 -Dhttps.proxyHost=代理服务器IP地址 -Dhttps.proxyPort=代理服务器端口\" bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-09 17:19:08 "},"origin/elasticsearch-问题总结.html":{"url":"origin/elasticsearch-问题总结.html","title":"问题总结","keywords":"","body":"一、elasticsearch集群开启“xpack的monitoring功能“导致”failed to flush export bulks和 there are no ingest nodes in this cluster”报错 原因： xpack的monitoring功能需要定义exporter用于导出监控数据， 默认的exporter是local exporter，也就是直接写入本地的集群，并且要求节点开启了ingest选项。 解决方案: 将集群的结点配置里的ingest角色打开 或者在集群设置elasticsearch.yml里，将local exporter的use ingest关掉 xpack.monitoring.exporters.my_local: type: local use_ingest: false 但一般的，使用local cluster监控自己存在很大的问题，故障发生时，监控也没法看到了。 生产上最好是设置一个单独的监控集群，然后可以配置一个HTTP exporter，将监控数据送往这个监控集群 参考： https://www.elastic.co/guide/en/x-pack/5.5/monitoring-cluster.html#http-exporter-reference https://elasticsearch.cn/question/1915 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-05 10:33:32 "},"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html":{"url":"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html","title":"Dockerfile中CMD与ENTRYPOINT命令的区别","keywords":"","body":"CMD命令设置容器启动后默认执行的命令及其参数，但CMD设置的命令能够被docker run命令后面的命令行参数替换 ENTRYPOINT配置容器启动时的执行命令（不会被忽略，一定会被执行，即使运行 docker run时指定了其他命令） Shell格式和Exec格式运行命令 我们可用两种方式指定 RUN、CMD 和 ENTRYPOINT 要运行的命令：Shell 格式和 Exec 格式： Shell格式： 。例如：apt-get install python3 Exec格式： [\"executable\", \"param1\", \"param2\", ...]。例如： [\"apt-get\", \"install\", \"python3\"] ENTRYPOINT命令 ENTRYPOINT 的 Exec 格式用于设置容器启动时要执行的命令及其参数，同时可通过CMD命令或者命令行参数提供额外的参数。ENTRYPOINT 中的参数始终会被使用，这是与CMD命令不同的一点。下面是一个例子： ENTRYPOINT [\"/bin/echo\", \"Hello\"] 当容器通过 docker run -it [image] 启动时，输出为： Hello 而如果通过 docker run -it [image] CloudMan 启动，则输出为： Hello CloudMan 将Dockerfile修改为： ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD [\"world\"] 当容器通过 docker run -it [image] 启动时，输出为： Hello world 而如果通过 docker run -it [image] CloudMan 启动，输出依旧为： Hello CloudMan ENTRYPOINT 中的参数始终会被使用，而 CMD 的额外参数可以在容器启动时动态替换掉。 参考链接 https://www.jianshu.com/p/f0a0f6a43907 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-02 17:30:56 "},"origin/docker-使用Makefile操作Dockerfile.html":{"url":"origin/docker-使用Makefile操作Dockerfile.html","title":"使用Makefile操作Dockerfile.md","keywords":"","body":" IMAGE_BASE = docker-registry-default.apps.okd311.curiouser.com/openshift IMAGE_NAME = demo-springboot2 IMAGE_VERSION = latest IMAGE_TAGVERSION = $(GIT_COMMIT) all: build tag push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . tag: docker tag ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} makefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误： Makefile:10: *** multiple target patterns. Stop. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-03 15:15:00 "},"origin/ceph-rbd单节点安装.html":{"url":"origin/ceph-rbd单节点安装.html","title":"Ceph RBD单节点安装","keywords":"","body":"Prerequisite Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 关闭防火墙和SeLinuxsystemctl disable firewalld ; systemctl stop firewalld ; sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config ; setenforce 0 ; sestatus -v SSH免密码登录打通ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ; ssh-copy-id root@allinone.curiouser.com ; ssh allinone.curiouser.com Hosts绑定IP地址域名解析echo \"192.168.1.21 allinone.curiouser.com\" >> /etc/hosts 创建ceph用户并设置用户密码和为其添加root权限useradd ceph && echo ceph:ceph | chpasswd ; echo \"ceph ALL=(root) NOPASSWD:ALL\" > /etc/sudoers.d/ceph ; chmod 0440 /etc/sudoers.d/ceph 配置Ceph和Epel的yum源仓库 vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=[http://download.ceph.com/rpm-jewel/el7/$basearch](http://download.ceph.com/rpm-jewel/el7/$basearch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=[http://download.ceph.com/rpm-jewel/el7/noarch](http://download.ceph.com/rpm-jewel/el7/noarch) enabled=1 gpgcheck=1 type=rpm-md gpgkey=[https://download.ceph.com/keys/release.asc](https://download.ceph.com/keys/release.asc) priority=1 # =========================================================================================== vim /etc/yum.repos.d/epel.repo [epel] name=Extra Packages for Enterprise Linux 7 - $basearch # baseurl=[http://download.fedoraproject.org/pub/epel/7/$basearch](http://download.fedoraproject.org/pub/epel/7/$basearch) metalink=[https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch](https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch) failovermethod=priority enabled=1 gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 # =========================================================================================== 可以修改ceph源（外国的源总是timeout） export CEPH_DEPLOY_REPO_URL=[http://mirrors.163.com/ceph/rpm-jewel/el7](http://mirrors.163.com/ceph/rpm-jewel/el7) ; export CEPH_DEPLOY_GPG_URL=[http://mirrors.163.com/ceph/keys/release.asc](http://mirrors.163.com/ceph/keys/release.asc) (可选)手动安装下载ceph的rpm包（使用ceph-deploy install 安装ceph包网速太慢。）官方下载: http://download.ceph.com/rpm-jewel/el7/x86_64/ 。需要下载的包如下： ceph-10.2.10-0.el7.x86_64.rpm ceph-base-10.2.10-0.el7.x86_64.rpm ceph-common-10.2.10-0.el7.x86_64.rpm ceph-mds-10.2.10-0.el7.x86_64.rpm ceph-mon-10.2.10-0.el7.x86_64.rpm ceph-osd-10.2.10-0.el7.x86_64.rpm ceph-radosgw-10.2.10-0.el7.x86_64.rpm ceph-selinux-10.2.10-0.el7.x86_64.rpm rbd-mirror-10.2.10-0.el7.x86_64.rpm yum localinstall -y ./*.rpm 一、安装Ceph-Deploy 安装Ceph-deploy yum install ceph-deploy -y 安装ceph相关的软件 ceph-deploy install $HOSTNAME 二、创建集群配置文件 创建ceph-deploy的集群配置文件夹，路径并切换过去mkdir my-cluster ;cd my-cluster 用 ceph-deploy 创建集群，用 new 命令、并指定主机作为初始监视器。 ceph-deploy new $HOSTNAME # 该操作会在~/my-cluster下会生成三个文件 $&gt; ls -rw-rw-r-- 1 ceph ceph 251 Jan 12 16:34 ceph.conf -rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log -rw------- 1 ceph ceph 73 Jan 12 16:30 ceph.mon.keyring # ceph.conf中默认的osd pool为3，对应了三个node节点。如果只有两个node节点，则需要修改ceph.conf中的默认值 [global] fsid = 25c13add-967e-4912-bb33-ebbc2cb9376d mon_initial_members = allinone.curiouser.com mon_host = 172.16.2.3 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx filestore_xattr_use_omap = true osd pool default size=1 三、创建Monitor ceph-deploy mon create $HOSTNAME ; ceph-deploy gatherkeys $HOSTNAME ; ceph mds stat #查看mds节点状态 四、创建OSD 方式一： (可选)手动节点上挂载lvm存储到某个目录下，作为node节点上OSD的数据存储目录 yum install -y lvm2 ; disk=/dev/vdc ; pvcreate ${disk} ; vgcreate ${disk} ; vgcreate -s 16m ceph-osd ${disk} ; PE_Number=`vgdisplay|grep \"Free PE\"|awk '{print $5}'` ; lvcreate -l ${PE_Number} -n ceph-osd ceph-osd ; mkfs.xfs /dev/ceph-osd/ceph-osd ; mkdir -p /data/ceph/osd ; chown -R ceph:ceph /data/ceph/osd ; echo \"/dev/ceph-osd/ceph-osd /data/ceph/osd xfs defaults 0 0\" &gt;&gt;/etc/fstab ; mount -a ; df -mh #LV的文件系统格式注意要xfs,CentOS推荐使用xfs的文件系统.如果是ext4，需要在/etc/ceph/ceph.conf 中添加参数用来限制文件名的长度 osd max object name len = 256 osd max object namespace len = 64 # 之后重启osd服务 systemctl restart ceph-osd.target 准备并激活node节点上的OSD #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/data/ceph/osd #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/data/ceph/osd #查看OSD状态 ceph osd tree 方式二：(不是以目录为OSD数据存储设备，而是直接以硬盘。其实就是省去手动在硬盘上创建分区的操作) #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/dev/vdc #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/dev/vdc1 #查看OSD状态 ceph osd tree 五、安装验证 #集群健康状态检查 $> ceph health HEALTH_OK $> ceph -s $> systemctl is-enabled ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 六、其他信息 Ceph相关的SystemD Units ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 13:49:20 "},"origin/ceph-filesystem单节点安装.html":{"url":"origin/ceph-filesystem单节点安装.html","title":"Ceph FileSystem单节点安装","keywords":"","body":"Context Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 一个cephfs至少要求两个librados存储池，一个为data，一个为metadata。当配置这两个存储池时，注意： 为metadata pool设置较高级别的副本级别，因为metadata的损坏可能导致整个文件系统不用 建议metadata pool使用低延时存储，比如SSD，因为metadata会直接影响客户端的响应速度 Preflight 一个 clean+active 的cluster（Ceph RBD单节点安装） cluster fb506b4e-43b8-4634-acb9-ea3ee5a97b91 health HEALTH_OK monmap e1: 1 mons at {allinone=192.168.1.96:6789/0} election epoch 29, quorum 0 allinone fsmap e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} osdmap e113: 1 osds: 1 up, 1 in flags sortbitwise,require_jewel_osds pgmap v61453: 192 pgs, 3 pools, 2639 MB data, 985 objects 2730 MB used, 94500 MB / 97231 MB avail 192 active+clean 一、操作 部署元数据服务器MDSceph-deploy mds create $HOSTNAME 创建cephfs需要的两个存储池：一个pool用来存储数据，一个pool用来存储元数据ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 创建CephFS ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls 二、验证 $ ceph mds stat e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} 三、客户端挂载 Kernel方式 #加载rbd内核模块 modprobe rbd lsmod | grep rbd # 获取client.admin用户的秘钥 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" mkdir /mnt/mycephfs mount -t ceph allinone.okd311.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== FUSE方式 yum -y install ceph-fuse ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m 192.168.197.154:6789 ~/mycephfs/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-01 13:46:21 "},"origin/npm仓库管理工具nrm.html":{"url":"origin/npm仓库管理工具nrm.html","title":"npm仓库管理工具nrm","keywords":"","body":"一、Overview Github地址：https://github.com/Pana/nrm 帮助快速切换npm仓库源。默认已经配置了npm,yarn,taobao,cnpm,nj,npmMirror,edunpm等常见的仓库源。 二、安装 npm install nrm -g 三、使用 $ nrm -h Usage: nrm [options] [command] Options: -V, --version output the version number -h, --help output usage information Commands: ls List all the registries current Show current registry name use Change registry to registry add [home] Add one custom registry set-auth [options] [value] Set authorize information for a custom registry with a base64 encoded string or username and pasword set-email Set email for a custom registry set-hosted-repo Set hosted npm repository for a custom registry to publish packages del Delete one custom registry home [browser] Open the homepage of registry with optional browser publish [options] [|] Publish package to current registry if current registry is a custom registry. if you're not using custom registry, this command will run npm publish directly test [registry] Show response time for specific or all registries help Print this help 查看默认支持的npm 仓库 $ nrm ls * npm -------- https://registry.npmjs.org/ yarn ------- https://registry.yarnpkg.com/ cnpm ------- http://r.cnpmjs.org/ taobao ----- https://registry.npm.taobao.org/ nj --------- https://registry.nodejitsu.com/ npmMirror -- https://skimdb.npmjs.com/registry/ edunpm ----- http://registry.enpmjs.org/ # \"*\"编注的仓库代表当前使用的仓库 添加私有的npm仓库 nrm add curiouser http://nexus.apps.okd311.curiouser.com/repository/NPM 切换npm仓库 nrm use 仓库名 删除仓库 nrm del 仓库名 测试仓库速度 nrm test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2019-08-02 10:31:37 "}}